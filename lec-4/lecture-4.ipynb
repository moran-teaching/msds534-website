{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Lecture 4\n",
    "execute:\n",
    "    error: true\n",
    "    warnings: false\n",
    "format:\n",
    "    html:\n",
    "        toc: true\n",
    "        html-math-method: katex\n",
    "        css: styles.css\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lecture, we study:\n",
    "\n",
    "- attention\n",
    "- building a transformer\n",
    "- fine-tuning language models\n",
    "\n",
    "#### References\n",
    "\n",
    "Note: much of this code is from Andrej Karpathy's excellent tutorials on building GPT from scratch: \n",
    "\n",
    "- [Youtube link](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=6253s)\n",
    "- [Google Collab notebook link](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing#scrollTo=O6medjfRsLD9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1170f8470>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to look at the Tiny Shakespeare dataset, which contains all the work of Shakespeare in a `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read it in to inspect it\n",
    "with open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset in characters:  1115393\n"
     ]
    }
   ],
   "source": [
    "print(\"length of dataset in characters: \", len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it \n"
     ]
    }
   ],
   "source": [
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we represent Tiny Shakespeare as numerical values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\n",
      "hello there\n"
     ]
    }
   ],
   "source": [
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) } # string to integer\n",
    "itos = { i:ch for i,ch in enumerate(chars) } # integer to string\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hello there\"))\n",
    "print(decode(encode(\"hello there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115393]) torch.float32\n",
      "tensor([18., 47., 56., 57., 58.,  1., 15., 47., 58., 47., 64., 43., 52., 10.,\n",
      "         0., 14., 43., 44., 53., 56., 43.,  1., 61., 43.,  1., 54., 56., 53.,\n",
      "        41., 43., 43., 42.,  1., 39., 52., 63.,  1., 44., 59., 56., 58., 46.,\n",
      "        43., 56.,  6.,  1., 46., 43., 39., 56.,  1., 51., 43.,  1., 57., 54.,\n",
      "        43., 39., 49.,  8.,  0.,  0., 13., 50., 50., 10.,  0., 31., 54., 43.,\n",
      "        39., 49.,  6.,  1., 57., 54., 43., 39., 49.,  8.,  0.,  0., 18., 47.,\n",
      "        56., 57., 58.,  1., 15., 47., 58., 47., 64., 43., 52., 10.,  0., 37.,\n",
      "        53., 59.,  1., 39., 56., 43.,  1., 39., 50., 50.,  1., 56., 43., 57.,\n",
      "        53., 50., 60., 43., 42.,  1., 56., 39., 58., 46., 43., 56.,  1., 58.,\n",
      "        53.,  1., 42., 47., 43.,  1., 58., 46., 39., 52.,  1., 58., 53.,  1.,\n",
      "        44., 39., 51., 47., 57., 46., 12.,  0.,  0., 13., 50., 50., 10.,  0.,\n",
      "        30., 43., 57., 53., 50., 60., 43., 42.,  8.,  1., 56., 43., 57., 53.,\n",
      "        50., 60., 43., 42.,  8.,  0.,  0., 18., 47., 56., 57., 58.,  1., 15.,\n",
      "        47., 58., 47., 64., 43., 52., 10.,  0., 18., 47., 56., 57., 58.,  6.,\n",
      "         1., 63., 53., 59.])\n"
     ]
    }
   ],
   "source": [
    "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
    "import torch \n",
    "data = torch.tensor(encode(text), dtype=torch.float)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:200]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now split up the data into train and validation sets\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18., 47., 56., 57., 58.,  1., 15., 47., 58.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_size = 8\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In language modeling, we want to predict the next word in a sequence. For a given block, what are we predicting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is tensor([18.]) the target: 47.0\n",
      "when input is tensor([18., 47.]) the target: 56.0\n",
      "when input is tensor([18., 47., 56.]) the target: 57.0\n",
      "when input is tensor([18., 47., 56., 57.]) the target: 58.0\n",
      "when input is tensor([18., 47., 56., 57., 58.]) the target: 1.0\n",
      "when input is tensor([18., 47., 56., 57., 58.,  1.]) the target: 15.0\n",
      "when input is tensor([18., 47., 56., 57., 58.,  1., 15.]) the target: 47.0\n",
      "when input is tensor([18., 47., 56., 57., 58.,  1., 15., 47.]) the target: 58.0\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[53., 59.,  6.,  1., 58., 56., 47., 40.],\n",
      "        [49., 43., 43., 54.,  1., 47., 58.,  1.],\n",
      "        [13., 52., 45., 43., 50., 53.,  8.,  0.],\n",
      "        [ 1., 39.,  1., 46., 53., 59., 57., 43.]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[59.,  6.,  1., 58., 56., 47., 40., 59.],\n",
      "        [43., 43., 54.,  1., 47., 58.,  1., 58.],\n",
      "        [52., 45., 43., 50., 53.,  8.,  0., 26.],\n",
      "        [39.,  1., 46., 53., 59., 57., 43.,  0.]])\n",
      "----\n",
      "when input is [53.0] the target: 59.0\n",
      "when input is [53.0, 59.0] the target: 6.0\n",
      "when input is [53.0, 59.0, 6.0] the target: 1.0\n",
      "when input is [53.0, 59.0, 6.0, 1.0] the target: 58.0\n",
      "when input is [53.0, 59.0, 6.0, 1.0, 58.0] the target: 56.0\n",
      "when input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0] the target: 47.0\n",
      "when input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0, 47.0] the target: 40.0\n",
      "when input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0, 47.0, 40.0] the target: 59.0\n",
      "when input is [49.0] the target: 43.0\n",
      "when input is [49.0, 43.0] the target: 43.0\n",
      "when input is [49.0, 43.0, 43.0] the target: 54.0\n",
      "when input is [49.0, 43.0, 43.0, 54.0] the target: 1.0\n",
      "when input is [49.0, 43.0, 43.0, 54.0, 1.0] the target: 47.0\n",
      "when input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0] the target: 58.0\n",
      "when input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0, 58.0] the target: 1.0\n",
      "when input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0, 58.0, 1.0] the target: 58.0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,)) # select a random integer from len(data) - block_size\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(2): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed in class, attention is the weighted average of previous word embeddings:\n",
    "\n",
    "$h_T = \\sum_{t=1}^T \\alpha_t x_t$\n",
    "\n",
    "Let's first write up the average (instead of weighted average).\n",
    "$h_T = \\frac{1}{T}\\sum_{t=1}^T x_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "B,T,C = 4,8,2 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
    "xrep = torch.zeros((B,T,C)) # this is our new representation, h\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # (t,C)\n",
    "        xrep[b,t] = torch.mean(xprev, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 2: using matrix multiply for a weighted aggregation\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / wei.sum(1, keepdim=True)\n",
    "xrep2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
    "torch.allclose(xrep, xrep2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3: use Softmax\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "xrep3 = wei @ x\n",
    "torch.allclose(xrep, xrep3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
       "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
       "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels (channels is the embedding size)\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False) # W_K matrix\n",
    "query = nn.Linear(C, head_size, bias=False) # W_Q matrix\n",
    "value = nn.Linear(C, head_size, bias=False) # W_V matrix\n",
    "k = key(x)   # (B, T, 16) W_K x\n",
    "q = query(x) # (B, T, 16) W_Q x\n",
    "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)   \n",
    "\n",
    "tril = torch.tril(torch.ones(T, T)) \n",
    "#wei = torch.zeros((T,T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf')) # mask the weight matrix so that we can't pay attention to future tokens\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "#out = wei @ x\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For regularization and to stabilize training, layer norm is used (instead of batch norm). Layer norm is the same as batch norm, except averages are taken over the sequence of tokens, not the batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 100])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LayerNorm1d:\n",
    "\n",
    "  def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
    "    self.eps = eps\n",
    "    self.gamma = torch.ones(dim)\n",
    "    self.beta = torch.zeros(dim)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    # calculate the forward pass\n",
    "    xmean = x.mean(1, keepdim=True) # sequence mean (in batch norm, axis=0 instead)\n",
    "    xvar = x.var(1, keepdim=True) # sequence variance (in batch norm, axis=0 instead)\n",
    "    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n",
    "    self.out = self.gamma * xhat + self.beta\n",
    "    return self.out\n",
    "\n",
    "  def parameters(self):\n",
    "    return [self.gamma, self.beta]\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "module = LayerNorm1d(100)\n",
    "x = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\n",
    "x = module(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "[Here](gpt.qmd) is a small GPT model from Andrej Karpathy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of turning text into discrete units (called tokens).  We saw we could map letters to numbers as we did with Tiny Shakespeare. However, this can be very inefficient.\n",
    "\n",
    "Many modern tokenizers use an algorithm such as byte pair encoding that greedily merges commonly occurring sub-strings based on their frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding Deep Learning, Figure 12.8\n",
    "\n",
    "![Understanding Deep Learning](fig/udl-12-8.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TikTokenizer](https://tiktokenizer.vercel.app/?model=google%2Fgemma-7b) is a nice tool to see how LLMs encode text into tokens:\n",
    "\n",
    "```\n",
    "Tokenization is at the heart of much weirdness of LLMs. \n",
    "\n",
    "127 + 456 = 583\n",
    "\n",
    "Apple.\n",
    "I have an apple.\n",
    "apple.\n",
    "Apple.\n",
    "\n",
    "for i in range(1, 101):\n",
    "    if i % 2 == 0:\n",
    "        print(\"hello world\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning\n",
    "\n",
    "We now look at finetuning a language representation model called DistilBERT (Sanh et al. 2019) [[arXiv](https://arxiv.org/abs/1910.01108)].\n",
    "\n",
    "DistilBERT is a 40% smaller, distilled version of BERT, which retains 97% of the original BERT model capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain pretrained models from [Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/distilbert).\n",
    "\n",
    "We need to install Hugging Face's transformers package:\n",
    "\n",
    "```{.bash}\n",
    "pip install transformers\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Included also is the model tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 7592, 2088,  102,    0,    0,    0,    0,    0,    0]])\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n",
      "[CLS] hello world [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "bert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "example_text = \"Hello world\"\n",
    "\n",
    "token_res = tokenizer(example_text, return_tensors='pt', max_length=10, padding='max_length') # pt is for pytorch tensors\n",
    "\n",
    "print(token_res['input_ids']) ## vector of token IDs\n",
    "print(token_res['attention_mask']) ## vector of 0/1 to indicate real tokens vs padding tokens\n",
    "\n",
    "out_text = tokenizer.decode(token_res['input_ids'][0])\n",
    "print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_model.config.hidden_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wine dataset\n",
    "\n",
    "We will finetune a wine dataset to predict *price* based on the wine *description*.\n",
    "\n",
    "The dataset has 120K wines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120975, 14)\n",
      "Description:  This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.\n",
      "Price:  15.0\n",
      "Description:  Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.\n",
      "Price:  14.0\n",
      "Description:  Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.\n",
      "Price:  13.0\n",
      "Most expensive wine:  This ripe wine shows plenty of blackberry fruits balanced well with some dry tannins. It is fresh, juicy with plenty of acidity, For a light vintage, it's perfumed, full of fresh flavors and will be ready to drink from 2017.\n",
      "Price:  3300.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x34ea5d6d0>,\n",
       "  <matplotlib.lines.Line2D at 0x34e9eb4d0>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x34e9e91d0>,\n",
       "  <matplotlib.lines.Line2D at 0x34e9e8e10>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x34ee4f890>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x34e9bfed0>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x34e9bd6d0>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhYAAAGdCAYAAABO2DpVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjUsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvWftoOwAAAAlwSFlzAAAPYQAAD2EBqD+naQAAHbJJREFUeJzt3X+QVfV9+P/X/pDLQnY3CckSdlhdWEwXFbWorWJopY3OEPWzDGK/1dDQOu3EhpqGthLJpEm0mE0k1rZxMJFkTC1V0zJIMyYx0zRjpOPSwbUm0kDkZ1wLQjV6d11wt/vj+4ezWxZBuXffu3fv7uMxc0fuue+79zX+wT4559xzSvr7+/sDACCB0kIPAACMH8ICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSKR/tD+zr64uDBw9GZWVllJSUjPbHAwB56O/vj46OjqitrY3S0lPvlxj1sDh48GDU1dWN9scCAAm0tbXFzJkzT/n6qIdFZWVlRLw5WFVV1Wh/PACQh/b29qirqxv8PX4qox4WA4c/qqqqhAUAFJl3Oo3ByZsAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhn1C2QB41Nvb29s3bo1Dh06FDNmzIiFCxdGWVlZoccCRpk9FsCwbd68OebMmROLFi2KG2+8MRYtWhRz5syJzZs3F3o0YJQJC2BYNm/eHMuWLYt58+ZFS0tLdHR0REtLS8ybNy+WLVsmLmCCKenv7+8fzQ9sb2+P6urqyGaz7hUCRa63tzfmzJkT8+bNiy1btgy5lXJfX18sWbIkduzYEbt373ZYBIrc6f7+tscCyNvWrVvjwIED8ZnPfGZIVERElJaWxpo1a2L//v2xdevWAk0IjDZhAeTt0KFDERFx3nnnnfT1ge0D64DxL6ew6Onpic9+9rMxa9asqKioiNmzZ8cdd9wRfX19IzUfMIbNmDEjIiJ27Nhx0tcHtg+sA8a/nMLiy1/+cnzta1+Le++9N3bu3Bl33XVXrFu3Lr761a+O1HzAGLZw4cKor6+PL37xi2/5B0ZfX180NzfHrFmzYuHChQWaEBhtOYVFS0tLNDU1xdVXXx319fWxbNmyuOqqq+Lpp58eqfmAMaysrCzuvvvueOyxx2LJkiVDvhWyZMmSeOyxx+IrX/mKEzdhAskpLD70oQ/Fv/3bv8Xzzz8fERE/+clP4t///d/jIx/5yIgMB4x9S5cujU2bNsVzzz0XCxYsiKqqqliwYEHs2LEjNm3aFEuXLi30iMAoyunKm5/+9Kcjm81GY2NjlJWVRW9vb9x5551xww03nPI9XV1d0dXVNfi8vb09/2mBMWnp0qXR1NTkyptAbmHx7W9/OzZu3BgPPfRQnHvuufHss8/Gpz71qaitrY0VK1ac9D3Nzc1x++23JxkWGLvKysriiiuuKPQYQIHldIGsurq6uO2222LlypWD29auXRsbN26MXbt2nfQ9J9tjUVdX5wJZAFBETvcCWTntsTh69OhbLoJTVlb2tl83zWQykclkcvkYAKBI5RQW1157bdx5551x5plnxrnnnhv/+Z//GX/9138dN91000jNBxSJ7u7uWL9+fezduzcaGhriE5/4REyaNKnQYwGjLKdDIR0dHfGXf/mX8eijj8aRI0eitrY2brjhhvjc5z532n+BuFcIjD+rV6+Oe+65J3p6ega3lZeXx6pVq+Kuu+4q4GRAKqf7+9tNyIBhWb16daxbty6mT58ea9eujWuuuSYee+yx+OxnPxuHDx+OW2+9VVzAOCAsgBHX3d0dU6dOjWnTpsWLL74Y5eX/d3S1p6cnZs6cGa+88kp0dnY6LAJFzt1NgRG3fv366OnpibVr1w6Jiog3D4Xccccd0dPTE+vXry/QhMBoExZA3vbu3RsREddcc81JXx/YPrAOGP+EBZC3hoaGiIh47LHHTvr6wPaBdcD45xwLIG/OsYCJwzkWwIibNGlSrFq1Kg4fPhwzZ86M+++/Pw4ePBj3339/zJw5Mw4fPhyrVq0SFTCB5HSBLIATDXyV9J577omPf/zjg9vLy8t91RQmIIdCgCRceRPGtxG5VwjAqZSVlcWFF14Y06dPjxkzZrhlOkxQzrEAhm3z5s3R0NAQixYtihtvvDEWLVoUDQ0NsXnz5kKPBowyYQEMy+bNm+O6666LF154Ycj2F154Ia677jpxAROMcyyAvPX29sa0adMim81GTU1NfOxjH4vZs2fHvn374sEHH4wjR45EdXV1vPLKKw6NQJFzjgUw4n70ox9FNpuNqVOnxuTJk+MrX/nK4GtnnnlmTJ06NbLZbPzoRz+KK6+8soCTAqPFoRAgb//wD/8QERGdnZ1xwQUXREtLS3R0dERLS0tccMEF0dnZOWQdMP7ZYwHkrb29PSIiLrnkktiyZUuUlr75b5VLL700tmzZEpdeemls3759cB0w/tljAeSttrY2IiJeffXVk77+y1/+csg6YPwTFkDeLrvssoiI2LNnTzQ1NQ05FNLU1DR4V9OBdcD451AIkLe6urrBP3/ve98bcpfT478Fcvw6YHwTFkDeFi5cGPX19VFWVhYHDhx4y+sNDQ3R19cXCxcuHP3hgIIQFkDeysrK4u67745ly5bFRz7ykWhoaIg33ngjJk+eHHv37o3vfe97sWnTJtewgAlEWADDsnTp0ti0aVP8+Z//eXz3u98d3D5r1qzYtGlTLF26tIDTAaPNlTeBJHp7e2Pr1q1x6NChmDFjRixcuNCeChhHXHkTGFVlZWVxxRVXFHoMoMCEBZBEd3d3rF+/Pvbu3RsNDQ3xiU98IiZNmlTosYBRJiyAYVu9enXcc8890dPTM7jt1ltvjVWrVsVdd91VwMmA0eYCWcCwrF69OtatWxfTpk2LDRs2xKFDh2LDhg0xbdq0WLduXaxevbrQIwKjyMmbQN66u7tj6tSpMW3atHjxxRejvPz/doL29PTEzJkz45VXXonOzk6HRaDIne7vb3ssgLytX78+enp6Yu3atUOiIiKivLw87rjjjujp6Yn169cXaEJgtAkLIG8D9wK55pprTvr6wPaBdcD4JyyAvDU0NEREDLlHyPEGtg+sA8Y/51gAeTv+HItf/OIX0dLSMniBrMsuuyzOOuss51jAOOECWcCImzRpUqxatSrWrVsXU6ZMib6+vsHXSktLo6+vL2699VZRAROIQyHAsFx66aUREXHizs+B5wOvAxODQyFA3np7e2POnDkxb968+Kd/+qf42te+NnjlzZtvvjl+53d+J3bs2BG7d+923xAocg6FACNu69atceDAgXj44YfjjDPOiAsvvDCmT58eM2bMiDPOOCPWrFkTCxYsiK1bt7qPCEwQwgLI26FDhyLiza+T3nDDDXHgwIHB1+rr62Pt2rVD1gHjn3MsgLzNmDEjIiKWL18e8+bNi5aWlujo6IiWlpaYN29eLF++fMg6YPxzjgWQN183hYnDORbAiHvqqaeip6cnDh8+HO95z3vi2LFjg69VVFQMPn/qqaecYwETRE6HQurr66OkpOQtj5UrV47UfMAYdvy5E8dHxYnPnWMBE0dOeyy2b98evb29g8937NgRV155ZVx//fXJBwPGvpqamiF//tjHPhazZ8+Offv2xYMPPhhHjhx5yzpgfMspLN7//vcPef6lL30pGhoa4jd/8zeTDgUUh56enoh4806mbW1tQ86juPPOO2Pq1KnR09MzuA4Y//L+Vkh3d3ds3LgxbrrppigpKUk5E1Ak/vEf/zEi3gyMZcuWDflWyLJlywaDYmAdMP7lHRZbtmyJ1157LX7/93//bdd1dXVFe3v7kAcwPnR0dERExJo1a+InP/lJLFiwIKqqqmLBggXx05/+NG677bYh64DxL++w+OY3vxmLFy+O2trat13X3Nwc1dXVg4+6urp8PxIYYxYuXBgREd/4xjfixRdfHPJaW1tbfOMb3xiyDhj/8rqOxS9+8YuYPXt2bN68OZqamt52bVdXV3R1dQ0+b29vj7q6OtexgHGgu7s7MpnMO67r6upyHQsociN6HYsHHnggampq4uqrr37HtZlM5rT+4gEAil/Oh0L6+vrigQceiBUrVkR5uetrwUT21a9+Nek6oPjlHBY//OEP44UXXoibbrppJOYBisjWrVsjImLOnDlvOX+qrq4u5syZM2QdMP7lvMvhqquuilG+vQgwRnV2dkZExJ49e6KiomLIay+//PLg1TcH1gHjn7ubAnmbP3/+4J9P/AfH8c+PXweMb06SAPL2vve9b/DP3d3dMWfOnCgtLY2+vr7Yt2/fSdcB45uwAPL26quvDv65r68v9uzZ847rgPHNoRAgbydeFGu464DiJyyAvJ3uXUvd3RQmDmEB5K2lpSXpOqD4CQsgb6d7U0E3H4SJQ1gAeZsyZUrSdUDxExZA3v7nf/4n6Tqg+AkLIG8vv/xy0nVA8RMWQN5ef/31pOuA4icsgLwdf9nu0tKhf50c/9z9hWDiEBZA3s4444zBP/f19Q157fjnx68DxjdhAeTt2muvTboOKH7CAsjbr/3aryVdBxQ/YQHk7Ze//GXSdUDxExZA3p5++umk64DiJyyAvB0+fDjpOqD4CQsgbyd+E2S464DiJyyAvHV3dyddBxQ/YQHkrbOzM+k6oPgJCyBvr732WtJ1QPETFkDeTvdS3S7pDROHsADy9r//+79J1wHFT1gAefOtEOBEwgIASEZYAHmrrq5Oug4ofsICyJtDIcCJhAWQNxfIAk4kLIC82WMBnEhYAHmbNGlS0nVA8RMWQN4cCgFOJCyAvLlAFnAiYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIJuew+O///u9Yvnx5TJs2LaZMmRIXXnhhtLa2jsRsAECRKc9l8auvvhqXX355LFq0KL7//e9HTU1N7N27N9797neP0HgAQDHJKSy+/OUvR11dXTzwwAOD2+rr61PPBBSJ8vLy6OnpOa11wMSQ06GQ73znO3HxxRfH9ddfHzU1NfGrv/qrsWHDhpGaDRjj+vv7k64Dil9OYbFv376477774uyzz44f/OAHcfPNN8cnP/nJePDBB0/5nq6urmhvbx/yAMaH3t7epOuA4lfSn8M/JSZNmhQXX3xxPPXUU4PbPvnJT8b27dujpaXlpO/5whe+ELfffvtbtmez2aiqqspjZGCsKCkpOe219lpAcWtvb4/q6up3/P2d0x6LGTNmxDnnnDNk29y5c+OFF1445XvWrFkT2Wx28NHW1pbLRwIARSSnM6ouv/zy+PnPfz5k2/PPPx9nnXXWKd+TyWQik8nkNx0AUFRy2mOxatWq2LZtW3zxi1+MPXv2xEMPPRT3339/rFy5cqTmAwCKSE5hcckll8Sjjz4aDz/8cJx33nnxV3/1V/E3f/M38dGPfnSk5gMAikhOJ2+mcLonfwBjn5M3YeIYkZM3AQDejrAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgmfJCDwAU1tGjR2PXrl0j/jnPPPNMzu9pbGyMKVOmjMA0wEjJKSy+8IUvxO233z5k2/Tp0+Oll15KOhQwenbt2hUXXXTRiH9OPp/R2toa8+fPH4FpgJGS8x6Lc889N374wx8OPi8rK0s6EDC6Ghsbo7W1Na/3Ll++PHbu3PmO6+bOnRsbN27M+ec3NjbmMxZQQDmHRXl5eXzgAx8YiVmAApgyZUreewVaW1tP61BFa2trVFRU5PUZQHHJ+eTN3bt3R21tbcyaNSt+93d/N/bt2zcScwFFoKKiIpqamt52TVNTk6iACSSnsPj1X//1ePDBB+MHP/hBbNiwIV566aVYsGBBvPLKK6d8T1dXV7S3tw95AOPHli1bThkXTU1NsWXLltEdCCionMJi8eLFcd1118W8efPiwx/+cHz3u9+NiIi///u/P+V7mpubo7q6evBRV1c3vImBMWfLli1x9OjRuP766yMi4vrrr4+jR4+KCpiAhnUdi6lTp8a8efNi9+7dp1yzZs2ayGazg4+2trbhfCQwRlVUVMRtt90WERG33Xabwx8wQQ3rOhZdXV2xc+fOWLhw4SnXZDKZyGQyw/kYAKBI5LTH4i/+4i/ixz/+cezfvz/+4z/+I5YtWxbt7e2xYsWKkZoPACgiOe2xePHFF+OGG26Il19+Od7//vfHpZdeGtu2bYuzzjprpOYDAIpITmHxyCOPjNQcAMA44CZkAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSGVZYNDc3R0lJSXzqU59KNA4AUMzyDovt27fH/fffH+eff37KeQCAIpZXWLz++uvx0Y9+NDZs2BDvec97Us8EABSpvMJi5cqVcfXVV8eHP/zhd1zb1dUV7e3tQx4AwPhUnusbHnnkkXjmmWdi+/btp7W+ubk5br/99pwHAwCKT057LNra2uJP//RPY+PGjTF58uTTes+aNWsim80OPtra2vIaFAAY+3LaY9Ha2hpHjhyJiy66aHBbb29vPPnkk3HvvfdGV1dXlJWVDXlPJpOJTCaTZloAYEzLKSx++7d/O5577rkh2/7gD/4gGhsb49Of/vRbogIAmFhyCovKyso477zzhmybOnVqTJs27S3bAYCJx5U3AYBkcv5WyImeeOKJBGMAAOOBPRYAQDLCAgBIRlgAAMkICwAgGWEBACQz7G+FAIWxe/fu6OjoKPQYQ+zcuXPIf8eKysrKOPvssws9BkwIwgKK0O7du+ODH/xgocc4peXLlxd6hLd4/vnnxQWMAmEBRWhgT8XGjRtj7ty5BZ7m/xw7diwOHDgQ9fX1UVFRUehxIuLNvSfLly8fc3t3YLwSFlDE5s6dG/Pnzy/0GENcfvnlhR4BKCAnbwIAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIpL/QAQH4+8K6SqHjt+YiD/n3wdipeez4+8K6SQo8BE4awgCL18YsmxdwnPx7xZKEnGdvmxpv/r4DRISygSH29tTv+v899K+Y2NhZ6lDFt565d8fW7b4z/V+hBYIIQFlCkXnq9P469+4MRtRcWepQx7dhLffHS6/2FHgMmDAdnAYBkhAUAkIywAACSERYAQDI5hcV9990X559/flRVVUVVVVVcdtll8f3vf3+kZgMAikxOYTFz5sz40pe+FE8//XQ8/fTT8Vu/9VvR1NQU//Vf/zVS8wEARSSnr5tee+21Q57feeedcd9998W2bdvi3HPPTToYAFB88r6ORW9vb/zzP/9zdHZ2xmWXXXbKdV1dXdHV1TX4vL29Pd+PBADGuJxP3nzuuefiXe96V2Qymbj55pvj0UcfjXPOOeeU65ubm6O6unrwUVdXN6yBAYCxK+ew+JVf+ZV49tlnY9u2bfHHf/zHsWLFivjZz352yvVr1qyJbDY7+GhraxvWwADA2JXzoZBJkybFnDlzIiLi4osvju3bt8ff/u3fxte//vWTrs9kMpHJZIY3JQBQFIZ9HYv+/v4h51AAABNXTnssPvOZz8TixYujrq4uOjo64pFHHoknnngiHn/88ZGaDwAoIjmFxeHDh+P3fu/34tChQ1FdXR3nn39+PP7443HllVeO1HwAQBHJKSy++c1vjtQcAMA4kPd1LIDCOXr0aEREPPPMMwWeZKhjx47FgQMHor6+PioqKgo9TkRE7Ny5s9AjwIQiLKAI7dq1KyIi/uiP/qjAkxSPysrKQo8AE4KwgCK0ZMmSiIhobGyMKVOmFHaY4+zcuTOWL18eGzdujLlz5xZ6nEGVlZVx9tlnF3oMmBCEBRSh973vffGHf/iHhR7jlObOnRvz588v9BhAAQz7OhYAAAOEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJCAsAIBlhAQAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkIywAACSySksmpub45JLLonKysqoqamJJUuWxM9//vORmg0AKDI5hcWPf/zjWLlyZWzbti3+9V//NXp6euKqq66Kzs7OkZoPACgi5bksfvzxx4c8f+CBB6KmpiZaW1vjN37jN5IOBgAUn5zC4kTZbDYiIt773veeck1XV1d0dXUNPm9vbx/ORwIAY1jeJ2/29/fHn/3Zn8WHPvShOO+88065rrm5OaqrqwcfdXV1+X4kADDG5R0Wf/InfxI//elP4+GHH37bdWvWrIlsNjv4aGtry/cjAYAxLq9DIbfcckt85zvfiSeffDJmzpz5tmszmUxkMpm8hgMAiktOYdHf3x+33HJLPProo/HEE0/ErFmzRmouAKAI5RQWK1eujIceeij+5V/+JSorK+Oll16KiIjq6uqoqKgYkQEBgOKR0zkW9913X2Sz2bjiiitixowZg49vf/vbIzUfAFBEcj4UAgBwKu4VAgAkIywAgGSEBQCQjLAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkExO9woBxp+jR4/Grl27kvysnTt3DvnvcDU2NsaUKVOS/CxgdAgLmOB27doVF110UdKfuXz58iQ/p7W1NebPn5/kZwGjQ1jABNfY2Bitra1JftaxY8fiwIEDUV9fHxUVFcP+eY2NjQmmAkZTSf8o3wu9vb09qqurI5vNRlVV1Wh+NACQp9P9/e3kTQAgGWEBACQjLACAZIQFkMT+/fujoqIiSktLo6KiIvbv31/okYAC8K0QYNjKysqir69v8Pkbb7wRs2fPjtLS0ujt7S3gZMBos8cCGJbjo6Kqqir+7u/+bvCM8b6+vigrKyvkeMAoExZA3vbv3z8YFYcPH45sNhu33HJLZLPZOHz4cES8GRcOi8DEISyAvJ1zzjkR8eaeipqamiGv1dTURGVl5ZB1wPgnLIC8dXV1RUTE2rVrT/r65z//+SHrgPHPlTeBvFVUVMQbb7wRVVVVkc1m3/J6VVVVdHR0xOTJk+PYsWMFmBBIxZU3gRH3s5/9LCLe/AvnyJEjQ147cuRIdHR0DFkHjH/CAsjbrFmzorT0zb9Gpk+fHlVVVXH33XdHVVVVTJ8+PSIiSktLY9asWYUcExhFDoUAw3bidSwGuI4FjB8OhQCjpre3N/bt2xeTJ0+OkpKSmDx5cuzbt09UwATkyptAErNmzXKCJmCPBQCQjrAAAJIRFgBAMsICAEhGWAAAyQgLACAZYQEAJCMsAIBkhAUAkMyoX3lz4NYk7e3to/3RAECeBn5vv9MtxkY9LAZuo1xXVzfaHw0ADFNHR0dUV1ef8vVRv7tpX19fHDx4MCorK6OkpGQ0PxoYYe3t7VFXVxdtbW3uXgzjTH9/f3R0dERtbW2Ulp76TIpRDwtg/Drd2yoD45eTNwGAZIQFAJCMsACSyWQy8fnPfz4ymUyhRwEKxDkWAEAy9lgAAMkICwAgGWEBACQjLACAZIQFMGxPPvlkXHvttVFbWxslJSWxZcuWQo8EFIiwAIats7MzLrjggrj33nsLPQpQYKN+EzJg/Fm8eHEsXry40GMAY4A9FgBAMsICAEhGWAAAyQgLACAZYQEAJONbIcCwvf7667Fnz57B5/v3749nn3023vve98aZZ55ZwMmA0ebupsCwPfHEE7Fo0aK3bF+xYkV861vfGv2BgIIRFgBAMs6xAACSERYAQDLCAgBIRlgAAMkICwAgGWEBACQjLACAZIQFAJCMsAAAkhEWAEAywgIASEZYAADJ/P9DdvLg/yV93gAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wine_df = pd.read_csv(\"data/wines.csv\")\n",
    "\n",
    "## keep only wines whose price is not NaN\n",
    "wine_df = wine_df[wine_df['price'].notna()]\n",
    "\n",
    "print(wine_df.shape)\n",
    "\n",
    "## key variables: price, description\n",
    "for i in range(3):\n",
    "    print(\"Description: \", wine_df['description'].iloc[i])\n",
    "    print(\"Price: \", wine_df['price'].iloc[i])\n",
    "\n",
    "## find the wine with the highest price\n",
    "max_price_idx = wine_df['price'].argmax()\n",
    "print(\"Most expensive wine: \", wine_df['description'].iloc[max_price_idx])\n",
    "print(\"Price: \", wine_df['price'].iloc[max_price_idx])\n",
    "\n",
    "## make box-plot of prices\n",
    "plt.boxplot(np.log(wine_df['price']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textClassDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "        self.tokens = tokenizer(df['description'].tolist(), return_tensors='pt', max_length=self.max_len, \n",
    "                                      padding='max_length', truncation=True) \n",
    "        self.price = torch.tensor(df['price'].to_numpy(), dtype=torch.float)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.price)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        input_ids = self.tokens['input_ids'][idx]\n",
    "        attention_mask = self.tokens['attention_mask'][idx]\n",
    "        price = self.price[idx]\n",
    "\n",
    "        return input_ids, attention_mask, price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 60487\n"
     ]
    }
   ],
   "source": [
    "dataset = textClassDataset(wine_df, tokenizer, 128)\n",
    "\n",
    "## split into train and test datasets\n",
    "train_size = int(0.5 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "print(\"training data size: \" + str(len(train_dataset)))\n",
    "\n",
    "## create dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertRegressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertRegressor, self).__init__()\n",
    "        self.bert = bert_model\n",
    "\n",
    "        ## for distilbert-base-uncased, hidden_size is 768\n",
    "        self.layer1 = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = bert_outputs[0][:, 0, :] # [CLS] token1 token2 ... this grabs [CLS] token\n",
    "\n",
    "        x = self.layer1(pooled_output)\n",
    "        x = x.squeeze(1)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can freeze bert parameters so that we only update the\n",
    "## prediction head\n",
    "# for param in model.bert.parameters():\n",
    "#     param.requires_grad = False\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                             lr=1e-4)\n",
    "\n",
    "num_epochs = 1\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "* 1 epoch of training takes about 20 mins.\n",
    "* You can skip the training to directly load from the saved model parameter file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        input_ids = batch[0]\n",
    "        attention_mask = batch[1]\n",
    "        logprice = np.log(batch[2])\n",
    "\n",
    "        pred = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(pred, logprice)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        it = it + 1\n",
    "        if (it % 100 == 0):\n",
    "            print(\"epoch: \", epoch, \"sgd iter: \" + str(it))\n",
    "    print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model parameters\n",
    "torch.save(model.state_dict(), \"fine-tuned-distilbert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## load model\n",
    "model.load_state_dict(torch.load(\"model/fine-tuned-distilbert.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/f0/m7l23y8s7p3_0x04b3td9nyjr2hyc8/T/ipykernel_6343/1264298161.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n",
      "  y_test = np.array([np.log(test_dataset[i][2]) for i in range(n_test)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.21289405226707458   Test R-squared: 0.4635502\n"
     ]
    }
   ],
   "source": [
    "## calculate testing error\n",
    "## takes about 2 minutes to run\n",
    "\n",
    "model.eval()\n",
    "mse = 0\n",
    "\n",
    "n_test = 600\n",
    "\n",
    "y_test = np.array([np.log(test_dataset[i][2]) for i in range(n_test)])\n",
    "\n",
    "for i in range(n_test):\n",
    "    pred = model(test_dataset[i][0].unsqueeze(0), test_dataset[i][1].unsqueeze(0))\n",
    "    mse = mse + (pred - y_test[i])**2\n",
    "\n",
    "mse = mse / n_test\n",
    "\n",
    "print(\"MSE:\", mse.item(), \"  Test R-squared:\", 1 - mse.item() / np.var(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Description: This white is both sour and bitter; it has a funny smell\n",
      "Predicted price:  15.726286888122559 \n",
      "\n",
      "My Description: the most amazing wine I have ever tasted\n",
      "Predicted price:  47.7934684753418 \n",
      "\n",
      "My Description: not bad at all; I would buy it again\n",
      "Predicted price:  12.506088256835938 \n",
      "\n",
      "My Description: actually quite bad; avoid if possible\n",
      "Predicted price:  14.870586395263672 \n",
      "\n",
      "My Description: great red and pretty cheap\n",
      "Predicted price:  12.206925392150879 \n",
      "\n",
      "My Description: great red but overpriced\n",
      "Predicted price:  24.468101501464844 \n",
      "\n",
      "My Description: great red and great price\n",
      "Predicted price:  12.49232006072998 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "\n",
    "my_reviews = [\"This white is both sour and bitter; it has a funny smell\",\n",
    "                \"the most amazing wine I have ever tasted\",\n",
    "                \"not bad at all; I would buy it again\",\n",
    "                \"actually quite bad; avoid if possible\",\n",
    "                \"great red and pretty cheap\",\n",
    "                \"great red but overpriced\",\n",
    "                \"great red and great price\"]\n",
    "\n",
    "for my_review in my_reviews:\n",
    "\n",
    "    token_res = tokenizer(my_review, return_tensors='pt')\n",
    "\n",
    "    pred = model(token_res['input_ids'], token_res['attention_mask'])\n",
    "    \n",
    "    print(\"My Description:\", my_review)\n",
    "    print(\"Predicted price: \", torch.exp(pred).item(), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources \n",
    "\n",
    "- [Text Classification with BERT](https://medium.com/@khang.pham.exxact/text-classification-with-bert-7afaacc5e49b)\n",
    "- [Hugging Face Tutorial](https://colab.research.google.com/drive/1pxc-ehTtnVM72-NViET_D2ZqOlpOi2LH?usp=sharing#scrollTo=SH_MAK-soD4F)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msds534",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
