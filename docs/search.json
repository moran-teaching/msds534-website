[
  {
    "objectID": "lec-1/lecture-1.html",
    "href": "lec-1/lecture-1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "# import packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker, cm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.interpolate import splrep, BSpline\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.pyplot import subplots\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#linear-regression-example",
    "href": "lec-1/lecture-1.html#linear-regression-example",
    "title": "Lecture 1",
    "section": "Linear Regression example",
    "text": "Linear Regression example\nSpecify parameters\n\nn = 100\np = 1\nbeta = 3\nsigma = 1\n\nGenerate data\n\nx = np.random.normal(size=(n, p))\ny = x * beta + np.random.normal(size=(n, 1)) * sigma\n\ncolnames = ['x' + str(i) for i in range(1, p+1)]\ncolnames.insert(0, 'y')\n\ndf = pd.DataFrame(np.hstack((y, x)), columns = colnames)\n\nFit linear regression model using sklearn\n\nlm = LinearRegression()\nlm.fit(x, y)\ny_hat = lm.predict(x)\nresid = y - y_hat\n\nPlot x vs. y using seaborn\n\nsns.set_theme()\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0])\n\n\n\n\n\n\n\n\nPlot x vs. y, including residual distances\n\ny_min = np.minimum(y, y_hat)\ny_max = np.maximum(y, y_hat)\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0])\nlm_plot.ax.vlines(x=list(x[:,0]), ymin=list(y_min[:,0]), ymax=list(y_max[:,0]), color = 'red', alpha=0.5)",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#overfitting-example",
    "href": "lec-1/lecture-1.html#overfitting-example",
    "title": "Lecture 1",
    "section": "Overfitting example",
    "text": "Overfitting example\n\nsort_ind = np.argsort(x, axis=0)\nxsort = np.take_along_axis(x, sort_ind, axis=0)\nysort = np.take_along_axis(y, sort_ind, axis=0)\ntck = splrep(xsort, ysort, s=20)\n\nxspline = np.arange(x.min(), x.max(), 0.01)\nyspline = BSpline(*tck)(xspline)\n\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3.5, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0], label = \"Linear Regression\")\nplt.plot(xspline, yspline,color='orange', label = \"Spline\")\nplt.legend(loc='upper left')",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#shrinkage-plot",
    "href": "lec-1/lecture-1.html#shrinkage-plot",
    "title": "Lecture 1",
    "section": "Shrinkage plot",
    "text": "Shrinkage plot",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#lasso-example",
    "href": "lec-1/lecture-1.html#lasso-example",
    "title": "Lecture 1",
    "section": "Lasso example",
    "text": "Lasso example\nThis code is adapted from ISLP labs.\n\nn = 100\np = 90\nbeta = np.zeros(p)\nbeta[0]=3\nbeta[1]=3\ncov = 0.6 * np.ones((p, p))\nnp.fill_diagonal(cov, 1)\nx = np.random.multivariate_normal(mean=np.zeros(p), cov=cov, size=n)\ny = np.matmul(x, beta) + np.random.normal(size=n)\n\nx_columns = ['x' + str(i+1) for i in range(p)]\n\n\n# set up cross-validation\nK=5\nkfold = sklearn.model_selection.KFold(K,random_state=0,shuffle=True)\n\n# function to standardize input\nscaler = StandardScaler(with_mean=True, with_std=True)\n\n\nlassoCV = sklearn.linear_model.ElasticNetCV(n_alphas=100, l1_ratio=1,cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),('lasso', lassoCV)])\npipeCV.fit(x, y)\ntuned_lasso = pipeCV.named_steps['lasso']\ntuned_lasso.alpha_\n\nlambdas, soln_array = sklearn.linear_model.Lasso.path(x, y,l1_ratio=1,n_alphas=100)[:2]\nsoln_path = pd.DataFrame(soln_array.T,columns=x_columns, index=-np.log(lambdas))\n\n\npath_fig, ax = subplots(figsize=(8,8))\nsoln_path.plot(ax=ax, legend=False)\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Standardized coefficiients', fontsize=20);\n\n\n\n\n\n\n\n\n\nlassoCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(tuned_lasso.alphas_),tuned_lasso.mse_path_.mean(1),yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#ridge-regression",
    "href": "lec-1/lecture-1.html#ridge-regression",
    "title": "Lecture 1",
    "section": "Ridge regression",
    "text": "Ridge regression\nThis code is adapted from ISLP labs\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\nridgeCV = sklearn.linear_model.ElasticNetCV(alphas=lambdas, \n                           l1_ratio=0,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('ridge', ridgeCV)])\npipeCV.fit(x, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([1.84404932e+07, 1.46137755e+07, 1.15811671e+07, 9.17787690e+06,\n       7.27331049e+06, 5.76397417e+06, 4.56785096e+06, 3.61994377e+06,\n       2.86874353e+06, 2.27343019e+06, 1.80165454e+06, 1.42778041e+06,\n       1.13149156e+06, 8.96687712e+05, 7.10609677e+05, 5.63146016e+05,\n       4.46283587e+05, 3.53672111e+05,...\n       1.53096214e-01, 1.21326131e-01, 9.61488842e-02, 7.61963464e-02,\n       6.03843014e-02, 4.78535262e-02, 3.79231012e-02, 3.00534091e-02,\n       2.38168128e-02, 1.88744168e-02, 1.49576525e-02, 1.18536838e-02,\n       9.39384172e-03, 7.44445891e-03, 5.89960638e-03, 4.67533716e-03,\n       3.70512474e-03, 2.93624800e-03, 2.32692632e-03, 1.84404932e-03]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('ridge', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    ElasticNetCV?Documentation for ElasticNetCV\n        \n            \n                Parameters\n                \n\n\n\n\nl1_ratio \n0\n\n\n\neps \n0.001\n\n\n\nn_alphas \n'deprecated'\n\n\n\nalphas \narray([1.8440...84404932e-03])\n\n\n\nfit_intercept \nTrue\n\n\n\nprecompute \n'auto'\n\n\n\nmax_iter \n1000\n\n\n\ntol \n0.0001\n\n\n\ncv \nKFold(n_split... shuffle=True)\n\n\n\ncopy_X \nTrue\n\n\n\nverbose \n0\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\nrandom_state \nNone\n\n\n\nselection \n'cyclic'\n\n\n\n\n            \n        \n    \n\n\n\nlambdas, soln_array = sklearn.linear_model.ElasticNet.path(x, y,l1_ratio=0,alphas=lambdas)[:2]\nsoln_path = pd.DataFrame(soln_array.T,columns=x_columns, index=-np.log(lambdas))\n\npath_fig, ax = subplots(figsize=(8,8))\nsoln_path.plot(ax=ax, legend=False)\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Standardized coefficiients', fontsize=20);\n\n\n\n\n\n\n\n\n\ntuned_ridge = pipeCV.named_steps['ridge']\nridgeCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#logistic-regression-example",
    "href": "lec-1/lecture-1.html#logistic-regression-example",
    "title": "Lecture 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nGenerate data\n\nn = 100\np = 2\nx = np.random.uniform(-2, 2, size=(n, p))\n\nbeta = np.array([2.5, -2.5])\nmu = np.matmul(x, beta)\nprob = 1/(1 + np.exp(-mu))\n\ny = np.zeros((n))\nfor i in range(n):\n    y[i] = np.random.binomial(1, prob[i], 1)[0]\n\ndf = np.hstack([y.reshape((n, 1)), x])\ndf = pd.DataFrame(df, columns = ['y', 'x1', 'x2'])\n\nsns.set_theme()\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nlogit_plot.figure.subplots_adjust(top=.9)\n\n\n\n\n\n\n\n\nFitting the logistic regression model\n\nlog_fit = LogisticRegression()\nlog_fit.fit(x, y)\ncoeffs = log_fit.coef_[0]\ncoeff = -coeffs[0]/coeffs[1]\n\nPlot x_1\\beta_1 + x_2\\beta_2 = 0\n\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nplt.axline([0,0], slope=coeff)\n## title\nlogit_plot.figure.subplots_adjust(top=.9)\nlogit_plot.figure.suptitle(str(round(coeffs[0], 2)) + r'$x_1$ - ' + str(round(-coeffs[1], 2)) + r'$x_2 = 0$')\n## fill in area\nx_fill = np.linspace(-2, 2, num=200)\ny_line = coeff * x_fill\nlogit_plot.ax.fill_between(x_fill, y_line, 2, color='blue', alpha=0.2)\nlogit_plot.ax.fill_between(x_fill, -2, y_line, color='orange', alpha=0.2)\n\nlogit_plot.ax.annotate(r'$\\bf P(Y=1)&lt;0.5$', (0.5, 2.1), color='blue')\nlogit_plot.ax.annotate(r'$\\bf P(Y=1)&gt;0.5$', (0.5, -2.2), color='darkorange')\n\nText(0.5, -2.2, '$\\\\bf P(Y=1)&gt;0.5$')\n\n\n\n\n\n\n\n\n\n\n# Create a meshgrid for x1 and x2\nx1_range = np.linspace(x[:, 0].min(), x[:, 0].max(), 200)\nx2_range = np.linspace(x[:, 1].min(), x[:, 1].max(), 200)\nX1, X2 = np.meshgrid(x1_range, x2_range)\n\n# Compute the sigmoid function using the fitted logistic regression coefficients\nZ = 1 / (1 + np.exp(-(log_fit.intercept_[0] + log_fit.coef_[0,0]*X1 + log_fit.coef_[0,1]*X2)))\n\nfig = plt.figure(figsize=(5, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Set background to white\nax.set_facecolor('white')\nfig.patch.set_facecolor('white')\n\n# Plot with smooth color transitions\nsurf = ax.plot_surface(X1, X2, Z, cmap='coolwarm', antialiased=True, linewidth=0, rstride=1, cstride=1)\n\nax.view_init(elev=15, azim=65+155)\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel(r'P(y=1)')\n\nax.set_title(\" \"*115) \nax.text2D(0.5, 0.91, r'g(' + str(round(coeffs[0], 2)) + r'$x_1$  ' +\n         str(round(coeffs[1], 2)) + r'$x_2)$',\n         transform=ax.transAxes, ha='center', va='top')\n\nText(0.5, 0.91, 'g(1.76$x_1$  -1.8$x_2)$')",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#principal-components-analysis",
    "href": "lec-1/lecture-1.html#principal-components-analysis",
    "title": "Lecture 1",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\npenguins = sns.load_dataset(\"penguins\")\n\nfig = plt.figure()\n\nax = Axes3D(fig)\nfig.add_axes(ax)\n\ncmap = matplotlib.colors.ListedColormap(sns.color_palette(\"Paired\", 3))\n\ncols = penguins['species'].copy()\ncols[cols=='Adelie']=1\ncols[cols=='Chinstrap']=2\ncols[cols=='Gentoo']=3\n\nsc = ax.scatter3D(penguins['bill_depth_mm'],\n                penguins['bill_length_mm'],\n                penguins['flipper_length_mm'],\n                c = cols,\n                cmap=cmap,\n                alpha=1)\nax.set_xlabel('bill depth')\nax.set_ylabel('bill length')\nax.set_zlabel('flipper length')\nax.set_facecolor((1.0, 1.0, 1.0, 0.0))\n\n\n\n\n\n\n\n\n\nx = penguins[['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm']]\nx = x.dropna(axis=0)\n\npca_fit = PCA()\npca_fit.fit(x)\nz = pca_fit.transform(x)\n\nz_df = pd.DataFrame(z[:, 0:2], columns = ['z1', 'z2'])\nz_df['species']=penguins['species']\n\nsns.set_theme()\npca_plot = sns.relplot(z_df, x='z1', y='z2', hue='species', palette=sns.color_palette(\"Paired\", 3), height=4)\n\n\n\n\n\n\n\n\n\nPC_values = np.linspace(1,3,3).reshape(3,1)\nscree_df = np.hstack([PC_values, pca_fit.explained_variance_ratio_.reshape(3,1)])\nscree_df = pd.DataFrame(scree_df, columns = ['Principal Components', 'Explained Variance Ratio'])\nscree_plot = sns.relplot(scree_df, x='Principal Components', y='Explained Variance Ratio', marker='o', kind='line', height=4)",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lec-2/lecture-2.html",
    "href": "lec-2/lecture-2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "# import packages\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#pytorch",
    "href": "lec-2/lecture-2.html#pytorch",
    "title": "Lecture 2",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source machine learning library. It is one of the most popular frameworks for deep learning.\nIn Python, the PyTorch package is called torch, available here.",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#simple-linear-regression",
    "href": "lec-2/lecture-2.html#simple-linear-regression",
    "title": "Lecture 2",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nLet’s see how PyTorch can help us solve a predictive problem using the most simple linear regression model.\nA simple linear regression model assumes that the observed data sample y_1, y_2, ..., y_n follows the following structure:\n\ny_i = a + bx_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,1).\n\n\nData Generation\nLet’s start by creating some synthetic data to work with. The following code block does the following:\n\nWe generate x_1, x_2, ..., x_{100} with each x_i \\sim \\text{Uniform}(-2,2).\nWe let a = 1 and b = 2.\nWe create y_1, y_2, ..., y_{100} by setting y_i = 1 + 2x_i + \\epsilon_i, where \\epsilon_i is some standard Gaussian noise.\n\n\n# Data Generation\n# np.random.uniform generates random samples from a uniform distribution over [0, 1).\nx = np.random.uniform(low=-2,high=2, size =(100, 1))                              \n# the resulting dimension of vector x will be 100 x 1.\n\n# np.random.randn generates random samples from a standard normal distribution.\ny = 1 + 2 * x + np.random.randn(100, 1)            \n\n\n\nTrain-Validation Split\nNow we do a train-validation split, by randomly picking 80% of the indices as the train set and the rest as validation.\n\n# Shuffles the indices\nidx = np.arange(100)\nnp.random.shuffle(idx)\n\n# Uses first 80 random indices for train\ntrain_idx = idx[:80]\n# Uses the remaining indices for validation\nval_idx = idx[80:]\n\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\n\n\n## Below shows a plot of the two sets of data.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxes[0].scatter(x_train, y_train, c = 'darkblue')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\naxes[0].set_title('Generated Data - Train')\naxes[1].scatter(x_val, y_val, c = 'darkred')\naxes[1].set_xlabel('x')\naxes[1].set_title('Generated Data - Validation')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMoving to PyTorch\nLet’s fit a linear regression model using PyTorch.\nFirst, we need to transform our data to PyTorch tensors.\n\n# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\nx_train_tensor = torch.tensor(x_train, dtype=torch.float)      \ny_train_tensor = torch.tensor(y_train, dtype=torch.float)\n\n\n# Here we can see the difference \nprint(type(x_train), type(x_train_tensor), x_train_tensor.type())\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'torch.Tensor'&gt; torch.FloatTensor\n\n\n\nInitialize Parameters and Require Gradients\n\n# Since we want to apply gradient descent on these parameters, we need\n# to set requires_grad = True\n\n### Initialize parameters... \na = torch.randn(1, requires_grad=True)      \nb = torch.randn(1, requires_grad=True)\n### ---------------------- \n\nprint(a, b)\n\ntensor([-1.0546], requires_grad=True) tensor([1.4521], requires_grad=True)\n\n\n\n\nGradient Computation with Autograd\nThe loss is:\n L(a, b) = \\frac{1}{n} \\sum_{i=1}^n (y_i - a- b\\cdot x_i)^2 \nThe gradients are:\n \\frac{\\partial L}{\\partial a} = -2 \\cdot \\frac{1}{n} \\sum_{i=1}^n (y_i - a - b\\cdot x_i)  \\frac{\\partial L}{\\partial b} = -2 \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i(y_i - a - b\\cdot x_i)\nWe won’t actually have to compute gradients - autograd is PyTorch’s automatic differentiation package, which will do it for us.\n\nThe backward() method helps us to compute partial derivatives of the loss function w.r.t. our parameters. It is essentially saying “do backpropagation”.\nWe obtain the computed gradients via the .grad attribute.\n\n\n# Specifying a learning rate\nlr = 1e-1\n\nyhat = a + b * x_train_tensor\nerror = y_train_tensor - yhat\nloss = (error ** 2).mean()\n\n# We just tell PyTorch to work its way BACKWARDS from the specified loss!\nloss.backward()\n# Let's check the computed gradients...\nprint('a grad (pytorch)', a.grad)\nprint('b grad (pytorch)', b.grad)\n\n# compare to actual gradients\nwith torch.no_grad():\n    a_grad = -2 * error.mean()\n    b_grad = -2 * (x_train_tensor * error).mean()\nprint('a grad (manual)', a_grad)\nprint('b grad (manual)', b_grad)\n\na grad (pytorch) tensor([-4.0241])\nb grad (pytorch) tensor([-0.7681])\na grad (manual) tensor(-4.0241)\nb grad (manual) tensor(-0.7681)\n\n\nNote: we use torch.no_grad() so PyTorch doesn’t keep track of the operations (otherwise, they may be included in the computation graph that PyTorch uses to calculate gradients).\nLet’s now take a gradient descent step.\n\n# update the parameters \nwith torch.no_grad():       \n    a -= lr * a.grad\n    b -= lr * b.grad\n# ---------------------- \n\n# PyTorch is \"clingy\" to its computed gradients, we need to tell it to zero out\na.grad.zero_()      # note the \"_\" which means \"in-place\"\nb.grad.zero_()\n\ntensor([0.])\n\n\nPutting it all together:\n\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    # No more manual computation of gradients! \n    # a_grad = -2 * error.mean()\n    # b_grad = -2 * (x_tensor * error).mean()\n\n    ### We just tell PyTorch to work its way BACKWARDS from the specified loss! \n    loss.backward()            \n    ### ---------------------- \n\n    # Let's check the computed gradients...\n    if epoch % 5 == 0:\n        print('Epoch:', epoch, '(a, b) grad', a.grad, b.grad)\n    \n    # Updating the parameters\n    with torch.no_grad():\n        a -= lr * a.grad\n        b -= lr * b.grad\n    \n    a.grad.zero_()\n    b.grad.zero_()\n    ### ---------------------- \n    \nprint('Final (a, b)', a, b)\n\nEpoch: 0 (a, b) grad tensor([-3.2369]) tensor([-0.6653])\nEpoch: 5 (a, b) grad tensor([-1.0961]) tensor([-0.2884])\nEpoch: 10 (a, b) grad tensor([-0.3738]) tensor([-0.1124])\nEpoch: 15 (a, b) grad tensor([-0.1280]) tensor([-0.0416])\nEpoch: 20 (a, b) grad tensor([-0.0440]) tensor([-0.0150])\nEpoch: 25 (a, b) grad tensor([-0.0151]) tensor([-0.0053])\nEpoch: 30 (a, b) grad tensor([-0.0052]) tensor([-0.0019])\nEpoch: 35 (a, b) grad tensor([-0.0018]) tensor([-0.0006])\nEpoch: 40 (a, b) grad tensor([-0.0006]) tensor([-0.0002])\nEpoch: 45 (a, b) grad tensor([-0.0002]) tensor([-7.8477e-05])\nEpoch: 50 (a, b) grad tensor([-7.3928e-05]) tensor([-2.7284e-05])\nEpoch: 55 (a, b) grad tensor([-2.5429e-05]) tensor([-9.3654e-06])\nEpoch: 60 (a, b) grad tensor([-9.0301e-06]) tensor([-3.1963e-06])\nEpoch: 65 (a, b) grad tensor([-3.0883e-06]) tensor([-1.1697e-06])\nEpoch: 70 (a, b) grad tensor([-1.0207e-06]) tensor([-5.7369e-07])\nEpoch: 75 (a, b) grad tensor([-5.3644e-07]) tensor([-5.5134e-07])\nEpoch: 80 (a, b) grad tensor([-5.3644e-07]) tensor([-5.5134e-07])\nEpoch: 85 (a, b) grad tensor([-5.3644e-07]) tensor([-5.5134e-07])\nEpoch: 90 (a, b) grad tensor([-5.3644e-07]) tensor([-5.5134e-07])\nEpoch: 95 (a, b) grad tensor([-5.3644e-07]) tensor([-5.5134e-07])\nFinal (a, b) tensor([1.0135], requires_grad=True) tensor([1.9413], requires_grad=True)\n\n\n\n\nUpdate All Parameters Simultaneously with Optimizer\nPreviously, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers, like SGD or Adam.\nAn optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.\nBesides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method and that’s it!\nIn the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters a and b. Note: we will use the whole dataset, not batches, so it is technically GD (not SGD). However, the optimizer in torch is still called SGD.\n\na = torch.randn(1, requires_grad=True)\nb = torch.randn(1, requires_grad=True)\nprint(f\"Initializations: a = {a.item():.3f}, b = {b.item():.3f}\")\n\nlr = 1e-1\nn_epochs = 1000\n\n### Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)\n### ---------------------- \n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    loss.backward()    \n    \n    # No more manual update!\n    # with torch.no_grad():\n    #     a -= lr * a.grad\n    #     b -= lr * b.grad\n\n    ### perform the update via step() ###\n    optimizer.step()\n    ### ---------------------- ###\n    \n    # No more telling PyTorch to let gradients go!\n    # a.grad.zero_()\n    # b.grad.zero_()\n\n    ### clearing the gradient ###\n    optimizer.zero_grad()\n    ### ---------------------- ###\n\nprint(f\"Output: a = {a.item():.3f}, b = {b.item():.3f}\")\n\nInitializations: a = 0.020, b = -1.185\nOutput: a = 1.013, b = 1.941\n\n\n\n\n\nModel Building\nPython is an object-oriented programming language.\nEvery item you interact with is an object. An object has a type, as well as:\n\nattributes (values)\nmethods (functions that can be applied to the object)\n\nAs an example, 'hello' in Python is an object of type str, with methods such as split, join etc.\nA Python class is a “blueprint” for an object.\nInheritance allows us to define a class that inherits all the methods and properties from another class.\n\nnn.Module Class\nIn PyTorch, a model is represented by a regular Python class that inherits from the nn.Module class. nn.Module is everywhere in PyTorch and represents mappings in neural networks.\nLet’s look at an example:\n\nclass ManualLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()              # inherits from nn.Module\n        # Initialization of a and b.\n        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n        self.a = nn.Parameter(torch.randn(1, requires_grad=True))\n        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n        \n    def forward(self, x):\n        # Computes the outputs / predictions\n        return self.a + self.b * x\n\n\nclass ManualLinearRegression(nn.Module)\n\nthis statement declares a class ManualLinearRegression which inherits from the base class nn.Module.\n\n\nIndented beneath the class statement are the methods of this class: __init__ and forward:\n\n__init__\n\nall classes have an __init__ which is executed when the class is instantiated\nself refers to an instance of the class\nin __init__, we have attached the parameters a and b as attributes\nsuper().__init__() is a call to the __init__ of nn.Module. For torch models, we will always be making this super() call as it is necessary for the model to be properly interpreted by torch.\n\nforward is called when the neural network is run on input data.\n\n\n# Now we can create a model \nmodel = ManualLinearRegression()\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # In PyTorch, models have a train() method which sets the model to training mode.\n    \n    ### enter training mode \n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n\n    ### this is the forward step \n    yhat = model(x_train_tensor)\n    ### ---------------------- \n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())\n\nOrderedDict({'a': tensor([-1.2859]), 'b': tensor([0.8673])})\nOrderedDict({'a': tensor([1.0135]), 'b': tensor([1.9413])})\n\n\n\n\nUtilizing PyTorch Layers\nFor simple tasks like building a linear regression, we could directly create a and b as nn.Parameters.\nInstead, we can use PyTorch’s nn.Linear to create a linear layer. Later, this will let us build more complicated networks. Note: nn.Linear automatically adds a bias term.\n\nclass PyTorchLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1) # arguments: (input dim, output dim)\n                \n    def forward(self, x):\n        return self.linear(x)\n\n\nfor name, param in PyTorchLinearRegression().named_parameters():\n    if param.requires_grad:\n        print(name, param.data)\n\nlinear.weight tensor([[0.8188]])\nlinear.bias tensor([-0.6576])\n\n\n\n# Now we can create a model \nmodel = PyTorchLinearRegression()\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # In PyTorch, models have a train() method which sets the model to training mode.\n    \n    ### enter training mode ###\n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n\n    ### this is the forward step ###\n    yhat = model(x_train_tensor)\n    ### ---------------------- ###\n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())\n\nOrderedDict({'linear.weight': tensor([[-0.3032]]), 'linear.bias': tensor([0.1377])})\nOrderedDict({'linear.weight': tensor([[1.9413]]), 'linear.bias': tensor([1.0135])})",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#xor-problem",
    "href": "lec-2/lecture-2.html#xor-problem",
    "title": "Lecture 2",
    "section": "XOR problem",
    "text": "XOR problem\nThe “XOR” (or “exclusive OR”) problem is often used to illustrate the ability of neural networks to fit complicated functions. The XOR problem has a checkerboard structure:\n\nn = 5000\np = 2\nx = np.random.uniform(-2, 2, size=(n, p))\n\n#x[,1] &lt;- first column (R)\n#x[:, 0] &lt;- first column (Python)\n\ny = ((x[:,0] &lt; 0) & (x[:, 1] &gt; 0)).astype(x.dtype) + ((x[:,0] &gt; 0) & (x[:, 1] &lt; 0)).astype(x.dtype)\n\ndf = np.hstack([y.reshape((n, 1)), x])\ndf = pd.DataFrame(df, columns = ['y', 'x1', 'x2'])\n\nsns.set_theme()\nxor_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nxor_plot.figure.subplots_adjust(top=.9)\n\n\n\n\n\n\n\n\nLogistic regression can only fit linear decision boundaries, and so fails the XOR problem.\n\n## logitstic regression doesn't work\nlog_fit = LogisticRegression()\nlog_fit.fit(x, y)\ncoeffs = log_fit.coef_[0]\ncoeff = -coeffs[0]/coeffs[1]\n\n## plot\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nplt.axline([0,0], slope=coeff)\n## title\nlogit_plot.figure.subplots_adjust(top=.9)\nlogit_plot.figure.suptitle(str(round(coeffs[0], 2)) + r'$x_1$ + ' + '(' + str(round(coeffs[1], 2)) + ')' + r'$x_2$')\n## fill in area\nx_fill = np.linspace(-2, 2, num=200)\ny_line = coeff * x_fill\nlogit_plot.ax.fill_between(x_fill, y_line, 2, color='blue', alpha=0.2)\nlogit_plot.ax.fill_between(x_fill, -2, y_line, color='orange', alpha=0.2)\nlogit_plot.ax.set_ylim(-2,2)\n\n\n\n\n\n\n\n\n\nMulti-Layer Neural Network\nHere is an example of a one hidden-layer neural network in torch named XORNet.\nInstead of a single linear layer, we have:\n\na linear layer\na ReLU activation\na linear layer\na sigmoid activation\n\nnn.Sequential allows us to stack these layers together - we store the final object as an attribute of the model: self.sequential.\nThe forward method then passes input data through self.sequential to get the output. That is:\n f(x_i) = g(W^{(2)}(a(W^{(1)}x_i + b^{(1)}))+b^{(2)})\n\nclass XORNet(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.sequential = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        y = self.sequential(x)\n        y = torch.flatten(y) # we do this to turn y into a one-dim tensor\n        return y\n\n    def loss_fn(self, y, y_pred):\n        loss = y * torch.log(y_pred + 1e-8) + (1-y) * torch.log(1-y_pred + 1e-8)\n        output = -loss.sum()\n        return output\n\n\nhidden_dim=2\nmodel = XORNet(input_dim=p, hidden_dim=hidden_dim)\n\n\nmodel\n\nXORNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=2, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)\n\n\n\n\nData\nFor torch to read the data, it needs to be a torch.tensor type:\n\nx_train = torch.tensor(x, dtype=torch.float)\ny_train = torch.tensor(y, dtype=torch.float)\n\nWe combine x_train and y_train into a TensorDataset, a dataset recognizable by torch. TensorDataset stores the samples and their labels. It is a subclass of the more general torch.utils.data.Dataset, which you can customize for non-standard data.\n\ntrain_data = TensorDataset(x_train, y_train)\n\nTensorDataset is helpful as it can be passed to DataLoader(). DataLoader wraps an iterable around the Dataset class. This lets us loop over the DataLoader to extract a mini-batch at each iteration.\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nTo retrieve a sample mini-batch, the following will return a list containing two tensors: one for the features, another one for the labels.\n\nnext(iter(train_loader))\n\n[tensor([[ 0.1671,  0.9133],\n         [ 1.1131,  1.9524],\n         [ 0.1303,  1.1588],\n         [-1.6487,  0.9334],\n         [ 1.3993, -1.5713],\n         [-0.7675,  1.2093],\n         [ 0.8532, -0.7282],\n         [ 0.3083, -1.5821],\n         [-0.3590,  0.9267],\n         [-0.7055, -0.4675]]),\n tensor([0., 0., 0., 1., 1., 1., 1., 1., 1., 0.])]\n\n\n\n\nTraining\nWe now set up the optimizer for training. We use Adam, and a base learning rate of lr=0.01. We set the number of epochs to 100. (Rule of thumb: pick largest lr that still results in convergence)\n\nlr = 0.01\nepochs = 100\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nXORNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=2, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)\n\n\nNow we train the model:\n\nfor epoch in range(epochs):\n    \n    # in each epoch, iterate over all batches of data (easily accessed through train_loader)\n    l = 0\n\n    for x_batch, y_batch in train_loader:\n\n        pred = model(x_batch)                   # this is the output from the forward function\n        loss = model.loss_fn(y_batch, pred)     # calculate loss function\n\n        loss.backward()                         # computes gradients wrt loss function\n        optimizer.step()                        # updates parameters \n        optimizer.zero_grad()                   # set the gradients back to zero (otherwise grads are accumulated)\n\n        l += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 2.9e+03\nepoch:  10 loss: 1.8e+03\nepoch:  20 loss: 1.79e+03\nepoch:  30 loss: 1.79e+03\nepoch:  40 loss: 1.79e+03\nepoch:  50 loss: 1.79e+03\nepoch:  60 loss: 1.78e+03\nepoch:  70 loss: 1.8e+03\nepoch:  80 loss: 1.79e+03\nepoch:  90 loss: 1.79e+03\n\n\nTo visualize the end result, we plot the predicted values over the whole space (the decision surface).\n\nx1 = np.arange(-2, 2, 0.05)\nx2 = np.arange(-2, 2, 0.05)\n\nx_test_np = np.array([(i, j) for i in x1 for j in x2])\ny_test_np = ((x_test_np[:,0] &lt; 0) & (x_test_np[:, 1] &gt; 0)).astype(x_test_np.dtype) + ((x_test_np[:,0] &gt; 0) & (x_test_np[:, 1] &lt; 0)).astype(x_test_np.dtype)\n\nx_test = torch.tensor(x_test_np, dtype=torch.float)\ny_test = torch.tensor(y_test_np)\n\n\nmodel.eval()\ny_pred = model(x_test)\n\ny_pred_np = y_pred.detach().numpy()\ny_pred_np = y_pred_np.reshape(x1.shape[0], x2.shape[0])\n\nseaborn_cols = sns.color_palette(\"tab10\")\ncols = [seaborn_cols[int(i)] for i in y]\n\ncustom_cmap = sns.diverging_palette(220, 50, s=70, l=70, as_cmap=True)\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(4, 4)\nax.contourf(x1, x2, y_pred_np, cmap=custom_cmap)\nax.scatter(x[0:100,0], x[0:100,1], c=cols[0:100])\n\n\n\n\n\n\n\n\n\nmodel.sequential[0].weight\n\nParameter containing:\ntensor([[2.8385, 2.7745],\n        [1.2609, 1.2796]], requires_grad=True)\n\n\n\nmodel.sequential[0].bias\n\nParameter containing:\ntensor([-0.0512,  3.6462], requires_grad=True)\n\n\n\nmodel.sequential[2].weight\n\nParameter containing:\ntensor([[-2.1883,  2.3246]], requires_grad=True)\n\n\nPlay around with different sizes of hidden_dim and see the difference!\n\nmodel.sequential[2].bias\n\nParameter containing:\ntensor([-4.8330], requires_grad=True)",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#mnist-example",
    "href": "lec-2/lecture-2.html#mnist-example",
    "title": "Lecture 2",
    "section": "MNIST example",
    "text": "MNIST example\nWe use torchvision.datasets to download the MNIST data.\n\n(mnist_train,\n mnist_test) = [torchvision.datasets.MNIST(root='data',\n                      train=train,\n                      download=True,\n                      transform=torchvision.transforms.ToTensor())\n                for train in [True, False]]\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    train_image, label = mnist_train[i]\n    plt.imshow(train_image[0], cmap=plt.cm.binary)\n    plt.xlabel(label)\nplt.show()\n\n\n\n\n\n\n\n\nSet up our dataloaders.\n\ntrain_loader = DataLoader(dataset=mnist_train, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=mnist_test, batch_size=10000, shuffle=False)\n\nLet’s define our neural network for the MNIST classification problem.\n\nclass MNISTNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            nn.Linear(28*28, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.Softmax()\n        )\n\n    def forward(self, x):\n        prob = self.layers(x)\n        return prob\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nInstantiate our model:\n\nmodel = MNISTNet()\n\nTrain our model:\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    l = 0\n    for x_batch, y_batch in train_loader:\n\n        x_batch = x_batch.reshape(x_batch.shape[0], 28*28)\n        y_batch = F.one_hot(y_batch, num_classes=10)\n        y_pred = model(x_batch)\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        l += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 2.64e+02\nepoch:  10 loss: 13.6\nepoch:  20 loss: 5.8\nepoch:  30 loss: 8.25\nepoch:  40 loss: 4.09\n\n\nCalculate our accuracy:\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\nx_batch = x_batch.reshape(x_batch.shape[0], 28 * 28)\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\nLet’s look at some interesting results (code adapted from here)\n\n# find interesting test images\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = range(8)\ninds2 = errors[:8]\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = mnist_test[n]\n    plt.imshow(image[0], cmap=plt.cm.binary)\n    plt.xlabel(label)\n    predicted_label = y_pred[n]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"truth={}, pred={}, score={:2.0f}%\".format(\n        label,\n        predicted_label,\n        100 * np.max(pred_array[n])),\n        color=color)\n\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    thisplot = plt.bar(range(10), pred_array[n], color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(pred_array[n])\n    thisplot[predicted_label].set_color('red')\n    thisplot[label].set_color('blue')",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#heteroskedastic-regression",
    "href": "lec-2/lecture-2.html#heteroskedastic-regression",
    "title": "Lecture 2",
    "section": "Heteroskedastic regression",
    "text": "Heteroskedastic regression\nThis code is adapted from here.\n\n# Make data\n\nx_range = [-20, 60]  # test\nx_ranges = [[-20, 60]]\nns = [1000]\n\ndef load_dataset():\n    def s(x):  # std of noise\n        g = (x - x_range[0]) / (x_range[1] - x_range[0])\n        return 0.25 + g**2.0\n\n    x = []\n    y = []\n    for i in range(len(ns)):\n        n = ns[i]\n        xr = x_ranges[i]\n        x1 = np.linspace(xr[0], xr[1], n)\n        eps = np.random.randn(n) * s(x1)\n        y1 = (1 * np.sin(0.2 * x1) + 0.1 * x1) + eps\n        x = np.concatenate((x, x1))\n        y = np.concatenate((y, y1))\n    # print(x.shape)\n    x = x[..., np.newaxis]\n    n_test = 150\n    x_test = np.linspace(*x_range, num=n_test).astype(np.float32)\n    x_test = x_test[..., np.newaxis]\n    return y, x, x_test\n\ny, x, x_test = load_dataset()\n\nDefine neural network\n\nclass HetNet(nn.Module):\n\n    def __init__(self, input_dim, output_dim, hidden_dims, mean_dims, var_dims):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dims\n        self.mean_dims = mean_dims\n        self.var_dims = var_dims\n\n        # create backbone\n        current_dim = input_dim\n        self.layers = nn.ModuleList()\n        for i in range(len(hidden_dims)):\n            hdim = hidden_dims[i]\n            self.layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n\n        # create heads\n        core_dim = hidden_dims[-1]\n        current_dim = core_dim\n        self.mean_layers = nn.ModuleList()\n        for i in range(len(mean_dims)):\n            hdim = mean_dims[i]\n            self.mean_layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n        self.mean_layers.append(nn.Linear(current_dim, output_dim))\n\n        current_dim = core_dim\n        self.var_layers = nn.ModuleList()\n        for i in range(len(var_dims)):\n            hdim = var_dims[i]\n            self.var_layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n        self.var_layers.append(nn.Linear(current_dim, output_dim))\n\n    def core_net(self, x):\n        for layer in self.layers:\n            x = F.relu(layer(x))\n        return x\n\n    def mean_net(self, x):\n        for layer in self.mean_layers[:-1]:\n            x = F.relu(layer(x))\n        x = self.mean_layers[-1](x)\n        return x\n\n    def var_net(self, x):\n        for layer in self.var_layers[:-1]:\n            x = F.relu(layer(x))\n        x = self.var_layers[-1](x)\n        return x\n\n    def forward(self, x):\n        mean = self.mean_net(self.core_net(x))\n        log_var = self.var_net(self.core_net(x))\n\n        return mean, log_var\n\n    def loss_fn(self, x, y):\n        mean, log_var = self.forward(x)\n        var = torch.exp(log_var)\n\n        loss = torch.pow(y-mean, 2) / var + log_var\n        out = loss.mean()\n\n        return out\n\nSet up data\n\nx_train = torch.tensor(x, dtype=torch.float)\ny_train = torch.tensor(y, dtype=torch.float)\ny_train = y_train.unsqueeze(-1)\n\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nInitialize model\n\nhidden_dims = [50, 50]\nmean_dims = [20]\nvar_dims = [20]\nmodel = HetNet(input_dim=1, output_dim=1, hidden_dims=hidden_dims, mean_dims=mean_dims, var_dims=var_dims)\n\nTrain\n\nlr = 0.001\nepochs = 500\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n\n    l = 0\n    for x_batch, y_batch in train_loader:\n\n        loss = model.loss_fn(x_batch, y_batch)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        l += loss.item()\n\n    if epoch % 50 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 8.67e+02\nepoch:  50 loss: 11.2\nepoch:  100 loss: -2.94\nepoch:  150 loss: -18.8\nepoch:  200 loss: -25.0\nepoch:  250 loss: -27.9\nepoch:  300 loss: -30.7\nepoch:  350 loss: -32.4\nepoch:  400 loss: -31.6\nepoch:  450 loss: -30.8\n\n\nPlot results\n\nmodel.eval()\nmean, log_var = model(x_train)\nsd = torch.exp(0.5 * log_var)\nmean_np = mean.detach().numpy()\nsd_np = sd.detach().numpy()\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(6, 4)\nax.plot(x, y, '.', label=\"observed\")\nax.plot(x, mean_np, 'r-')\nax.plot(x, mean_np + 2 * sd_np, 'g-')\nax.plot(x, mean_np - 2 * sd_np, 'g-')",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#fall-2025",
    "href": "index.html#fall-2025",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "Getting started",
    "text": "Getting started\nSee here for details about how to get started with python, VSCode and PyTorch."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "Lectures",
    "text": "Lectures\nThe code from the lectures is in the left sidebar.\nLecture slides and information about homeworks, office hours etc. can be found on Canvas."
  },
  {
    "objectID": "getting-started/getting-started.html",
    "href": "getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#anaconda-installation",
    "href": "getting-started/getting-started.html#anaconda-installation",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#managing-packages",
    "href": "getting-started/getting-started.html#managing-packages",
    "title": "Getting started",
    "section": "Managing packages",
    "text": "Managing packages\nThere are many open source Python packages for statistics and machine learning.\nTo download packages, two popular package managers are Conda and Pip. Both Conda and Pip come with the Anaconda distribution.\nConda is a general-purpose package management system, designed to build and manage software of any type from any language. This means conda can take advantage of many non-python packages (like BLAS, for linear algebra operations).\nPip is a package manager for python. You may see people using pip with environments using virtualenv or venv.\nWe recommend:\n\nuse a conda environment\nwithin this environment, use conda to install base packages such as pandas and numpy\nif a package is not available via conda, then use pip\n\nSee here for some conda vs pip misconceptions, and why conda is helpful.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#environments",
    "href": "getting-started/getting-started.html#environments",
    "title": "Getting started",
    "section": "Environments",
    "text": "Environments\n\nAbout\nIt is good coding practice to use virtual environments with Python. From this blog:\n\nA Python virtual environment consists of two essential components: the Python interpreter that the virtual environment runs on and a folder containing third-party libraries installed in the virtual environment. These virtual environments are isolated from the other virtual environments, which means any changes on dependencies installed in a virtual environment don’t affect the dependencies of the other virtual environments or the system-wide libraries. Thus, we can create multiple virtual environments with different Python versions, plus different libraries or the same libraries in different versions.\n\n\n\n\nCreating an environment for MSDS-534\nWe recommend creating a virtual environment for your MSDS-534 coding projects. This way, you can have an environment with all the necessary packages and you can easily keep track of what versions of the packages you used.\n\nOpen Terminal (macOS) or a shell\nCreate an environment called msds534 using Conda with the command: conda create --name msds534\nTo install packages in your environment, first activate your environment: conda activate msds534\nThen, install the following packages using the command: conda install numpy pandas scikit-learn matplotlib seaborn jupyter ipykernel\nInstall PyTorch by running the appropriate command from here (for macOS, the command is: pip3 install torch torchvision)\nTo exit your environment: conda deactivate\n\nHere is a helpful cheatsheet for conda environment commands.\nFor more details about the shell / bash, here is a helpful resource.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#vscode",
    "href": "getting-started/getting-started.html#vscode",
    "title": "Getting started",
    "section": "VSCode",
    "text": "VSCode\nThere are a number of Python IDEs (integrated development environments). In class, we will be using VSCode (download here).\n\nDownload lecture-1.ipynb here and open it in VSCode.\nTo use your msds534 environment, on the top right hand corner, click “Select Kernel” &gt; “Python Environments” &gt; msds534. If it prompts you to install ipykernel, follow the prompts to install it.\n\nJupyter notebooks (.ipynb files) are useful to combine code cells with text (as markdown cells).\nVSCode also has a Python interactive window (details here).",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#learning-python",
    "href": "getting-started/getting-started.html#learning-python",
    "title": "Getting started",
    "section": "Learning Python",
    "text": "Learning Python\nIn this class, we will assume some familiarity with:\n\nNumpy\nPandas\nObject-oriented programming",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  }
]