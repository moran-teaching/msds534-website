[
  {
    "objectID": "lec-8/lecture-8-caveat.html",
    "href": "lec-8/lecture-8-caveat.html",
    "title": "Caveat for Conformal Inference",
    "section": "",
    "text": "This simulation shows that we only get (1-\\alpha)-coverage ON AVERAGE over the calibration set. That is, if we fix the calibration set, our coverage is not guaranteed.\nBelow, we see the distribution of coverages for a fixed calibration set of size 10.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import beta\n\n# Parameters\nN = 10  # Total number of variables\nnprob = 1000  # samples to estimate coverage probability\nndraw = 5000 # estimating beta distribution\n\nalpha = 0.1\n\npreds10 = np.zeros((ndraw, nprob))\n\nfor i in range(ndraw):\n\n    preds = []\n    # fixed calibration set\n    observed_variables = np.random.normal(0, 1, N)\n    # quantile of fixed calibration set\n    qhat = np.quantile(observed_variables, np.ceil((1-alpha) * (N+1)) / N)\n\n    for j in range(nprob):\n\n        # Generate the N-th random variable\n        new_variable = np.random.normal(0, 1, 1)\n        # is this variable in the prediction set???\n        preds10[i, j] = (new_variable &lt; qhat)\n\n/var/folders/f0/m7l23y8s7p3_0x04b3td9nyjr2hyc8/T/ipykernel_4161/1945614140.py:27: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  preds10[i, j] = (new_variable &lt; qhat)\n\n\n\n# each row corresponds to a fixed calibration set\nplt.hist(preds10.mean(1))\n\n(array([2.000e+00, 7.000e+00, 1.200e+01, 3.000e+01, 7.800e+01, 1.520e+02,\n        3.680e+02, 6.730e+02, 1.265e+03, 2.413e+03]),\n array([0.39 , 0.451, 0.512, 0.573, 0.634, 0.695, 0.756, 0.817, 0.878,\n        0.939, 1.   ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\nIf you average over the calibration sets, we recover (1-\\alpha) coverage.\n\npreds10.mean()\n\n0.9108314\n\n\nIf we increase the size of our calibration set, the coverage probabilities are closer to (1-\\alpha) (for a fixed calibration set.)\n\n# Parameters\nN = 50  # Total number of variables\nnprob = 1000  # Number of simulations\nndraw = 5000\n\nalpha = 0.1\n\npreds50 = np.zeros((ndraw, nprob))\n\nfor i in range(ndraw):\n\n    preds = []\n\n    observed_variables = np.random.normal(0, 1, N)\n\n    qhat = np.quantile(observed_variables, np.ceil((1-alpha) * (N+1)) / N)\n\n    for j in range(nprob):\n\n        # Generate the N-th random variable\n        new_variable = np.random.normal(0, 1, 1)\n\n        preds50[i, j] = (new_variable &lt; qhat)\n\n/var/folders/f0/m7l23y8s7p3_0x04b3td9nyjr2hyc8/T/ipykernel_4161/3492330592.py:23: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  preds50[i, j] = (new_variable &lt; qhat)\n\n\n\n# each row corresponds to a fixed calibration set\nplt.hist(preds50.mean(1))\n\n\n# Parameters\nN = 1000  # Total number of variables\nnprob = 1000  # Number of simulations\nndraw = 5000\n\nalpha = 0.1\n\npreds1000 = np.zeros((ndraw, nprob))\n\nfor i in range(ndraw):\n\n    preds = []\n\n    observed_variables = np.random.normal(0, 1, N)\n\n    qhat = np.quantile(observed_variables, np.ceil((1-alpha) * (N+1)) / N)\n\n    for j in range(nprob):\n\n        # Generate the N-th random variable\n        new_variable = np.random.normal(0, 1, 1)\n\n        preds1000[i, j] = (new_variable &lt; qhat)\n\n/var/folders/f0/m7l23y8s7p3_0x04b3td9nyjr2hyc8/T/ipykernel_4161/2919383409.py:23: DeprecationWarning: Conversion of an array with ndim &gt; 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n  preds1000[i, j] = (new_variable &lt; qhat)\n\n\n\nplt.figure(figsize=(4,3))\nplt.hist(preds10.mean(axis=1),density=True,alpha=0.5, label=r'$N_2=10$')\nplt.hist(preds50.mean(axis=1),density=True,alpha=0.5, label=r'$N_2=50$')\nplt.hist(preds1000.mean(axis=1),density=True,alpha=0.5, label=r'$N_2=500$')\nplt.legend()\n\n\n\n\n\n\n\n\n\nprint(preds10.mean())\nprint(preds50.mean())\nprint(preds1000.mean())\n\n0.9061804\n0.9037104\n0.902485\n\n\nWhy does this happen? With a smaller, fixed calibration set, we get worse approximations to the distribution, and the ranks are not uniform!!!\n\n# Parameters\nN = 10  \nndraw = 5000\n\nrank = np.zeros((ndraw))\nobserved_variables = np.random.normal(0, 1, N-1)\n\nfor i in range(ndraw):\n    \n    new_variable = np.random.normal(0, 1, 1)\n\n    variables = np.concatenate([observed_variables,new_variable])\n    variables_rank = variables.argsort().argsort() + 1\n    rank[i] = variables_rank[-1]\n\n\nplt.hist(rank)\n\n(array([ 365.,  420.,  167.,   50., 1575.,  141.,  969.,  270.,  357.,\n         686.]),\n array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1, 10. ]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\nIn contrast, the ranks are uniform when we re-draw the calibration set for each new variable (i.e. average over calibration set).\n\n# Parameters\nN = 10  \nndraw = 5000\n\nrank = np.zeros((ndraw))\n\nfor i in range(ndraw):\n    \n    observed_variables = np.random.normal(0, 1, N-1)\n    new_variable = np.random.normal(0, 1, 1)\n\n    variables = np.concatenate([observed_variables,new_variable])\n    variables_rank = variables.argsort().argsort() + 1\n    rank[i] = variables_rank[-1]\n\n\nplt.hist(rank)\n\n(array([499., 481., 515., 521., 497., 502., 524., 454., 512., 495.]),\n array([ 1. ,  1.9,  2.8,  3.7,  4.6,  5.5,  6.4,  7.3,  8.2,  9.1, 10. ]),\n &lt;BarContainer object of 10 artists&gt;)",
    "crumbs": [
      "Home",
      "Lecture 8 - Conformalized Classification",
      "Caveat for Conformal Inference"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html",
    "href": "lec-1/lecture-1.html",
    "title": "Lecture 1 - Intro",
    "section": "",
    "text": "# import packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker, cm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.interpolate import splrep, BSpline\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.pyplot import subplots\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#linear-regression-example",
    "href": "lec-1/lecture-1.html#linear-regression-example",
    "title": "Lecture 1 - Intro",
    "section": "Linear Regression example",
    "text": "Linear Regression example\nSpecify parameters\n\nn = 100\np = 1\nbeta = 3\nsigma = 1\n\nGenerate data\n\nx = np.random.normal(size=(n, p))\ny = x * beta + np.random.normal(size=(n, 1)) * sigma\n\ncolnames = ['x' + str(i) for i in range(1, p+1)]\ncolnames.insert(0, 'y')\n\ndf = pd.DataFrame(np.hstack((y, x)), columns = colnames)\n\nFit linear regression model using sklearn\n\nlm = LinearRegression()\nlm.fit(x, y)\ny_hat = lm.predict(x)\nresid = y - y_hat\n\nPlot x vs. y using seaborn\n\nsns.set_theme()\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0])\n\n\n\n\n\n\n\n\nPlot x vs. y, including residual distances\n\ny_min = np.minimum(y, y_hat)\ny_max = np.maximum(y, y_hat)\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0])\nlm_plot.ax.vlines(x=list(x[:,0]), ymin=list(y_min[:,0]), ymax=list(y_max[:,0]), color = 'red', alpha=0.5)",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#overfitting-example",
    "href": "lec-1/lecture-1.html#overfitting-example",
    "title": "Lecture 1 - Intro",
    "section": "Overfitting example",
    "text": "Overfitting example\n\nsort_ind = np.argsort(x, axis=0)\nxsort = np.take_along_axis(x, sort_ind, axis=0)\nysort = np.take_along_axis(y, sort_ind, axis=0)\ntck = splrep(xsort, ysort, s=20)\n\nxspline = np.arange(x.min(), x.max(), 0.01)\nyspline = BSpline(*tck)(xspline)\n\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3.5, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0], label = \"Linear Regression\")\nplt.plot(xspline, yspline,color='orange', label = \"Spline\")\nplt.legend(loc='upper left')",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#shrinkage-plot",
    "href": "lec-1/lecture-1.html#shrinkage-plot",
    "title": "Lecture 1 - Intro",
    "section": "Shrinkage plot",
    "text": "Shrinkage plot",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#lasso-example",
    "href": "lec-1/lecture-1.html#lasso-example",
    "title": "Lecture 1 - Intro",
    "section": "Lasso example",
    "text": "Lasso example\nThis code is adapted from ISLP labs.\n\nn = 100\np = 90\nbeta = np.zeros(p)\nbeta[0]=3\nbeta[1]=3\ncov = 0.6 * np.ones((p, p))\nnp.fill_diagonal(cov, 1)\nx = np.random.multivariate_normal(mean=np.zeros(p), cov=cov, size=n)\ny = np.matmul(x, beta) + np.random.normal(size=n)\n\nx_columns = ['x' + str(i+1) for i in range(p)]\n\n\n# set up cross-validation\nK=5\nkfold = sklearn.model_selection.KFold(K,random_state=0,shuffle=True)\n\n# function to standardize input\nscaler = StandardScaler(with_mean=True, with_std=True)\n\n\nlassoCV = sklearn.linear_model.ElasticNetCV(n_alphas=100, l1_ratio=1,cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),('lasso', lassoCV)])\npipeCV.fit(x, y)\ntuned_lasso = pipeCV.named_steps['lasso']\ntuned_lasso.alpha_\n\nlambdas, soln_array = sklearn.linear_model.Lasso.path(x, y,l1_ratio=1,n_alphas=100)[:2]\nsoln_path = pd.DataFrame(soln_array.T,columns=x_columns, index=-np.log(lambdas))\n\n\npath_fig, ax = subplots(figsize=(8,8))\nsoln_path.plot(ax=ax, legend=False)\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Standardized coefficiients', fontsize=20);\n\n\n\n\n\n\n\n\n\nlassoCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(tuned_lasso.alphas_),tuned_lasso.mse_path_.mean(1),yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#ridge-regression",
    "href": "lec-1/lecture-1.html#ridge-regression",
    "title": "Lecture 1 - Intro",
    "section": "Ridge regression",
    "text": "Ridge regression\nThis code is adapted from ISLP labs\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\nridgeCV = sklearn.linear_model.ElasticNetCV(alphas=lambdas, \n                           l1_ratio=0,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('ridge', ridgeCV)])\npipeCV.fit(x, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([1.84404932e+07, 1.46137755e+07, 1.15811671e+07, 9.17787690e+06,\n       7.27331049e+06, 5.76397417e+06, 4.56785096e+06, 3.61994377e+06,\n       2.86874353e+06, 2.27343019e+06, 1.80165454e+06, 1.42778041e+06,\n       1.13149156e+06, 8.96687712e+05, 7.10609677e+05, 5.63146016e+05,\n       4.46283587e+05, 3.53672111e+05,...\n       1.53096214e-01, 1.21326131e-01, 9.61488842e-02, 7.61963464e-02,\n       6.03843014e-02, 4.78535262e-02, 3.79231012e-02, 3.00534091e-02,\n       2.38168128e-02, 1.88744168e-02, 1.49576525e-02, 1.18536838e-02,\n       9.39384172e-03, 7.44445891e-03, 5.89960638e-03, 4.67533716e-03,\n       3.70512474e-03, 2.93624800e-03, 2.32692632e-03, 1.84404932e-03]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('ridge', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    ElasticNetCV?Documentation for ElasticNetCV\n        \n            \n                Parameters\n                \n\n\n\n\nl1_ratio \n0\n\n\n\neps \n0.001\n\n\n\nn_alphas \n'deprecated'\n\n\n\nalphas \narray([1.8440...84404932e-03])\n\n\n\nfit_intercept \nTrue\n\n\n\nprecompute \n'auto'\n\n\n\nmax_iter \n1000\n\n\n\ntol \n0.0001\n\n\n\ncv \nKFold(n_split... shuffle=True)\n\n\n\ncopy_X \nTrue\n\n\n\nverbose \n0\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\nrandom_state \nNone\n\n\n\nselection \n'cyclic'\n\n\n\n\n            \n        \n    \n\n\n\nlambdas, soln_array = sklearn.linear_model.ElasticNet.path(x, y,l1_ratio=0,alphas=lambdas)[:2]\nsoln_path = pd.DataFrame(soln_array.T,columns=x_columns, index=-np.log(lambdas))\n\npath_fig, ax = subplots(figsize=(8,8))\nsoln_path.plot(ax=ax, legend=False)\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Standardized coefficiients', fontsize=20);\n\n\n\n\n\n\n\n\n\ntuned_ridge = pipeCV.named_steps['ridge']\nridgeCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#logistic-regression-example",
    "href": "lec-1/lecture-1.html#logistic-regression-example",
    "title": "Lecture 1 - Intro",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nGenerate data\n\nn = 100\np = 2\nx = np.random.uniform(-2, 2, size=(n, p))\n\nbeta = np.array([2.5, -2.5])\nmu = np.matmul(x, beta)\nprob = 1/(1 + np.exp(-mu))\n\ny = np.zeros((n))\nfor i in range(n):\n    y[i] = np.random.binomial(1, prob[i], 1)[0]\n\ndf = np.hstack([y.reshape((n, 1)), x])\ndf = pd.DataFrame(df, columns = ['y', 'x1', 'x2'])\n\nsns.set_theme()\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nlogit_plot.figure.subplots_adjust(top=.9)\n\n\n\n\n\n\n\n\nFitting the logistic regression model\n\nlog_fit = LogisticRegression()\nlog_fit.fit(x, y)\ncoeffs = log_fit.coef_[0]\ncoeff = -coeffs[0]/coeffs[1]\n\nPlot x_1\\beta_1 + x_2\\beta_2 = 0\n\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nplt.axline([0,0], slope=coeff)\n## title\nlogit_plot.figure.subplots_adjust(top=.9)\nlogit_plot.figure.suptitle(str(round(coeffs[0], 2)) + r'$x_1$ - ' + str(round(-coeffs[1], 2)) + r'$x_2 = 0$')\n## fill in area\nx_fill = np.linspace(-2, 2, num=200)\ny_line = coeff * x_fill\nlogit_plot.ax.fill_between(x_fill, y_line, 2, color='blue', alpha=0.2)\nlogit_plot.ax.fill_between(x_fill, -2, y_line, color='orange', alpha=0.2)\n\nlogit_plot.ax.annotate(r'$\\bf P(Y=1)&lt;0.5$', (0.5, 2.1), color='blue')\nlogit_plot.ax.annotate(r'$\\bf P(Y=1)&gt;0.5$', (0.5, -2.2), color='darkorange')\n\nText(0.5, -2.2, '$\\\\bf P(Y=1)&gt;0.5$')\n\n\n\n\n\n\n\n\n\n\n# Create a meshgrid for x1 and x2\nx1_range = np.linspace(x[:, 0].min(), x[:, 0].max(), 200)\nx2_range = np.linspace(x[:, 1].min(), x[:, 1].max(), 200)\nX1, X2 = np.meshgrid(x1_range, x2_range)\n\n# Compute the sigmoid function using the fitted logistic regression coefficients\nZ = 1 / (1 + np.exp(-(log_fit.intercept_[0] + log_fit.coef_[0,0]*X1 + log_fit.coef_[0,1]*X2)))\n\nfig = plt.figure(figsize=(5, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Set background to white\nax.set_facecolor('white')\nfig.patch.set_facecolor('white')\n\n# Plot with smooth color transitions\nsurf = ax.plot_surface(X1, X2, Z, cmap='coolwarm', antialiased=True, linewidth=0, rstride=1, cstride=1)\n\nax.view_init(elev=15, azim=65+155)\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel(r'P(y=1)')\n\nax.set_title(\" \"*115) \nax.text2D(0.5, 0.91, r'g(' + str(round(coeffs[0], 2)) + r'$x_1$  ' +\n         str(round(coeffs[1], 2)) + r'$x_2)$',\n         transform=ax.transAxes, ha='center', va='top')\n\nText(0.5, 0.91, 'g(1.76$x_1$  -1.8$x_2)$')",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#principal-components-analysis",
    "href": "lec-1/lecture-1.html#principal-components-analysis",
    "title": "Lecture 1 - Intro",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\npenguins = sns.load_dataset(\"penguins\")\n\nfig = plt.figure()\n\nax = Axes3D(fig)\nfig.add_axes(ax)\n\ncmap = matplotlib.colors.ListedColormap(sns.color_palette(\"Paired\", 3))\n\ncols = penguins['species'].copy()\ncols[cols=='Adelie']=1\ncols[cols=='Chinstrap']=2\ncols[cols=='Gentoo']=3\n\nsc = ax.scatter3D(penguins['bill_depth_mm'],\n                penguins['bill_length_mm'],\n                penguins['flipper_length_mm'],\n                c = cols,\n                cmap=cmap,\n                alpha=1)\nax.set_xlabel('bill depth')\nax.set_ylabel('bill length')\nax.set_zlabel('flipper length')\nax.set_facecolor((1.0, 1.0, 1.0, 0.0))\n\n\n\n\n\n\n\n\n\nx = penguins[['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm']]\nx = x.dropna(axis=0)\n\npca_fit = PCA()\npca_fit.fit(x)\nz = pca_fit.transform(x)\n\nz_df = pd.DataFrame(z[:, 0:2], columns = ['z1', 'z2'])\nz_df['species']=penguins['species']\n\nsns.set_theme()\npca_plot = sns.relplot(z_df, x='z1', y='z2', hue='species', palette=sns.color_palette(\"Paired\", 3), height=4)\n\n\n\n\n\n\n\n\n\nPC_values = np.linspace(1,3,3).reshape(3,1)\nscree_df = np.hstack([PC_values, pca_fit.explained_variance_ratio_.reshape(3,1)])\nscree_df = pd.DataFrame(scree_df, columns = ['Principal Components', 'Explained Variance Ratio'])\nscree_plot = sns.relplot(scree_df, x='Principal Components', y='Explained Variance Ratio', marker='o', kind='line', height=4)",
    "crumbs": [
      "Home",
      "Lecture 1 - Intro"
    ]
  },
  {
    "objectID": "lec-10/lecture-10.html",
    "href": "lec-10/lecture-10.html",
    "title": "Lecture 10 - Multiple Testing",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy.stats import t\nfrom scipy.stats import ttest_1samp\n\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x1369eb170&gt;",
    "crumbs": [
      "Home",
      "Lecture 10 - Multiple Testing"
    ]
  },
  {
    "objectID": "lec-10/lecture-10.html#simple-example-a-coin-toss",
    "href": "lec-10/lecture-10.html#simple-example-a-coin-toss",
    "title": "Lecture 10 - Multiple Testing",
    "section": "Simple example: A coin toss",
    "text": "Simple example: A coin toss\nLet’s toss a coin 10 times.\n\nsample = np.random.binomial(10,0.5)\n\n\nsample\n\n3\n\n\nLet’s repeat this 10,000 times.\nHow many times will we see 10 heads and 0 tails?\n\nnsim = 10000\nnheads = np.zeros((nsim))\n\nfor i in range(nsim):\n    sample = np.random.binomial(10,0.5)\n    nheads[i] = sample\n\n\nplt.hist(nheads)\n\n(array([   8.,   69.,  466., 1193., 2022., 2436., 2080., 1197.,  433.,\n          96.]),\n array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.]),\n &lt;BarContainer object of 10 artists&gt;)\n\n\n\n\n\n\n\n\n\n\nsum(nheads == 0)\n\n10\n\n\nIf we do enough tests, we will see rare things, even when the null hypothesis is true!\nHow do we account for this?",
    "crumbs": [
      "Home",
      "Lecture 10 - Multiple Testing"
    ]
  },
  {
    "objectID": "lec-10/lecture-10.html#one-sample-t-tests-for-mean",
    "href": "lec-10/lecture-10.html#one-sample-t-tests-for-mean",
    "title": "Lecture 10 - Multiple Testing",
    "section": "One-sample t-tests for mean",
    "text": "One-sample t-tests for mean\n\nRun tests for p hypotheses\nBonferroni\nHolms\nFDR\n\n\nn = 100        # number of samples\np = 500         # number of features\n\nX = np.random.normal(size = (n, p))\n#h0 is true\n\n\nX[:, 0]\n\narray([-1.23144817, -1.6221481 ,  0.78530339,  0.19165089,  0.84894177,\n        1.06585683, -0.28432315, -0.29614501, -2.4748051 ,  0.47858428,\n       -0.24213327,  0.52410957,  0.23189972, -0.94610517, -1.29467143,\n        0.13072292, -0.2909665 , -0.39655049, -0.39069435,  1.73524628,\n       -0.12923568,  0.13841862,  0.49422539,  0.46360528,  0.03464935,\n        1.85822524, -0.19238505,  0.44577581, -0.05795199, -1.51849376,\n        0.30713414, -0.38423689, -0.18367048, -1.13872914,  0.42513157,\n        2.03316686,  0.63474446, -0.14888316,  1.21969421,  0.30649158,\n       -1.09807878, -0.33338102, -0.27322733,  0.13588905,  0.94608741,\n       -0.28643423, -0.66412568, -0.1883097 , -1.13789014, -0.6014846 ,\n       -0.53071217, -0.9349938 ,  0.29486146,  1.72115537, -1.73645224,\n       -1.71836627, -1.33991474,  2.04787357,  0.61984414, -0.32908192,\n       -0.63547235, -0.61090382, -1.4197087 , -0.90519644, -0.71474274,\n       -0.24549478,  0.95192999, -0.59069714, -0.62842783, -1.42996909,\n        1.27005003,  0.45029865, -0.14369338,  0.78588132,  1.8973945 ,\n       -0.13167774, -1.19428275,  1.13671073,  0.47600343,  1.15980724,\n       -0.40547833, -2.00610012, -0.65188175, -1.61280317,  0.07945569,\n        0.51100756,  0.37331791,  0.29070579, -0.54083063, -0.24666013,\n       -1.34339946,  0.59813569, -0.30900759, -0.81160593,  1.32828321,\n       -0.94538251,  0.54760281,  0.39206287,  0.61582615, -2.07603177])\n\n\n\nplt.scatter(range(p), X.mean(axis=0))\n\n\n\n\n\n\n\n\n\ndef ttest_1(x, h0_mean=0):\n\n    df = n-1\n\n    mean = x.mean() # sample mean x_bar\n    d = mean - h0_mean # x_bar - mu   (mu=0 under H_0)\n    v = np.var(x) # sample variance\n    denom = np.sqrt(v / n) # variance of sample mean\n    tstat = np.abs(d / denom)\n    # xmean - h0_mean / (sqrt(var/n))\n\n    # our test-stat is a t distributed random variable with n-1 degrees of freedom\n\n    pval = t.cdf(-tstat, df = df) + (1 - t.cdf(tstat, df = df)) \n\n    # pval - probability in the lower and upper tails of our t distribution\n\n    return pval\n\n\npvals = np.zeros((p))\nfor j in range(p):\n    pvals[j] = ttest_1samp(X[:, j], 0).pvalue\n\n\nplt.scatter(range(p), pvals)\nplt.axhline(y=0.05, color='r', linestyle='--')\n\n\n\n\n\n\n\n\n\n# no multiple testing correction\n# p = 500 (testing 500 hypotheses)\n# we expect to reject 500 * 0.05 = 25\nalpha  = 0.05\nnmp = np.where(pvals &lt; alpha)[0]\nprint(\"No multiple testing correction: reject \", nmp.shape[0])\n\n# bonferroni\nbf = np.where(pvals &lt; alpha/p)[0]\nprint(\"Bonferroni: reject \", bf.shape[0])\n\n# holms\nord_pvals = np.argsort(pvals)\nholms = []\nfor j, s in enumerate(ord_pvals):\n    #j = 0, s is index of smallest p-val\n    denom = p - j\n    if pvals[s] &lt;= (alpha/denom):\n        holms.append(s)\n    else:\n        break\nprint(\"Holms: reject \", len(holms))\n\n# FDR, BH procedure # this is different from holms and bonferroni in that \n# we control FDR, not FWER\nq = 0.05\nbh = []\nfor j, s in enumerate(ord_pvals):\n    val = q * (j + 1) /p # zero indexing\n    if pvals[s] &lt;= val:\n        bh.append(s)\n    else:\n        break\n\nprint(\"Benjamini-Hochberg: reject \", len(bh))\n\nNo multiple testing correction: reject  21\nBonferroni: reject  0\nHolms: reject  0\nBenjamini-Hochberg: reject  0\n\n\n\ntrue_mean = np.array([1.0] * int(p/10) + [0] * int(p * 9/10))\n\nX = np.random.normal(size = (n, p))\nX = X + true_mean\n\nplt.scatter(range(p), X.mean(axis=0))\n\n\n\n\n\n\n\n\n\npvals = np.zeros((p))\nfor j in range(p):\n    pvals[j] = ttest_1samp(X[:, j], 0).pvalue\nplt.scatter(range(p), pvals)\nplt.axhline(y=0.05, color='r', linestyle='--')\n\n\n\n\n\n\n\n\n\nprint('Number of hypotheses we should reject: ', int(p/10))\n\n# no multiple testing correction\nalpha  = 0.05\nnmp = np.where(pvals &lt; alpha)[0]\nprint(\"No multiple testing correction: reject \", nmp.shape[0])\n\n# bonferroni\nbon = np.where(pvals &lt; alpha/p)[0]\nprint(\"Bonferroni: reject \", bon.shape[0])\n\n# holms\nord_pvals = np.argsort(pvals)\nholms = []\nfor j, s in enumerate(ord_pvals):\n    denom = p - j\n    if pvals[s] &lt;= (alpha/denom):\n        holms.append(s)\n    else:\n        break\nprint(\"Holms: reject \", len(holms))\n\n# FDR, BH procedure\nq = 0.05\nbh = []\nfor j, s in enumerate(ord_pvals):\n    val = q * (j + 1) /p # zero indexing\n    if pvals[s] &lt;= val:\n        bh.append(s)\n    else:\n        break\n\nprint(\"Benjamini-Hochberg: reject \", len(bh))\n\nNumber of hypotheses we should reject:  50\nNo multiple testing correction: reject  74\nBonferroni: reject  50\nHolms: reject  50\nBenjamini-Hochberg: reject  55",
    "crumbs": [
      "Home",
      "Lecture 10 - Multiple Testing"
    ]
  },
  {
    "objectID": "lec-11/lecture-11.html",
    "href": "lec-11/lecture-11.html",
    "title": "Lecture 11 - Conditional Tests",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy.stats import t\nfrom sklearn.linear_model import LinearRegression\nimport pandas as pd\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom typing import Callable, Tuple\nfrom tqdm import tqdm\nfrom sklearn import datasets, linear_model\nimport statsmodels.api as sm\nfrom scipy import stats",
    "crumbs": [
      "Home",
      "Lecture 11 - Conditional Tests"
    ]
  },
  {
    "objectID": "lec-11/lecture-11.html#variable-exclusion-test",
    "href": "lec-11/lecture-11.html#variable-exclusion-test",
    "title": "Lecture 11 - Conditional Tests",
    "section": "Variable Exclusion Test",
    "text": "Variable Exclusion Test\nLet’s consider a nonlinear regression problem. Our data is (y_i, x_{i1},x_{i2},\\dots, x_{ip}), i=1,\\dots, n.\nThe true model is:\ny_i = 5\\sin(x_{i1}) + \\varepsilon_i, \\quad \\varepsilon_i\\sim t_{100}.\nThat is, only one of the p predictors is important for predicting y.\n\nn = 1000\np = 50\nx = np.random.uniform(-10,10, size=(n, p))\ny = 5 * np.sin(x[:, 0]) + t.rvs(100, size=n)\n\nHere is the plot of y_i vs. x_{i1}.\n\nplt.scatter(x[:, 0], y)\n\n\n\n\n\n\n\n\nHere is the plot of y_i vs. x_{i2} (which has no relationship!)\n\nplt.scatter(x[:, 1], y)\n\n\n\n\n\n\n\n\nLet’s fit a linear regression to this data.\n\nlm = LinearRegression()\nlm.fit(x, y)\ny_hat = lm.predict(x)\nresid = y - y_hat\n\nOur coefficients are very small, as we may expect (fitting a linear function to nonlinear data).\n\nprint('Intercept:', round(lm.intercept_, 3),'\\n', 'Slope:', round(lm.coef_[0],3))\n\nIntercept: -0.183 \n Slope: 0.13\n\n\n\nplt.scatter(x[:,0], y)\nplt.axline((0,lm.intercept_), slope=lm.coef_[0])\n\n\n\n\n\n\n\n\nLet’s train a nonlinear model instead (gradient boosted trees, XGBoost).\n\nn_train = int(n/2)\nx_train = x[:n_train]\nx_test = x[n_train:]\ny_train = y[:n_train]\ny_test = y[n_train:]\n\n\ndtrain = xgb.DMatrix(x_train, label=y_train)\ndtest = xgb.DMatrix(x_test, label=y_test)\n\nparams = {\n    'max_depth': 3,\n    'eta': 0.3,\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'nthread': 4\n}\n\n# Train the model (suppress training output)\nnum_rounds = 100\neval_list = [(dtrain, 'train'), (dtest, 'eval')]\nmodel_with = xgb.train(params, dtrain, num_rounds, eval_list, early_stopping_rounds=10, verbose_eval=False)\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages/xgboost/core.py:726: FutureWarning: Pass `evals` as keyword args.\n  warnings.warn(msg, FutureWarning)\n\n\n\n# make predictions\ny_pred_with = model_with.predict(dtest)\n# z\nresid_with = np.power(y_test - y_pred_with, 2)\n\n\nn_lin = 1000\nx_linspace = np.linspace(x_train.min(), x_train.max(), n_lin).reshape(n_lin, 1)\nx_linspace = np.hstack((x_linspace, np.zeros((n_lin, p-1))))\ny_linspace = model_with.predict(xgb.DMatrix(x_linspace))\nplt.scatter(x_test[:, 0], y_test)\nplt.plot(x_linspace[:, 0], y_linspace, c='red')\n\n\n\n\n\n\n\n\nWe will conduct a “Variable Exclusion Test” to test the contribution of x_{i1} in predicting y_i.\nOur null hypothesis is:\nH_0: \\mathbb{E}[y_i|x_{i1},\\dots, x_{ip}] = \\mathbb{E}[y_i|x_{i2},x_{i3},\\dots,x_{ip}]\nOn the training data, we train two models:\n\nf(x_{i1},x_{i2},\\dots, x_{ip}) to predict y_i\ng(x_{i2}, x_{i3},\\dots, x_{ip}) to predict y_i\n\nFor each i, we calculate:\n\nz_i = y_i - f(x_{i1},x_{i2},\\dots, x_{ip})\n\\widetilde{z}_i = y_i - g(x_{i2},x_{i3},\\dots, x_{ip})\n\nOur test-statistic is: T_{obs} = \\frac{1}{n}\\sum_{i=1}^n (z_i - \\widetilde{z}_i)\nTo get the null distribution of T, for b=1,\\dots, B:\n\nSwap z_i and \\widetilde{z}_i with probability 0.5. This gives z_{i}^{(b)} and \\widetilde{z}_i^{(b)}\nCalculate T^{(b)}=\\frac{1}{n}\\sum_{i=1}^n (z_i^{(b)} - \\widetilde{z}_i^{b})\n\nThe p-value is:\np-\\text{value} = \\frac{1}{B}\\sum_{b=1}^B I(T^{(b)} \\leq T_{obs})\n\n# we have trained f already\n# now train g\ndtrain = xgb.DMatrix(x_train[:, range(1, p)], label=y_train)\ndtest = xgb.DMatrix(x_test[:, range(1, p)], label=y_test)\n\nparams = {\n    'max_depth': 3,\n    'eta': 0.3,\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'nthread': 4\n}\n\n# Train the model\nnum_rounds = 100\neval_list = [(dtrain, 'train'), (dtest, 'eval')]\nmodel_without = xgb.train(params, dtrain, num_rounds, eval_list, early_stopping_rounds=10,verbose_eval=False)\n\n\ny_pred_without = model_without.predict(dtest)\n# ~z\nresid_without = np.power(y_pred_without - y_test, 2)\n\n\nn_test = y_test.shape[0]\n\n\nprint(f\"Mean Test Errors: {resid_with.mean():.4f}\")\nprint(f\"Mean Null Test Errors: {resid_without.mean():.4f}\")\n\nobs_T = resid_with.mean() - resid_without.mean()\n\nMean Test Errors: 1.3923\nMean Null Test Errors: 12.8082\n\n\n\n# get columns Z | Z_tilde\nresid_all = np.hstack((resid_with.reshape(n_test, 1), resid_without.reshape(n_test, 1)))\n\n\nn_permutations = 500\nresampled_Ts = []\n\nfor _ in range(n_permutations):\n    heads = np.random.choice(np.array((0,1)), size=n_test)\n\n    resamp_with = resid_all[range(n_test), heads]\n    resamp_without = resid_all[range(n_test), 1-heads]\n    # test error using X - test error not using X\n    resampled_T = np.mean(resamp_with) - np.mean(resamp_without)\n    resampled_Ts.append(resampled_T)\n\n## Calculate p-value\np_value = sum([1 for t in resampled_Ts if t &lt;= obs_T]) / n_permutations\n\n\nresampled_Ts = np.array(resampled_Ts)\nplt.hist(resampled_Ts, bins=20 ,density=True, alpha=0.7)\nplt.axvline(obs_T, color='red')\nplt.show()\n\nprint(f\"P-Value: {p_value:.4f}\")\n\n\n\n\n\n\n\n\nP-Value: 0.0000\n\n\nWe reject the null hypothesis.\nNow let’s do the test for x_{i2}, which we know has no relationship with y_i.\n\nj = 1\nx_js = np.array([i for i in range(p) if i != j])\n\n\nx_js\n\narray([ 0,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49])\n\n\n\ndtrain = xgb.DMatrix(x_train[:, x_js], label=y_train)\ndtest = xgb.DMatrix(x_test[:, x_js], label=y_test)\n\nparams = {\n    'max_depth': 3,\n    'eta': 0.3,\n    'objective': 'reg:squarederror',\n    'eval_metric': 'rmse',\n    'nthread': 4\n}\n\n# Train the model\nnum_rounds = 100\neval_list = [(dtrain, 'train'), (dtest, 'eval')]\nmodel_without = xgb.train(params, dtrain, num_rounds, eval_list, early_stopping_rounds=10,verbose_eval=False)\n\ny_pred_without = model_without.predict(dtest)\nresid_without = np.power(y_pred_without - y_test, 2)\n\n\nprint(f\"Mean Test Errors: {resid_with.mean():.4f}\")\nprint(f\"Mean Null Test Errors: {resid_without.mean():.4f}\")\n\nobs_T = resid_with.mean() - resid_without.mean()\n\nMean Test Errors: 1.3923\nMean Null Test Errors: 1.4237\n\n\n\nresid_all = np.hstack((resid_with.reshape(n_test, 1), resid_without.reshape(n_test, 1)))\n\nn_permutations = 1000\nresampled_Ts = []\n\nfor _ in range(n_permutations):\n    heads = np.random.choice(np.array((0,1)), size=n_test)\n\n    resamp_with = resid_all[range(n_test), heads]\n    resamp_without = resid_all[range(n_test), 1-heads]\n    # test error using X - test error not using X\n    resampled_T = np.mean(resamp_with) - np.mean(resamp_without)\n    resampled_Ts.append(resampled_T)\n\n## Calculate p-value\np_value = sum([1 for t in resampled_Ts if t &lt;= obs_T]) / n_permutations\n\n\nresampled_Ts = np.array(resampled_Ts)\nplt.hist(resampled_Ts, bins=20, density=True, alpha=0.7)\nplt.axvline(obs_T, color='red')\nplt.show()\n\nprint(f\"P-Value: {p_value:.4f}\")\n\n\n\n\n\n\n\n\nP-Value: 0.2110\n\n\nWe do not reject the null hypothesis (as expected).",
    "crumbs": [
      "Home",
      "Lecture 11 - Conditional Tests"
    ]
  },
  {
    "objectID": "lec-11/lecture-11.html#conditional-randomization-test",
    "href": "lec-11/lecture-11.html#conditional-randomization-test",
    "title": "Lecture 11 - Conditional Tests",
    "section": "Conditional Randomization Test",
    "text": "Conditional Randomization Test\nWe generate a new dataset:\nZ_{i1} \\sim N(0, 1), Z_{i2}\\sim N(0,1)\nX_i = Z_{i1} + N(0, 0.1^2)\nY_i = 0.4 X_{i} + 0.4 Z_{i2} + N(0, 0.5^2)\n\nX_i is correlated with Z_{i1}.\nOnly X_i and Z_{i2} are predictive of Y_i\n\n\n# Generate example data\nnp.random.seed(42)\nn_samples = 1000\n\n# Generate confounding variable Z\nZ = np.random.normal(0, 1, (n_samples, 2))\n\n# Generate X with dependence on Z\nx_sd = 0.2\nX = Z[:, 0] + np.random.normal(0, x_sd, n_samples)\n\n# Generate Y with dependence on both X and Z\n# X, Z1, Z2\nY = 0.5 * X + 0.5 * Z[:, 1] + np.random.normal(0, 0.5, n_samples)\n\nWe run a linear regression and obtain estimates of our coefficients for X, Z_1 and Z_2.\nWe also conduct a t-test for each variable - as expected, we see for X, Z_2 we reject the null that the coefficient is zero. For Z_1 we retain the null hypothesis.\n\ndat = np.hstack((X.reshape(-1,1), Z))\nest = sm.OLS(Y, dat)\nest2 = est.fit()\nprint(est2.summary())\n\n                                 OLS Regression Results                                \n=======================================================================================\nDep. Variable:                      y   R-squared (uncentered):                   0.642\nModel:                            OLS   Adj. R-squared (uncentered):              0.641\nMethod:                 Least Squares   F-statistic:                              596.5\nDate:                Tue, 25 Nov 2025   Prob (F-statistic):                   6.32e-222\nTime:                        15:13:12   Log-Likelihood:                         -750.23\nNo. Observations:                1000   AIC:                                      1506.\nDf Residuals:                     997   BIC:                                      1521.\nDf Model:                           3                                                  \nCovariance Type:            nonrobust                                                  \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nx1             0.5610      0.083      6.793      0.000       0.399       0.723\nx2            -0.0909      0.085     -1.073      0.284      -0.257       0.075\nx3             0.4900      0.016     30.675      0.000       0.459       0.521\n==============================================================================\nOmnibus:                        2.779   Durbin-Watson:                   2.037\nProb(Omnibus):                  0.249   Jarque-Bera (JB):                2.408\nSkew:                          -0.002   Prob(JB):                        0.300\nKurtosis:                       2.760   Cond. No.                         9.95\n==============================================================================\n\nNotes:\n[1] R² is computed without centering (uncentered) since the model does not contain a constant.\n[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\nIn linear regression, the t-test for H_0:\\beta_j=0 is testing whether x_j is independent of y, conditional on (x_1,\\dots, x_{j-1}, x_{j+1}, \\dots, x_p), if we make assumptions of linearity and Gaussianity.\nWhat if we do not want to make these assumptions? We can conduct a conditional randomization test:\nH_0: X_j \\text{ ind } Y | X_1,\\dots, X_{j-1}, X_{j+1}, \\dots X_p\nConditional Randomization Test (Candes et al. 2018)\n\nOn observed data, calculate T_{obs}\nFor b=1,\\dots, B:\n\nDraw X_j^{(b)} \\sim Q(X_j|X_{\\backslash j})\nCalculate T^{(b)} based on Y_i, X_j^{(b)}, X_{\\backslash j}\n\nCalculate p-value:\n\n\\frac{1}{B}\\sum_{b=1}^B I(T^{(b)} \\geq T_{obs})\nIn this example, we use the true Q(X|Z) distribution. In practice, we do not know this distribution - we generally have to estimate it.\n\nB = 1000\n\n# Compute observed test statistic\nobserved_stat = est2.params[0]\n\n# Perform sampling\nnull_stats = np.zeros(B)\n\nfor i in tqdm(range(B)):\n    # sample conditional x\n    X_draw = np.zeros_like(X)\n    for j in range(len(X)):\n        # Draw from conditional distribution\n        X_draw[j] = Z[j, 0] + np.random.normal(0, x_sd) # this is the true conditional distribution\n\n# Compute test statistic for permuted data\n    dat = np.hstack((X_draw.reshape(-1,1), Z))\n    est = sm.OLS(Y, dat)\n    est2 = est.fit()\n\n    null_stats[i] = est2.params[0]\n\n# Compute p-value\np_value = np.mean(np.abs(null_stats) &gt;= np.abs(observed_stat))\n        \n\n100%|██████████| 1000/1000 [00:00&lt;00:00, 1341.46it/s]\n\n\n\nprint(\"\\nConditional Randomization Test Results:\")\nprint(f\"Observed Statistic: {observed_stat:.4f}\")\nprint(f\"P-value: {p_value:.4f}\")\n\nplt.figure(figsize=(10, 6))\nplt.hist(null_stats, bins=50, density=True, alpha=0.7)\nplt.axvline(observed_stat, color='red', linestyle='--', \n            label=f'Observed Statistic: {observed_stat:.4f}')\n\nplt.title('Null Distribution from Conditional Randomization Test')\nplt.xlabel('Test Statistic')\nplt.ylabel('Density')\nplt.legend()\nplt.show()\n\n\nConditional Randomization Test Results:\nObserved Statistic: 0.5610\nP-value: 0.0000\n\n\n\n\n\n\n\n\n\nBonus visualization:\n\nthe conditional draws X_j^{(b)} are close to actual X_j (as expected)\nthe conditional draws X_j^{(b)} are not related to Y, adjusting for Z (as required for the null)\n\n\n# this is adjusting for z in linear regression\nadj_mat = (np.eye(Z.shape[0]) - Z @ (np.linalg.inv(np.transpose(Z) @ Z) @ np.transpose(Z)))\ny_adj_z = adj_mat @ Y\nx_adj_z = adj_mat @ X\nx_draw_adj_z = adj_mat @ X_draw\nc1 = LinearRegression().fit(x_draw_adj_z.reshape(-1,1), y_adj_z).coef_[0]\nc2 = LinearRegression().fit(x_adj_z.reshape(-1,1), y_adj_z).coef_[0]\n\n\nfig, axes = plt.subplots(1,3, figsize=(12, 4))\naxes[0].scatter(X_draw, X)\naxes[0].set_xlabel('X draw')\naxes[0].set_ylabel('X actual')\naxes[0].set_title('X actual vs X_draw')\naxes[1].scatter(x_draw_adj_z, y_adj_z)\naxes[1].set_xlabel('X draw')\naxes[1].set_ylabel('Y')\naxes[1].axline((0,0),slope=c1, color='red',label=f'Slope: {c1:.2f}')\naxes[1].legend()\naxes[1].set_title('Y vs. X draw (adj for Z)')\naxes[2].scatter(x_adj_z, y_adj_z)\naxes[2].set_xlabel('X actual')\naxes[2].set_ylabel('Y')\naxes[2].axline((0,0), slope=c2, color='red',label=f'Slope: {c2:.2f}')\naxes[2].legend()\naxes[2].set_title('Y vs. X (adj for Z)')\nplt.tight_layout()",
    "crumbs": [
      "Home",
      "Lecture 11 - Conditional Tests"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html",
    "href": "getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#anaconda-installation",
    "href": "getting-started/getting-started.html#anaconda-installation",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#managing-packages",
    "href": "getting-started/getting-started.html#managing-packages",
    "title": "Getting started",
    "section": "Managing packages",
    "text": "Managing packages\nThere are many open source Python packages for statistics and machine learning.\nTo download packages, two popular package managers are Conda and Pip. Both Conda and Pip come with the Anaconda distribution.\nConda is a general-purpose package management system, designed to build and manage software of any type from any language. This means conda can take advantage of many non-python packages (like BLAS, for linear algebra operations).\nPip is a package manager for python. You may see people using pip with environments using virtualenv or venv.\nWe recommend:\n\nuse a conda environment\nwithin this environment, use conda to install base packages such as pandas and numpy\nif a package is not available via conda, then use pip\n\nSee here for some conda vs pip misconceptions, and why conda is helpful.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#environments",
    "href": "getting-started/getting-started.html#environments",
    "title": "Getting started",
    "section": "Environments",
    "text": "Environments\n\nAbout\nIt is good coding practice to use virtual environments with Python. From this blog:\n\nA Python virtual environment consists of two essential components: the Python interpreter that the virtual environment runs on and a folder containing third-party libraries installed in the virtual environment. These virtual environments are isolated from the other virtual environments, which means any changes on dependencies installed in a virtual environment don’t affect the dependencies of the other virtual environments or the system-wide libraries. Thus, we can create multiple virtual environments with different Python versions, plus different libraries or the same libraries in different versions.\n\n\n\n\nCreating an environment for MSDS-534\nWe recommend creating a virtual environment for your MSDS-534 coding projects. This way, you can have an environment with all the necessary packages and you can easily keep track of what versions of the packages you used.\n\nOpen Terminal (macOS) or a shell\nCreate an environment called msds534 using Conda with the command: conda create --name msds534\nTo install packages in your environment, first activate your environment: conda activate msds534\nThen, install the following packages using the command: conda install numpy pandas scikit-learn matplotlib seaborn jupyter ipykernel\nInstall PyTorch by running the appropriate command from here (for macOS, the command is: pip3 install torch torchvision)\nTo exit your environment: conda deactivate\n\nHere is a helpful cheatsheet for conda environment commands.\nFor more details about the shell / bash, here is a helpful resource.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#vscode",
    "href": "getting-started/getting-started.html#vscode",
    "title": "Getting started",
    "section": "VSCode",
    "text": "VSCode\nThere are a number of Python IDEs (integrated development environments). In class, we will be using VSCode (download here).\n\nDownload lecture-1.ipynb here and open it in VSCode.\nTo use your msds534 environment, on the top right hand corner, click “Select Kernel” &gt; “Python Environments” &gt; msds534. If it prompts you to install ipykernel, follow the prompts to install it.\n\nJupyter notebooks (.ipynb files) are useful to combine code cells with text (as markdown cells).\nVSCode also has a Python interactive window (details here).",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#learning-python",
    "href": "getting-started/getting-started.html#learning-python",
    "title": "Getting started",
    "section": "Learning Python",
    "text": "Learning Python\nIn this class, we will assume some familiarity with:\n\nNumpy\nPandas\nObject-oriented programming",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#fall-2025",
    "href": "index.html#fall-2025",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "Getting started",
    "text": "Getting started\nSee here for details about how to get started with python, VSCode and PyTorch."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "Lectures",
    "text": "Lectures\nThe code from the lectures is in the left sidebar.\nLecture slides and information about homeworks, office hours etc. can be found on Canvas."
  },
  {
    "objectID": "lec-2/lecture-2.html",
    "href": "lec-2/lecture-2.html",
    "title": "Lecture 2 - Neural Networks",
    "section": "",
    "text": "# import packages\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Lecture 2 - Neural Networks"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#pytorch",
    "href": "lec-2/lecture-2.html#pytorch",
    "title": "Lecture 2 - Neural Networks",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source machine learning library. It is one of the most popular frameworks for deep learning.\nIn Python, the PyTorch package is called torch, available here.",
    "crumbs": [
      "Home",
      "Lecture 2 - Neural Networks"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#simple-linear-regression",
    "href": "lec-2/lecture-2.html#simple-linear-regression",
    "title": "Lecture 2 - Neural Networks",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nLet’s see how PyTorch can help us solve a predictive problem using the most simple linear regression model.\nA simple linear regression model assumes that the observed data sample y_1, y_2, ..., y_n follows the following structure:\n\ny_i = a + bx_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,1).\n\n\nData Generation\nLet’s start by creating some synthetic data to work with. The following code block does the following:\n\nWe generate x_1, x_2, ..., x_{100} with each x_i \\sim \\text{Uniform}(-2,2).\nWe let a = 1 and b = 2.\nWe create y_1, y_2, ..., y_{100} by setting y_i = 1 + 2x_i + \\epsilon_i, where \\epsilon_i is some standard Gaussian noise.\n\n\n# Data Generation\n# np.random.uniform generates random samples from a uniform distribution over [0, 1).\nx = np.random.uniform(low=-2,high=2, size =(100, 1))                              \n# the resulting dimension of vector x will be 100 x 1.\n\n# np.random.randn generates random samples from a standard normal distribution.\ny = 1 + 2 * x + np.random.randn(100, 1)            \n\n\n\nTrain-Validation Split\nNow we do a train-validation split, by randomly picking 80% of the indices as the train set and the rest as validation.\n\n# Shuffles the indices\nidx = np.arange(100)\nnp.random.shuffle(idx)\n\n# Uses first 80 random indices for train\ntrain_idx = idx[:80]\n# Uses the remaining indices for validation\nval_idx = idx[80:]\n\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\n\n\n## Below shows a plot of the two sets of data.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxes[0].scatter(x_train, y_train, c = 'darkblue')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\naxes[0].set_title('Generated Data - Train')\naxes[1].scatter(x_val, y_val, c = 'darkred')\naxes[1].set_xlabel('x')\naxes[1].set_title('Generated Data - Validation')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMoving to PyTorch\nLet’s fit a linear regression model using PyTorch.\nFirst, we need to transform our data to PyTorch tensors.\n\n# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\nx_train_tensor = torch.tensor(x_train, dtype=torch.float)      \ny_train_tensor = torch.tensor(y_train, dtype=torch.float)\n\n\n# Here we can see the difference \nprint(type(x_train), type(x_train_tensor), x_train_tensor.type())\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'torch.Tensor'&gt; torch.FloatTensor\n\n\n\nInitialize Parameters and Require Gradients\n\n# Since we want to apply gradient descent on these parameters, we need\n# to set requires_grad = True\n\n### Initialize parameters... \na = torch.randn(1, requires_grad=True)      \nb = torch.randn(1, requires_grad=True)\n### ---------------------- \n\nprint(a, b)\n\ntensor([0.0764], requires_grad=True) tensor([1.3134], requires_grad=True)\n\n\n\n\nGradient Computation with Autograd\nThe loss is:\n L(a, b) = \\frac{1}{n} \\sum_{i=1}^n (y_i - a- b\\cdot x_i)^2 \nThe gradients are:\n \\frac{\\partial L}{\\partial a} = -2 \\cdot \\frac{1}{n} \\sum_{i=1}^n (y_i - a - b\\cdot x_i)  \\frac{\\partial L}{\\partial b} = -2 \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i(y_i - a - b\\cdot x_i)\nWe won’t actually have to compute gradients - autograd is PyTorch’s automatic differentiation package, which will do it for us.\n\nThe backward() method helps us to compute partial derivatives of the loss function w.r.t. our parameters. It is essentially saying “do backpropagation”.\nWe obtain the computed gradients via the .grad attribute.\n\n\n# Specifying a learning rate\nlr = 1e-1\n\nyhat = a + b * x_train_tensor\nerror = y_train_tensor - yhat\nloss = (error ** 2).mean()\n\n# We just tell PyTorch to work its way BACKWARDS from the specified loss!\nloss.backward()\n# Let's check the computed gradients...\nprint('a grad (pytorch)', a.grad)\nprint('b grad (pytorch)', b.grad)\n\n# compare to actual gradients\nwith torch.no_grad():\n    a_grad = -2 * error.mean()\n    b_grad = -2 * (x_train_tensor * error).mean()\nprint('a grad (manual)', a_grad)\nprint('b grad (manual)', b_grad)\n\na grad (pytorch) tensor([-1.9982])\nb grad (pytorch) tensor([-2.5759])\na grad (manual) tensor(-1.9982)\nb grad (manual) tensor(-2.5759)\n\n\nNote: we use torch.no_grad() so PyTorch doesn’t keep track of the operations (otherwise, they may be included in the computation graph that PyTorch uses to calculate gradients).\nLet’s now take a gradient descent step.\n\n# update the parameters \nwith torch.no_grad():       \n    a -= lr * a.grad\n    b -= lr * b.grad\n# ---------------------- \n\n# PyTorch is \"clingy\" to its computed gradients, we need to tell it to zero out\na.grad.zero_()      # note the \"_\" which means \"in-place\"\nb.grad.zero_()\n\ntensor([0.])\n\n\nPutting it all together:\n\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    # No more manual computation of gradients! \n    # a_grad = -2 * error.mean()\n    # b_grad = -2 * (x_tensor * error).mean()\n\n    ### We just tell PyTorch to work its way BACKWARDS from the specified loss! \n    loss.backward()            \n    ### ---------------------- \n\n    # Let's check the computed gradients...\n    if epoch % 5 == 0:\n        print('Epoch:', epoch, '(a, b) grad', a.grad, b.grad)\n    \n    # Updating the parameters\n    with torch.no_grad():\n        a -= lr * a.grad\n        b -= lr * b.grad\n    \n    a.grad.zero_()\n    b.grad.zero_()\n    ### ---------------------- \n    \nprint('Final (a, b)', a, b)\n\nEpoch: 0 (a, b) grad tensor([-1.5175]) tensor([-1.7315])\nEpoch: 5 (a, b) grad tensor([-0.4175]) tensor([-0.2149])\nEpoch: 10 (a, b) grad tensor([-0.1279]) tensor([-0.0152])\nEpoch: 15 (a, b) grad tensor([-0.0417]) tensor([0.0038])\nEpoch: 20 (a, b) grad tensor([-0.0141]) tensor([0.0027])\nEpoch: 25 (a, b) grad tensor([-0.0048]) tensor([0.0012])\nEpoch: 30 (a, b) grad tensor([-0.0017]) tensor([0.0004])\nEpoch: 35 (a, b) grad tensor([-0.0006]) tensor([0.0002])\nEpoch: 40 (a, b) grad tensor([-0.0002]) tensor([5.5239e-05])\nEpoch: 45 (a, b) grad tensor([-6.8247e-05]) tensor([1.9357e-05])\nEpoch: 50 (a, b) grad tensor([-2.3618e-05]) tensor([6.4149e-06])\nEpoch: 55 (a, b) grad tensor([-8.0764e-06]) tensor([2.4475e-06])\nEpoch: 60 (a, b) grad tensor([-2.6971e-06]) tensor([1.3188e-06])\nEpoch: 65 (a, b) grad tensor([-8.7917e-07]) tensor([8.7172e-07])\nEpoch: 70 (a, b) grad tensor([-3.4273e-07]) tensor([7.8976e-07])\nEpoch: 75 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 80 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 85 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 90 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 95 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nFinal (a, b) tensor([0.9563], requires_grad=True) tensor([2.0711], requires_grad=True)\n\n\nThis is the same as the simple linear regression formula:\n\nb_hat = ((x_train-x_train.mean()) * (y_train-y_train.mean())).sum() / ((x_train-x_train.mean())**2).sum()\n\n\na_hat = y_train.mean() - b_hat * x_train.mean()\n\n\nprint(f\"a (formula): {a_hat:.4f}, b (formula): {b_hat:.4f}\")\n\na (ls): 0.9563, b (ls): 2.0711\n\n\n\n\nUpdate All Parameters Simultaneously with Optimizer\nPreviously, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers, like SGD or Adam.\nAn optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.\nBesides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method and that’s it!\nIn the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters a and b. Note: we will use the whole dataset, not batches, so it is technically GD (not SGD). However, the optimizer in torch is still called SGD.\n\na = torch.randn(1, requires_grad=True)\nb = torch.randn(1, requires_grad=True)\nprint(f\"Initializations: a = {a.item():.3f}, b = {b.item():.3f}\")\n\nlr = 1e-1\nn_epochs = 1000\n\n### Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)\n### ---------------------- \n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    loss.backward()    \n    \n    # No more manual update!\n    # with torch.no_grad():\n    #     a -= lr * a.grad\n    #     b -= lr * b.grad\n\n    ### perform the update via step() ###\n    optimizer.step()\n    ### ---------------------- ###\n    \n    # No more telling PyTorch to let gradients go!\n    # a.grad.zero_()\n    # b.grad.zero_()\n\n    ### clearing the gradient ###\n    optimizer.zero_grad()\n    ### ---------------------- ###\n\nprint(f\"Output: a = {a.item():.3f}, b = {b.item():.3f}\")\n\nInitializations: a = -0.684, b = 0.537\nOutput: a = 1.113, b = 1.973\n\n\n\n\n\nModel Building\nPython is an object-oriented programming language.\nEvery item you interact with is an object. An object has a type, as well as:\n\nattributes (values)\nmethods (functions that can be applied to the object)\n\nAs an example, 'hello' in Python is an object of type str, with methods such as split, join etc.\nA Python class is a “blueprint” for an object.\nInheritance allows us to define a class that inherits all the methods and properties from another class.\n\nnn.Module Class\nIn PyTorch, a model is represented by a regular Python class that inherits from the nn.Module class. nn.Module is everywhere in PyTorch and represents mappings in neural networks.\nLet’s look at an example:\n\nclass ManualLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()              # inherits from nn.Module\n        # Initialization of a and b.\n        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n        self.a = nn.Parameter(torch.randn(1, requires_grad=True))\n        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n        \n    def forward(self, x):\n        # Computes the outputs / predictions\n        return self.a + self.b * x\n\n\nclass ManualLinearRegression(nn.Module)\n\nthis statement declares a class ManualLinearRegression which inherits from the base class nn.Module.\n\n\nIndented beneath the class statement are the methods of this class: __init__ and forward:\n\n__init__\n\nall classes have an __init__ which is executed when the class is instantiated\nself refers to an instance of the class\nin __init__, we have attached the parameters a and b as attributes\nsuper().__init__() is a call to the __init__ of nn.Module. For torch models, we will always be making this super() call as it is necessary for the model to be properly interpreted by torch.\n\nforward is called when the neural network is run on input data.\n\n\n# Now we can create a model \nmodel = ManualLinearRegression()\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # In PyTorch, models have a train() method which sets the model to training mode.\n    \n    ### enter training mode \n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n\n    ### this is the forward step \n    yhat = model(x_train_tensor)\n    ### ---------------------- \n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())\n\nOrderedDict({'a': tensor([-0.6481]), 'b': tensor([1.4267])})\nOrderedDict({'a': tensor([1.1129]), 'b': tensor([1.9731])})\n\n\n\n\nUtilizing PyTorch Layers\nFor simple tasks like building a linear regression, we could directly create a and b as nn.Parameters.\nInstead, we can use PyTorch’s nn.Linear to create a linear layer. Later, this will let us build more complicated networks. Note: nn.Linear automatically adds a bias term.\n\nclass PyTorchLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1) # arguments: (input dim, output dim)\n                \n    def forward(self, x):\n        return self.linear(x)\n\n\nfor name, param in PyTorchLinearRegression().named_parameters():\n    if param.requires_grad:\n        print(name, param.data)\n\nlinear.weight tensor([[-0.5715]])\nlinear.bias tensor([-0.5772])\n\n\n\n# Now we can create a model \nmodel = PyTorchLinearRegression()\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # In PyTorch, models have a train() method which sets the model to training mode.\n    \n    ### enter training mode ###\n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n\n    ### this is the forward step ###\n    yhat = model(x_train_tensor)\n    ### ---------------------- ###\n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())\n\nOrderedDict({'linear.weight': tensor([[-0.0886]]), 'linear.bias': tensor([0.0320])})\nOrderedDict({'linear.weight': tensor([[1.9731]]), 'linear.bias': tensor([1.1129])})",
    "crumbs": [
      "Home",
      "Lecture 2 - Neural Networks"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#xor-problem",
    "href": "lec-2/lecture-2.html#xor-problem",
    "title": "Lecture 2 - Neural Networks",
    "section": "XOR problem",
    "text": "XOR problem\nThe “XOR” (or “exclusive OR”) problem is often used to illustrate the ability of neural networks to fit complicated functions. The XOR problem has a checkerboard structure:\n\nn = 5000\np = 2\nx = np.random.uniform(-2, 2, size=(n, p))\n\n#x[,1] &lt;- first column (R)\n#x[:, 0] &lt;- first column (Python)\n\ny = ((x[:,0] &lt; 0) & (x[:, 1] &gt; 0)).astype(x.dtype) + ((x[:,0] &gt; 0) & (x[:, 1] &lt; 0)).astype(x.dtype)\n\ndf = np.hstack([y.reshape((n, 1)), x])\ndf = pd.DataFrame(df, columns = ['y', 'x1', 'x2'])\n\nsns.set_theme()\nxor_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nxor_plot.figure.subplots_adjust(top=.9)\n\n\n\n\n\n\n\n\nLogistic regression can only fit linear decision boundaries, and so fails the XOR problem.\n\n## logitstic regression doesn't work\nlog_fit = LogisticRegression()\nlog_fit.fit(x, y)\ncoeffs = log_fit.coef_[0]\ncoeff = -coeffs[0]/coeffs[1]\n\n## plot\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nplt.axline([0,0], slope=coeff)\n## title\nlogit_plot.figure.subplots_adjust(top=.9)\nlogit_plot.figure.suptitle(str(round(coeffs[0], 2)) + r'$x_1$ + ' + '(' + str(round(coeffs[1], 2)) + ')' + r'$x_2$')\n## fill in area\nx_fill = np.linspace(-2, 2, num=200)\ny_line = coeff * x_fill\nlogit_plot.ax.fill_between(x_fill, y_line, 2, color='blue', alpha=0.2)\nlogit_plot.ax.fill_between(x_fill, -2, y_line, color='orange', alpha=0.2)\nlogit_plot.ax.set_ylim(-2,2)\n\n\n\n\n\n\n\n\n\nMulti-Layer Neural Network\nHere is an example of a one hidden-layer neural network in torch named XORNet.\nInstead of a single linear layer, we have:\n\na linear layer\na ReLU activation\na linear layer\na sigmoid activation\n\nnn.Sequential allows us to stack these layers together - we store the final object as an attribute of the model: self.sequential.\nThe forward method then passes input data through self.sequential to get the output. That is:\n f(x_i) = g(W^{(2)}(a(W^{(1)}x_i + b^{(1)}))+b^{(2)})\n\nclass XORNet(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.sequential = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        y = self.sequential(x)\n        y = torch.flatten(y) # we do this to turn y into a one-dim tensor\n        return y\n\n    def loss_fn(self, y, y_pred):\n        loss = y * torch.log(y_pred + 1e-8) + (1-y) * torch.log(1-y_pred + 1e-8)\n        output = -loss.sum()\n        return output\n\n\nhidden_dim=2\nmodel = XORNet(input_dim=p, hidden_dim=hidden_dim)\n\n\nmodel\n\nXORNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=2, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)\n\n\n\n\nData\nFor torch to read the data, it needs to be a torch.tensor type:\n\nx_train = torch.tensor(x, dtype=torch.float)\ny_train = torch.tensor(y, dtype=torch.float)\n\nWe combine x_train and y_train into a TensorDataset, a dataset recognizable by torch. TensorDataset stores the samples and their labels. It is a subclass of the more general torch.utils.data.Dataset, which you can customize for non-standard data.\n\ntrain_data = TensorDataset(x_train, y_train)\n\nTensorDataset is helpful as it can be passed to DataLoader(). DataLoader wraps an iterable around the Dataset class. This lets us loop over the DataLoader to extract a mini-batch at each iteration.\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nTo retrieve a sample mini-batch, the following will return a list containing two tensors: one for the features, another one for the labels.\n\nnext(iter(train_loader))\n\n[tensor([[-1.0248, -0.2131],\n         [-0.7861,  1.6893],\n         [-1.7932, -0.0574],\n         [ 0.7178, -0.4897],\n         [ 1.9000,  1.6038],\n         [ 0.7674, -1.9498],\n         [-0.2336, -1.4478],\n         [-1.9838, -0.8512],\n         [-0.2094,  0.1471],\n         [-1.2884, -0.1538]]),\n tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0.])]\n\n\n\n\nTraining\nWe now set up the optimizer for training. We use Adam, and a base learning rate of lr=0.01. We set the number of epochs to 100. (Rule of thumb: pick largest lr that still results in convergence)\n\nlr = 0.01\nepochs = 100\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nXORNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=2, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)\n\n\nNow we train the model:\n\nfor epoch in range(epochs):\n    \n    # in each epoch, iterate over all batches of data (easily accessed through train_loader)\n    l = 0\n\n    for x_batch, y_batch in train_loader:\n\n        pred = model(x_batch)                   # this is the output from the forward function\n        loss = model.loss_fn(y_batch, pred)     # calculate loss function\n\n        loss.backward()                         # computes gradients wrt loss function\n        optimizer.step()                        # updates parameters \n        optimizer.zero_grad()                   # set the gradients back to zero (otherwise grads are accumulated)\n\n        l += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 2.55e+03\nepoch:  10 loss: 1.79e+03\nepoch:  20 loss: 1.79e+03\nepoch:  30 loss: 1.79e+03\nepoch:  40 loss: 1.79e+03\nepoch:  50 loss: 1.79e+03\nepoch:  60 loss: 1.79e+03\nepoch:  70 loss: 1.79e+03\nepoch:  80 loss: 1.79e+03\nepoch:  90 loss: 1.78e+03\n\n\nTo visualize the end result, we plot the predicted values over the whole space (the decision surface).\n\nx1 = np.arange(-2, 2, 0.05)\nx2 = np.arange(-2, 2, 0.05)\n\nx_test_np = np.array([(i, j) for i in x1 for j in x2])\ny_test_np = ((x_test_np[:,0] &lt; 0) & (x_test_np[:, 1] &gt; 0)).astype(x_test_np.dtype) + ((x_test_np[:,0] &gt; 0) & (x_test_np[:, 1] &lt; 0)).astype(x_test_np.dtype)\n\nx_test = torch.tensor(x_test_np, dtype=torch.float)\ny_test = torch.tensor(y_test_np)\n\n\nmodel.eval()\ny_pred = model(x_test)\n\ny_pred_np = y_pred.detach().numpy()\ny_pred_np = y_pred_np.reshape(x1.shape[0], x2.shape[0])\n\nseaborn_cols = sns.color_palette(\"tab10\")\ncols = [seaborn_cols[int(i)] for i in y]\n\ncustom_cmap = sns.diverging_palette(220, 50, s=70, l=70, as_cmap=True)\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(4, 4)\nax.contourf(x1, x2, y_pred_np, cmap=custom_cmap)\nax.scatter(x[0:100,0], x[0:100,1], c=cols[0:100])\n\n\n\n\n\n\n\n\n\nmodel.sequential[0].weight\n\nParameter containing:\ntensor([[-2.3491,  2.3204],\n        [ 2.4103, -2.3208]], requires_grad=True)\n\n\n\nmodel.sequential[0].bias\n\nParameter containing:\ntensor([-0.0632, -0.1064], requires_grad=True)\n\n\n\nmodel.sequential[2].weight\n\nParameter containing:\ntensor([[1.3308, 1.3007]], requires_grad=True)\n\n\n\nmodel.sequential[2].bias\n\nParameter containing:\ntensor([-3.4300], requires_grad=True)\n\n\nPlay around with different sizes of hidden_dim and see the difference!",
    "crumbs": [
      "Home",
      "Lecture 2 - Neural Networks"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#mnist-example",
    "href": "lec-2/lecture-2.html#mnist-example",
    "title": "Lecture 2 - Neural Networks",
    "section": "MNIST example",
    "text": "MNIST example\nWe use torchvision.datasets to download the MNIST data.\n\n(mnist_train,\n mnist_test) = [torchvision.datasets.MNIST(root='../data',\n                      train=train,\n                      download=True,\n                      transform=torchvision.transforms.ToTensor())\n                for train in [True, False]]\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    train_image, label = mnist_train[i]\n    plt.imshow(train_image[0], cmap=plt.cm.binary)\n    plt.xlabel(label)\nplt.show()\n\n\n\n\n\n\n\n\nSet up our dataloaders.\n\ntrain_loader = DataLoader(dataset=mnist_train, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=mnist_test, batch_size=10000, shuffle=False)\n\nLet’s define our neural network for the MNIST classification problem.\n\nclass MNISTNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            nn.Linear(28*28, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.Softmax()\n        )\n\n    def forward(self, x):\n        prob = self.layers(x)\n        return prob\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nInstantiate our model:\n\nmodel = MNISTNet()\n\nTrain our model:\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    l = 0\n    for x_batch, y_batch in train_loader:\n\n        x_batch = x_batch.reshape(x_batch.shape[0], 28*28)\n        y_batch = F.one_hot(y_batch, num_classes=10)\n        y_pred = model(x_batch)\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        l += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 2.59e+02\nepoch:  10 loss: 13.7\nepoch:  20 loss: 7.05\nepoch:  30 loss: 7.11\nepoch:  40 loss: 7.72\n\n\nCalculate our accuracy:\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\nx_batch = x_batch.reshape(x_batch.shape[0], 28 * 28)\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\nLet’s look at some interesting results (code adapted from here)\n\n# find interesting test images\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = range(8)\ninds2 = errors[:8]\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = mnist_test[n]\n    plt.imshow(image[0], cmap=plt.cm.binary)\n    plt.xlabel(label)\n    predicted_label = y_pred[n]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"truth={}, pred={}, score={:2.0f}%\".format(\n        label,\n        predicted_label,\n        100 * np.max(pred_array[n])),\n        color=color)\n\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    thisplot = plt.bar(range(10), pred_array[n], color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(pred_array[n])\n    thisplot[predicted_label].set_color('red')\n    thisplot[label].set_color('blue')",
    "crumbs": [
      "Home",
      "Lecture 2 - Neural Networks"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#heteroskedastic-regression",
    "href": "lec-2/lecture-2.html#heteroskedastic-regression",
    "title": "Lecture 2 - Neural Networks",
    "section": "Heteroskedastic regression",
    "text": "Heteroskedastic regression\nThis code is adapted from here.\n\n# Make data\n\nx_range = [-20, 60]  # test\nx_ranges = [[-20, 60]]\nns = [1000]\n\ndef load_dataset():\n    def s(x):  # std of noise\n        g = (x - x_range[0]) / (x_range[1] - x_range[0])\n        return 0.25 + g**2.0\n\n    x = []\n    y = []\n    for i in range(len(ns)):\n        n = ns[i]\n        xr = x_ranges[i]\n        x1 = np.linspace(xr[0], xr[1], n)\n        eps = np.random.randn(n) * s(x1)\n        y1 = (1 * np.sin(0.2 * x1) + 0.1 * x1) + eps\n        x = np.concatenate((x, x1))\n        y = np.concatenate((y, y1))\n    # print(x.shape)\n    x = x[..., np.newaxis]\n    n_test = 150\n    x_test = np.linspace(*x_range, num=n_test).astype(np.float32)\n    x_test = x_test[..., np.newaxis]\n    return y, x, x_test\n\ny, x, x_test = load_dataset()\n\nDefine neural network\n\nclass HetNet(nn.Module):\n\n    def __init__(self, input_dim, output_dim, hidden_dims, mean_dims, var_dims):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dims\n        self.mean_dims = mean_dims\n        self.var_dims = var_dims\n\n        # create backbone\n        current_dim = input_dim\n        self.layers = nn.ModuleList()\n        for i in range(len(hidden_dims)):\n            hdim = hidden_dims[i]\n            self.layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n\n        # create heads\n        core_dim = hidden_dims[-1]\n        current_dim = core_dim\n        self.mean_layers = nn.ModuleList()\n        for i in range(len(mean_dims)):\n            hdim = mean_dims[i]\n            self.mean_layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n        self.mean_layers.append(nn.Linear(current_dim, output_dim))\n\n        current_dim = core_dim\n        self.var_layers = nn.ModuleList()\n        for i in range(len(var_dims)):\n            hdim = var_dims[i]\n            self.var_layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n        self.var_layers.append(nn.Linear(current_dim, output_dim))\n\n    def core_net(self, x):\n        for layer in self.layers:\n            x = F.relu(layer(x))\n        return x\n\n    def mean_net(self, x):\n        for layer in self.mean_layers[:-1]:\n            x = F.relu(layer(x))\n        x = self.mean_layers[-1](x)\n        return x\n\n    def var_net(self, x):\n        for layer in self.var_layers[:-1]:\n            x = F.relu(layer(x))\n        x = self.var_layers[-1](x)\n        return x\n\n    def forward(self, x):\n        mean = self.mean_net(self.core_net(x))\n        log_var = self.var_net(self.core_net(x))\n\n        return mean, log_var\n\n    def loss_fn(self, x, y):\n        mean, log_var = self.forward(x)\n        var = torch.exp(log_var)\n\n        loss = torch.pow(y-mean, 2) / var + log_var\n        out = loss.mean()\n\n        return out\n\nSet up data\n\nx_train = torch.tensor(x, dtype=torch.float)\ny_train = torch.tensor(y, dtype=torch.float)\ny_train = y_train.unsqueeze(-1)\n\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nInitialize model\n\nhidden_dims = [50, 50]\nmean_dims = [20]\nvar_dims = [20]\nmodel = HetNet(input_dim=1, output_dim=1, hidden_dims=hidden_dims, mean_dims=mean_dims, var_dims=var_dims)\n\nTrain\n\nlr = 0.001\nepochs = 500\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n\n    l = 0\n    for x_batch, y_batch in train_loader:\n\n        loss = model.loss_fn(x_batch, y_batch)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        l += loss.item()\n\n    if epoch % 50 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 1.27e+02\nepoch:  50 loss: 2.11\nepoch:  100 loss: -13.7\nepoch:  150 loss: -23.8\nepoch:  200 loss: -26.2\nepoch:  250 loss: -26.0\nepoch:  300 loss: -31.2\nepoch:  350 loss: -31.0\nepoch:  400 loss: -31.0\nepoch:  450 loss: -29.0\n\n\nPlot results\n\nmodel.eval()\nmean, log_var = model(x_train)\nsd = torch.exp(0.5 * log_var)\nmean_np = mean.detach().numpy()\nsd_np = sd.detach().numpy()\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(6, 4)\nax.plot(x, y, '.', label=\"observed\")\nax.plot(x, mean_np, 'r-')\nax.plot(x, mean_np + 2 * sd_np, 'g-')\nax.plot(x, mean_np - 2 * sd_np, 'g-')",
    "crumbs": [
      "Home",
      "Lecture 2 - Neural Networks"
    ]
  },
  {
    "objectID": "lec-4/gpt.html",
    "href": "lec-4/gpt.html",
    "title": "GPT",
    "section": "",
    "text": "Here is a small GPT model from Andrej Karpathy.\n\n\ngpt.py\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse: \n    device = 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -&gt; (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers",
      "GPT"
    ]
  },
  {
    "objectID": "lec-12/lec12.html",
    "href": "lec-12/lec12.html",
    "title": "Lecture 12 - Causal Inference",
    "section": "",
    "text": "import numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nnp.random.seed(42)"
  },
  {
    "objectID": "lec-12/lec12.html#average-treatment-effect-estimation",
    "href": "lec-12/lec12.html#average-treatment-effect-estimation",
    "title": "Lecture 12 - Causal Inference",
    "section": "Average Treatment Effect Estimation",
    "text": "Average Treatment Effect Estimation\nThe Average Treatment Effect is: \\tau = \\mathbb{E}[Y_i^{(1)} - Y_i^{(0)}] where\n\nY_i^{(1)} is potential outcome for unit i under treatment\nY_i^{(0)} is potential outcome for unit i under control\n\nT_i is the treatment assignment for unit i:\n\nT_i=1 if unit i assigned to treatment\nT_i=0 if unit i assigned to control\n\nFor each unit i, we only observe one of Y_i^{(1)}, Y_i^{(0)}.\n\nRandomized Treatment\nWe first consider a randomized treatment (no confounding). That is:\nY_i^{(1)}, Y_i^{(0)} \\text{ ind } T_i.\n\nn = 500\n\n# randomly draw treatment\nt = np.random.binomial(1, 0.5, size=n)\n\n# treatment effect\ntau = 4\n\n# outcome\ny = tau * t + np.random.normal(size=n)\n\n\nplt.scatter(range(n), y, c=['red' if i == 0 else 'blue' for i in t], marker='o')\nplt.axhline(y = 0, color = 'red', label='T=0')\nplt.axhline(y = tau, color = 'blue',label='T=1')\nplt.legend()\n\n\n\n\n\n\n\n\nBecause \\mathbb{E}[Y_i|T_i=1]-\\mathbb{E}[Y_i|T_i=0] = \\mathbb{E}[Y_i^{(1)}=Y_i^{(0)}], we can estimate the ATE by taking the difference in means:\n\nnp.mean(y[t==1]) - np.mean(y[t==0])\n\nnp.float64(4.102895888143023)\n\n\nThis is equivalent to fitting a linear model with an intercept.\n\nT = sm.add_constant(t)\nmodel = sm.OLS(y, T).fit()\nmodel.params\n\narray([-0.04568022,  4.10289589])\n\n\nPermutation test for H_0: \\tau = 0 vs. H_1: \\tau \\neq 0\n\nt_obs = np.mean(y[t==1]) - np.mean(y[t==0])\n\nB = 1000\nt_dist = np.zeros((B))\n\nfor b in range(B):\n    t_draw = t[np.random.choice(n, size=n, replace=False)]\n    t_dist[b] = np.mean(y[t_draw==1]- np.mean(y[t_draw==0]))\n\n\np_value = sum([1 for t in t_dist if t &gt;= t_obs]) / B\n\nprint(f\"P-Value: {p_value:.4f}\")\n\nP-Value: 0.0000\n\n\n\nplt.hist(t_dist, bins=20, alpha=0.7)\nplt.axvline(t_obs, color='red')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nObserved confounder\nSuppose we have an observed confounder, X.\nIf we know X affects Y linearly, we can fit the model:\nY_i = \\beta X_i + \\tau T_i + \\varepsilon_i,\nwhere \\varepsilon_i is Gaussian noise.\n\nn = 500\nx = np.random.uniform(-3, 3, n)\nt = 1 * (x &lt; 0) # if x &lt; 0, you get the treatment, otherwise you get control\n\n\ntau = 4\ny = x + tau * t + np.random.normal(size=n)\n\n\nplt.scatter(x, y, c=['red' if i == 0 else 'blue' for i in t], marker='o')\nplt.plot(x, x, color = 'red', label='T=0')\nplt.plot(x, x + tau, color = 'blue', label='T=1')\nplt.legend()\n\n\n\n\n\n\n\n\nIf we do not adjust for X, we do not get the correct treatment effect:\n\n# no adjustment for x\nnp.mean(y[t==1]) - np.mean(y[t==0])\n\nnp.float64(1.055527008176574)\n\n\nAdjusting for X, we get the correct treatment effect:\n\ndf = pd.DataFrame({'t': t, 'x': x})\nmodel = sm.OLS(y, df).fit()\nmodel.params\n\nt    4.064826\nx    1.022550\ndtype: float64\n\n\nAnother way to calculate the treatment effect, adjusting for X:\n\nx = x.reshape(n, 1)\nH = x @ np.linalg.inv(x.T @ x) @ x.T\n# H = X(t(X)X)^{-1}t(X)\n\n\ny_adj = (np.eye(n) - H) @ y # residuals after regression on x\nt_adj = (np.eye(n) - H) @ t # residuals after regression on x\n\n\nmodel = sm.OLS(y_adj, t_adj).fit()\nmodel.params\n\narray([4.06482618])\n\n\n\n\nPropensity score\nWe could also use the propensity score to account for observed confounding, provided we have overlap.\nWe generate a simulated dataset:\n\nx = np.random.uniform(-3, 3, n)\n\nt_prob = 1/(1 + np.exp(-x)) # exp(x)/(1+exp(x)) -- this is the link function from logistic regression\nt = np.random.binomial(1, t_prob, n)\n\ntau = 4\ny = x + tau * t + np.random.normal(size=n)\n\nFor this simulated data, we check our min and max propensity scores to make sure overlap is satisfied. (That is, we can’t have P(T=1|X=x)=0 or 1.)\n\nprint(f\"Min propensity: {t_prob.min():.4f}, Max propensity: {t_prob.max():.4f}\")\n\nMin propensity: 0.0474, Max propensity: 0.9514\n\n\n\n# Plotting\nplt.scatter(x, y, c=['red' if i == 0 else 'blue' for i in t], marker='o')\nplt.plot(x, x, color='red', label='T=0')\nplt.plot(x, x + tau, color='blue', label='T=1')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.legend()\n\n\n\n\n\n\n\n\nIn practice, we do not know the propensity score. We estimate the propensity score using logistic regression (i.e. T vs. X).\n\n# Propensity score model - logistic regression predicting T from X (no Y)\n# q_hat(x) = estimated P(T=1 | X=x)\n\nX_prop = sm.add_constant(x)\nprop_model = sm.GLM(t, X_prop, family=sm.families.Binomial()).fit()\n\n\nprop_score = prop_model.predict(X_prop)\n\nQuick check: the estimated propensity score is close to the true propensity score:\n\nplt.figure(figsize=(2,2))\nplt.scatter(prop_score, t_prob)\nplt.xlabel('Estimated propensity')\nplt.ylabel('True propensity')\n\nText(0, 0.5, 'True propensity')\n\n\n\n\n\n\n\n\n\nWe now calculate the inverse probability weighting estimator as an estimator of the ATE:\nIPW = \\frac{1}{n}\\sum_{i=1}^n \\frac{T_i Y_i}{q(T_i|X_i)} + \\frac{(1-T_i)Y_i}{(1-q(T_i|X_i))}\n\n# Inverse probability weighting (IPW)\nipw = y * t / prop_score - (1 - t) * y / (1 - prop_score)\n\n# if t = 1, y/prop_score, if t = 0, y/(1-prop_score)\n\nprint(\"Mean of IPW:\", np.mean(ipw))\n\nMean of IPW: 4.115397597113825\n\n\n\n\nColliders\nWhat happens if we have a collider (instead of a confounder)?\nWe should NOT adjust for colliders. Let’s see what happens if we do in simulated data.\n\nn = 500\n\n# randomly draw treatment\nt = np.random.binomial(1, 0.5, size=n)\n\n# treatment effect\ntau = 4\n\n# outcome\ny = tau * t + np.random.normal(size=n)\n\nx = t + 0.5 * y + np.random.normal(scale = 0.2, size = n)\n\nTo estimate ATE, take difference in means of treatment and control:\n\nnp.mean(y[t==1]) - np.mean(y[t==0])\n\nnp.float64(4.072612320862982)\n\n\nIf we mistakenly include X, we do not estimate the ATE well (the estimate is negative instead of positive!)\n\ndf = pd.DataFrame({'t': t, 'x': x})\nmodel = sm.OLS(y, df).fit()\nmodel.params\n\nt   -1.193571\nx    1.743958\ndtype: float64\n\n\n\nAnother example of a collider\nHere, the true treatment effect is zero.\n\nn = 500\n\n# randomly draw treatment\nt = np.random.binomial(1, 0.5, size=n)\n\n# treatment effect\ntau = 0\n\n# outcome\ny = tau * t + np.random.normal(size=n)\n\nx = t + 0.5 * y + np.random.normal(scale = 0.2, size = n)\n\nThe difference in means of treatment and control is unbiased for the ATE:\n\nnp.mean(y[t==1]) - np.mean(y[t==0])\n\nnp.float64(-0.06499427497289205)\n\n\nBut if we mistakenly condition on the collider, we have a non-zero estimate, which is wrong.\n\ndf = pd.DataFrame({'t': t, 'x': x})\nmodel = sm.OLS(y, df).fit()\nmodel.params\n\nt   -1.725415\nx    1.713434\ndtype: float64"
  },
  {
    "objectID": "lec-4/lecture-4.html",
    "href": "lec-4/lecture-4.html",
    "title": "Lecture 4 - Transformers",
    "section": "",
    "text": "In this lecture, we study:\n\nattention\nbuilding a transformer\nfine-tuning language models\n\n\n\nNote: much of this code is from Andrej Karpathy’s excellent tutorials on building GPT from scratch:\n\nYoutube link\nGoogle Collab notebook link",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#overview",
    "href": "lec-4/lecture-4.html#overview",
    "title": "Lecture 4 - Transformers",
    "section": "",
    "text": "In this lecture, we study:\n\nattention\nbuilding a transformer\nfine-tuning language models\n\n\n\nNote: much of this code is from Andrej Karpathy’s excellent tutorials on building GPT from scratch:\n\nYoutube link\nGoogle Collab notebook link",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#attention",
    "href": "lec-4/lecture-4.html#attention",
    "title": "Lecture 4 - Transformers",
    "section": "Attention",
    "text": "Attention\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\n&lt;torch._C.Generator at 0x1170f8470&gt;\n\n\nWe are going to look at the Tiny Shakespeare dataset, which contains all the work of Shakespeare in a .txt file.\n\n# read it in to inspect it\nwith open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115393\n\n\n\nprint(text[:400])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it \n\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\nHow do we represent Tiny Shakespeare as numerical values?\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) } # string to integer\nitos = { i:ch for i,ch in enumerate(chars) } # integer to string\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hello there\"))\nprint(decode(encode(\"hello there\")))\n\n[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\nhello there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch \ndata = torch.tensor(encode(text), dtype=torch.float)\nprint(data.shape, data.dtype)\nprint(data[:200]) \n\ntorch.Size([1115393]) torch.float32\ntensor([18., 47., 56., 57., 58.,  1., 15., 47., 58., 47., 64., 43., 52., 10.,\n         0., 14., 43., 44., 53., 56., 43.,  1., 61., 43.,  1., 54., 56., 53.,\n        41., 43., 43., 42.,  1., 39., 52., 63.,  1., 44., 59., 56., 58., 46.,\n        43., 56.,  6.,  1., 46., 43., 39., 56.,  1., 51., 43.,  1., 57., 54.,\n        43., 39., 49.,  8.,  0.,  0., 13., 50., 50., 10.,  0., 31., 54., 43.,\n        39., 49.,  6.,  1., 57., 54., 43., 39., 49.,  8.,  0.,  0., 18., 47.,\n        56., 57., 58.,  1., 15., 47., 58., 47., 64., 43., 52., 10.,  0., 37.,\n        53., 59.,  1., 39., 56., 43.,  1., 39., 50., 50.,  1., 56., 43., 57.,\n        53., 50., 60., 43., 42.,  1., 56., 39., 58., 46., 43., 56.,  1., 58.,\n        53.,  1., 42., 47., 43.,  1., 58., 46., 39., 52.,  1., 58., 53.,  1.,\n        44., 39., 51., 47., 57., 46., 12.,  0.,  0., 13., 50., 50., 10.,  0.,\n        30., 43., 57., 53., 50., 60., 43., 42.,  8.,  1., 56., 43., 57., 53.,\n        50., 60., 43., 42.,  8.,  0.,  0., 18., 47., 56., 57., 58.,  1., 15.,\n        47., 58., 47., 64., 43., 52., 10.,  0., 18., 47., 56., 57., 58.,  6.,\n         1., 63., 53., 59.])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18., 47., 56., 57., 58.,  1., 15., 47., 58.])\n\n\nIn language modeling, we want to predict the next word in a sequence. For a given block, what are we predicting?\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18.]) the target: 47.0\nwhen input is tensor([18., 47.]) the target: 56.0\nwhen input is tensor([18., 47., 56.]) the target: 57.0\nwhen input is tensor([18., 47., 56., 57.]) the target: 58.0\nwhen input is tensor([18., 47., 56., 57., 58.]) the target: 1.0\nwhen input is tensor([18., 47., 56., 57., 58.,  1.]) the target: 15.0\nwhen input is tensor([18., 47., 56., 57., 58.,  1., 15.]) the target: 47.0\nwhen input is tensor([18., 47., 56., 57., 58.,  1., 15., 47.]) the target: 58.0\n\n\n\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,)) # select a random integer from len(data) - block_size\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(2): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[53., 59.,  6.,  1., 58., 56., 47., 40.],\n        [49., 43., 43., 54.,  1., 47., 58.,  1.],\n        [13., 52., 45., 43., 50., 53.,  8.,  0.],\n        [ 1., 39.,  1., 46., 53., 59., 57., 43.]])\ntargets:\ntorch.Size([4, 8])\ntensor([[59.,  6.,  1., 58., 56., 47., 40., 59.],\n        [43., 43., 54.,  1., 47., 58.,  1., 58.],\n        [52., 45., 43., 50., 53.,  8.,  0., 26.],\n        [39.,  1., 46., 53., 59., 57., 43.,  0.]])\n----\nwhen input is [53.0] the target: 59.0\nwhen input is [53.0, 59.0] the target: 6.0\nwhen input is [53.0, 59.0, 6.0] the target: 1.0\nwhen input is [53.0, 59.0, 6.0, 1.0] the target: 58.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0] the target: 56.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0] the target: 47.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0, 47.0] the target: 40.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0, 47.0, 40.0] the target: 59.0\nwhen input is [49.0] the target: 43.0\nwhen input is [49.0, 43.0] the target: 43.0\nwhen input is [49.0, 43.0, 43.0] the target: 54.0\nwhen input is [49.0, 43.0, 43.0, 54.0] the target: 1.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0] the target: 47.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0] the target: 58.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0, 58.0] the target: 1.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0, 58.0, 1.0] the target: 58.0\n\n\nAs discussed in class, attention is the weighted average of previous word embeddings:\nh_T = \\sum_{t=1}^T \\alpha_t x_t\nLet’s first write up the average (instead of weighted average). h_T = \\frac{1}{T}\\sum_{t=1}^T x_t\n\n# consider the following toy example:\n\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxrep = torch.zeros((B,T,C)) # this is our new representation, h\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xrep[b,t] = torch.mean(xprev, axis=0)\n\n\n# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxrep2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xrep, xrep2)\n\nTrue\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxrep3 = wei @ x\ntorch.allclose(xrep, xrep3)\n\nTrue\n\n\n\nwei\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (channels is the embedding size)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False) # W_K matrix\nquery = nn.Linear(C, head_size, bias=False) # W_Q matrix\nvalue = nn.Linear(C, head_size, bias=False) # W_V matrix\nk = key(x)   # (B, T, 16) W_K x\nq = query(x) # (B, T, 16) W_Q x\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)   \n\ntril = torch.tril(torch.ones(T, T)) \n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # mask the weight matrix so that we can't pay attention to future tokens\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\nFor regularization and to stabilize training, layer norm is used (instead of batch norm). Layer norm is the same as batch norm, except averages are taken over the sequence of tokens, not the batches.\n\nclass LayerNorm1d:\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # sequence mean (in batch norm, axis=0 instead)\n    xvar = x.var(1, keepdim=True) # sequence variance (in batch norm, axis=0 instead)\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#transformer",
    "href": "lec-4/lecture-4.html#transformer",
    "title": "Lecture 4 - Transformers",
    "section": "Transformer",
    "text": "Transformer\nHere is a small GPT model from Andrej Karpathy.",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#tokenization",
    "href": "lec-4/lecture-4.html#tokenization",
    "title": "Lecture 4 - Transformers",
    "section": "Tokenization",
    "text": "Tokenization\nTokenization is the process of turning text into discrete units (called tokens). We saw we could map letters to numbers as we did with Tiny Shakespeare. However, this can be very inefficient.\nMany modern tokenizers use an algorithm such as byte pair encoding that greedily merges commonly occurring sub-strings based on their frequency.\nUnderstanding Deep Learning, Figure 12.8\n\n\n\nUnderstanding Deep Learning\n\n\nTikTokenizer is a nice tool to see how LLMs encode text into tokens:\nTokenization is at the heart of much weirdness of LLMs. \n\n127 + 456 = 583\n\nApple.\nI have an apple.\napple.\nApple.\n\nfor i in range(1, 101):\n    if i % 2 == 0:\n        print(\"hello world\")",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#finetuning",
    "href": "lec-4/lecture-4.html#finetuning",
    "title": "Lecture 4 - Transformers",
    "section": "Finetuning",
    "text": "Finetuning\nWe now look at finetuning a language representation model called DistilBERT (Sanh et al. 2019) [arXiv].\nDistilBERT is a 40% smaller, distilled version of BERT, which retains 97% of the original BERT model capabilities.\nWe can obtain pretrained models from Hugging Face.\nWe need to install Hugging Face’s transformers package:\npip install transformers\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import nn\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nIncluded also is the model tokenizer.\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\nbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n\nexample_text = \"Hello world\"\n\ntoken_res = tokenizer(example_text, return_tensors='pt', max_length=10, padding='max_length') # pt is for pytorch tensors\n\nprint(token_res['input_ids']) ## vector of token IDs\nprint(token_res['attention_mask']) ## vector of 0/1 to indicate real tokens vs padding tokens\n\nout_text = tokenizer.decode(token_res['input_ids'][0])\nprint(out_text)\n\ntensor([[ 101, 7592, 2088,  102,    0,    0,    0,    0,    0,    0]])\ntensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n[CLS] hello world [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\n\n\nbert_model.config.hidden_size\n\n768\n\n\n\nWine dataset\nWe will finetune a wine dataset to predict price based on the wine description.\nThe dataset has 120K wines.\n\nwine_df = pd.read_csv(\"data/wines.csv\")\n\n## keep only wines whose price is not NaN\nwine_df = wine_df[wine_df['price'].notna()]\n\nprint(wine_df.shape)\n\n## key variables: price, description\nfor i in range(3):\n    print(\"Description: \", wine_df['description'].iloc[i])\n    print(\"Price: \", wine_df['price'].iloc[i])\n\n## find the wine with the highest price\nmax_price_idx = wine_df['price'].argmax()\nprint(\"Most expensive wine: \", wine_df['description'].iloc[max_price_idx])\nprint(\"Price: \", wine_df['price'].iloc[max_price_idx])\n\n## make box-plot of prices\nplt.boxplot(np.log(wine_df['price']))\n\n(120975, 14)\nDescription:  This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.\nPrice:  15.0\nDescription:  Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.\nPrice:  14.0\nDescription:  Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.\nPrice:  13.0\nMost expensive wine:  This ripe wine shows plenty of blackberry fruits balanced well with some dry tannins. It is fresh, juicy with plenty of acidity, For a light vintage, it's perfumed, full of fresh flavors and will be ready to drink from 2017.\nPrice:  3300.0\n\n\n{'whiskers': [&lt;matplotlib.lines.Line2D at 0x34ea5d6d0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x34e9eb4d0&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x34e9e91d0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x34e9e8e10&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x34ee4f890&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x34e9bfed0&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x34e9bd6d0&gt;],\n 'means': []}\n\n\n\n\n\n\n\n\n\n\nclass textClassDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        self.tokens = tokenizer(df['description'].tolist(), return_tensors='pt', max_length=self.max_len, \n                                      padding='max_length', truncation=True) \n        self.price = torch.tensor(df['price'].to_numpy(), dtype=torch.float)\n\n    def __len__(self):\n        return len(self.price)\n    \n    def __getitem__(self, idx):\n        \n        input_ids = self.tokens['input_ids'][idx]\n        attention_mask = self.tokens['attention_mask'][idx]\n        price = self.price[idx]\n\n        return input_ids, attention_mask, price\n\n\ndataset = textClassDataset(wine_df, tokenizer, 128)\n\n## split into train and test datasets\ntrain_size = int(0.5 * len(dataset))\ntest_size = len(dataset) - train_size\n\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\nprint(\"training data size: \" + str(len(train_dataset)))\n\n## create dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n\ntraining data size: 60487\n\n\n\nbatch = next(iter(train_dataloader))\n\n\nclass BertRegressor(nn.Module):\n    def __init__(self):\n        super(BertRegressor, self).__init__()\n        self.bert = bert_model\n\n        ## for distilbert-base-uncased, hidden_size is 768\n        self.layer1 = nn.Linear(self.bert.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = bert_outputs[0][:, 0, :] # [CLS] token1 token2 ... this grabs [CLS] token\n\n        x = self.layer1(pooled_output)\n        x = x.squeeze(1)\n        return x\n    \n\n\nmodel = BertRegressor()\n\n\n## We can freeze bert parameters so that we only update the\n## prediction head\n# for param in model.bert.parameters():\n#     param.requires_grad = False\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=1e-4)\n\nnum_epochs = 1\n\nmodel.train()\n\n\nTraining\n\n1 epoch of training takes about 20 mins.\nYou can skip the training to directly load from the saved model parameter file.\n\n\nit = 0\n\nfor epoch in range(num_epochs):\n\n    for batch in train_dataloader:\n\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        logprice = np.log(batch[2])\n\n        pred = model(input_ids, attention_mask)\n        loss = loss_fn(pred, logprice)\n        loss.backward()\n        optimizer.step()\n\n        optimizer.zero_grad()\n        \n        it = it + 1\n        if (it % 100 == 0):\n            print(\"epoch: \", epoch, \"sgd iter: \" + str(it))\n    print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n\n\n## save model parameters\ntorch.save(model.state_dict(), \"model/fine-tuned-distilbert.pt\")\n\n\n## load model\nmodel.load_state_dict(torch.load(\"model/fine-tuned-distilbert.pt\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\n## calculate testing error\n## takes about 2 minutes to run\n\nmodel.eval()\nmse = 0\n\nn_test = 600\n\ny_test = np.array([np.log(test_dataset[i][2]) for i in range(n_test)])\n\nfor i in range(n_test):\n    pred = model(test_dataset[i][0].unsqueeze(0), test_dataset[i][1].unsqueeze(0))\n    mse = mse + (pred - y_test[i])**2\n\nmse = mse / n_test\n\nprint(\"MSE:\", mse.item(), \"  Test R-squared:\", 1 - mse.item() / np.var(y_test))\n\n/var/folders/f0/m7l23y8s7p3_0x04b3td9nyjr2hyc8/T/ipykernel_6343/1264298161.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n  y_test = np.array([np.log(test_dataset[i][2]) for i in range(n_test)])\n\n\nMSE: 0.21289405226707458   Test R-squared: 0.4635502\n\n\n\n## \n\nmy_reviews = [\"This white is both sour and bitter; it has a funny smell\",\n                \"the most amazing wine I have ever tasted\",\n                \"not bad at all; I would buy it again\",\n                \"actually quite bad; avoid if possible\",\n                \"great red and pretty cheap\",\n                \"great red but overpriced\",\n                \"great red and great price\"]\n\nfor my_review in my_reviews:\n\n    token_res = tokenizer(my_review, return_tensors='pt')\n\n    pred = model(token_res['input_ids'], token_res['attention_mask'])\n    \n    print(\"My Description:\", my_review)\n    print(\"Predicted price: \", torch.exp(pred).item(), '\\n')\n\nMy Description: This white is both sour and bitter; it has a funny smell\nPredicted price:  15.726286888122559 \n\nMy Description: the most amazing wine I have ever tasted\nPredicted price:  47.7934684753418 \n\nMy Description: not bad at all; I would buy it again\nPredicted price:  12.506088256835938 \n\nMy Description: actually quite bad; avoid if possible\nPredicted price:  14.870586395263672 \n\nMy Description: great red and pretty cheap\nPredicted price:  12.206925392150879 \n\nMy Description: great red but overpriced\nPredicted price:  24.468101501464844 \n\nMy Description: great red and great price\nPredicted price:  12.49232006072998",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#resources",
    "href": "lec-4/lecture-4.html#resources",
    "title": "Lecture 4 - Transformers",
    "section": "Resources",
    "text": "Resources\n\nText Classification with BERT\nHugging Face Tutorial\nTokenization discussion",
    "crumbs": [
      "Home",
      "Lecture 4 - Transformers"
    ]
  },
  {
    "objectID": "lec-3/lecture-3.html",
    "href": "lec-3/lecture-3.html",
    "title": "Lecture 3 - CNNs",
    "section": "",
    "text": "This notebook is adapted from ISLP.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\n\nFirst, download CIFAR100 data.\n\nbatch_size = 128\n\n(cifar_train,\n cifar_test) = [torchvision.datasets.CIFAR100(root=\"../data\",\n                         train=train,\n                         download=True)\n             for train in [True, False]]\n\nlabel_names = cifar_test.classes\n\ntransform = torchvision.transforms.ToTensor()\ncifar_train_X = torch.stack([transform(x) for x in\n                            cifar_train.data])\ncifar_test_X = torch.stack([transform(x) for x in\n                            cifar_test.data])\ncifar_train = TensorDataset(cifar_train_X,\n                            torch.tensor(cifar_train.targets))\ncifar_test = TensorDataset(cifar_test_X,\n                            torch.tensor(cifar_test.targets))\n\ntrain_loader = DataLoader(cifar_train, batch_size=batch_size)\ntest_loader = DataLoader(cifar_test, batch_size=1000)\n\n\nfig, axes = plt.subplots(5, 5, figsize=(10,10))\nrng = np.random.default_rng(4)\nindices = rng.choice(np.arange(len(cifar_train)), 25,\n                     replace=False).reshape((5,5))\nfor i in range(5):\n    for j in range(5):\n        idx = indices[i,j]\n        axes[i,j].imshow(np.transpose(cifar_train[idx][0],\n                                      [1,2,0]),\n                                      interpolation=None)\n        axes[i,j].set_xticks([])\n        axes[i,j].set_yticks([])\n\n\n# note that torch requires image tensors to be [N, C, H, W]\n# N: samples, C: channels, H: height, W: width\n\n\n\n\n\n\n\n\nLet’s set up our convolutional neural network for CIFAR100.\n\n\nclass BuildingBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n\n        super(BuildingBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=(3,3),\n                              padding='same')\n        self.activation = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n\n    def forward(self, x):\n        return self.pool(self.activation(self.conv(x)))\n\n# If the syntax *expression appears in the function call,\n# expression must evaluate to an iterable. Elements from\n# this iterable are treated as if they were additional\n# positional arguments; if there are positional arguments\n# x1, ..., xN, and expression evaluates to a sequence y1, ..., yM,\n# this is equivalent to a call with M+N positional arguments x1, ..., xN, y1, ..., yM.\n\nclass CIFARModel(nn.Module):\n\n    def __init__(self):\n        super(CIFARModel, self).__init__()\n        sizes = [(3,32),\n                 (32,64),\n                 (64,128),\n                 (128,256)]\n        self.conv = nn.Sequential(*[BuildingBlock(in_, out_)\n                                    for in_, out_ in sizes])  # A single star * unpacks a sequence into positional arguments\n\n        self.output = nn.Sequential(nn.Dropout(0.5),\n                                    nn.Linear(2*2*256, 512),\n                                    nn.ReLU(),\n                                    nn.Linear(512, 100),\n                                    nn.Softmax())\n    def forward(self, x):\n        val = self.conv(x)\n        val = torch.flatten(val, start_dim=1)\n        return self.output(val)\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nLet’s instantiate the CIFAR100 model. Note: training the CIFAR100 model takes awhile (43 minutes)\n\nmodel = CIFARModel()\n\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    epoch_loss = 0\n\n    for x_batch, y_batch in train_loader:\n\n        y_pred = model(x_batch)\n        y_batch = F.one_hot(y_batch, num_classes=100)\n\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_loss += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n\n\nepoch:  0 loss: 1.61e+03\nepoch:  10 loss: 8.59e+02\nepoch:  20 loss: 6.62e+02\nepoch:  30 loss: 5.58e+02\nepoch:  40 loss: 4.91e+02\n\n\nWe save the trained CIFAR100 model so we can access it later.\n\nfilename = 'cifar100-model'\n\n\ntorch.save(model.state_dict(), filename)\n\nLoad model:\n\nmodel.load_state_dict(torch.load(filename))\n\n&lt;All keys matched successfully&gt;\n\n\nLet’s calculate the accuracy of the model on a subset of the data.\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n\n\n\nacc\n\ntensor(0.4410)\n\n\nA random classifier for 100 classes would get 1%. So not bad… but better models can get up to 90s.\nLet’s look at some examples the model got wrong:\n\ncorrect =  torch.where(y_pred == y_batch)[0]\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = np.random.choice(correct.numpy(), size = 8)\ninds2 = np.random.choice(errors.numpy(), size = 8)\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, num_cols, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = x_batch[n], y_batch[n]\n    label = label_names[label]\n\n    plt.imshow(np.transpose(image,\n                        [1, 2, 0]),\n           interpolation=None)\n    plt.xlabel(label)\n    predicted_label = label_names[y_pred[n]]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"true={}, pred={}\".format(\n        label,\n        predicted_label),\n        color=color)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\n\nThe torchvision.models package provides pre-trained models like ResNet-50 (a ResNet model with 50 layers). Here, we get the predictions from ResNet-50 for pictures of the professors’ dogs. (Note: this uses code from ISLP)\n\nfrom torchvision.models import (resnet50, ResNet50_Weights)\nfrom torchvision.transforms import (Resize,\n                                    Normalize,\n                                    CenterCrop,\n                                    ToTensor)\nfrom torchvision.io import read_image\nfrom glob import glob\nimport json\nimport pandas as pd\n\n\nresnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n\n# We need to normalize the images for use for ResNet-50\n\nresize = Resize((232,232), antialias=True)\ncrop = CenterCrop(224)\nnormalize = Normalize([0.485,0.456,0.406],\n                      [0.229,0.224,0.225])\nimgfiles = sorted([f for f in glob('dogs/*')])\nimgs = torch.stack([torch.div(crop(resize(read_image(f))), 255)\n                    for f in imgfiles])\nimgs_no_norm = imgs\nimgs = normalize(imgs)\nimgs.size()\n\ntorch.Size([3, 3, 224, 224])\n\n\n\nplt.figure(figsize=(2 * len(imgfiles), 2))\n\nfor i, imgfile in enumerate(imgfiles):\n    plt.subplot(1, len(imgfiles), i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n\n    plt.imshow(np.transpose(imgs_no_norm[i].numpy(),\n                        [1, 2, 0]),\n           interpolation=None)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\nresnet_model.eval()\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\nimg_preds = resnet_model(imgs)\n\n\nimg_preds.shape\n\ntorch.Size([3, 1000])\n\n\n\n# apply softmax to the outputs of the ResNet\nimg_probs = np.exp(np.asarray(img_preds.detach()))\nimg_probs /= img_probs.sum(1)[:,None]\n\n\n# get class labels -- this is available at https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nlabs = json.load(open('imagenet_class_index.json'))\nclass_labels = pd.DataFrame([(int(k), v[1]) for k, v in \n                           labs.items()],\n                           columns=['idx', 'label'])\nclass_labels = class_labels.set_index('idx')\nclass_labels = class_labels.sort_index()\n\n\nfor i, imgfile in enumerate(imgfiles):\n    img_df = class_labels.copy()\n    img_df['prob'] = img_probs[i]\n    img_df = img_df.sort_values(by='prob', ascending=False)[:3]\n    print(f'Image: {imgfile}')\n    print(img_df.reset_index().drop(columns=['idx']))\n\nImage: dogs/bonnie.JPG\n               label      prob\n0  Brabancon_griffon  0.243078\n1      affenpinscher  0.013340\n2           Pekinese  0.004563\nImage: dogs/maya.jpeg\n                label      prob\n0            malinois  0.221570\n1     German_shepherd  0.102747\n2  Norwegian_elkhound  0.007779\nImage: dogs/milo.jpeg\n              label      prob\n0        toy_poodle  0.302865\n1  miniature_poodle  0.133538\n2          Shih-Tzu  0.024314",
    "crumbs": [
      "Home",
      "Lecture 3 - CNNs"
    ]
  },
  {
    "objectID": "lec-3/lecture-3.html#convolutional-neural-networks",
    "href": "lec-3/lecture-3.html#convolutional-neural-networks",
    "title": "Lecture 3 - CNNs",
    "section": "",
    "text": "This notebook is adapted from ISLP.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\n\nFirst, download CIFAR100 data.\n\nbatch_size = 128\n\n(cifar_train,\n cifar_test) = [torchvision.datasets.CIFAR100(root=\"../data\",\n                         train=train,\n                         download=True)\n             for train in [True, False]]\n\nlabel_names = cifar_test.classes\n\ntransform = torchvision.transforms.ToTensor()\ncifar_train_X = torch.stack([transform(x) for x in\n                            cifar_train.data])\ncifar_test_X = torch.stack([transform(x) for x in\n                            cifar_test.data])\ncifar_train = TensorDataset(cifar_train_X,\n                            torch.tensor(cifar_train.targets))\ncifar_test = TensorDataset(cifar_test_X,\n                            torch.tensor(cifar_test.targets))\n\ntrain_loader = DataLoader(cifar_train, batch_size=batch_size)\ntest_loader = DataLoader(cifar_test, batch_size=1000)\n\n\nfig, axes = plt.subplots(5, 5, figsize=(10,10))\nrng = np.random.default_rng(4)\nindices = rng.choice(np.arange(len(cifar_train)), 25,\n                     replace=False).reshape((5,5))\nfor i in range(5):\n    for j in range(5):\n        idx = indices[i,j]\n        axes[i,j].imshow(np.transpose(cifar_train[idx][0],\n                                      [1,2,0]),\n                                      interpolation=None)\n        axes[i,j].set_xticks([])\n        axes[i,j].set_yticks([])\n\n\n# note that torch requires image tensors to be [N, C, H, W]\n# N: samples, C: channels, H: height, W: width\n\n\n\n\n\n\n\n\nLet’s set up our convolutional neural network for CIFAR100.\n\n\nclass BuildingBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n\n        super(BuildingBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=(3,3),\n                              padding='same')\n        self.activation = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n\n    def forward(self, x):\n        return self.pool(self.activation(self.conv(x)))\n\n# If the syntax *expression appears in the function call,\n# expression must evaluate to an iterable. Elements from\n# this iterable are treated as if they were additional\n# positional arguments; if there are positional arguments\n# x1, ..., xN, and expression evaluates to a sequence y1, ..., yM,\n# this is equivalent to a call with M+N positional arguments x1, ..., xN, y1, ..., yM.\n\nclass CIFARModel(nn.Module):\n\n    def __init__(self):\n        super(CIFARModel, self).__init__()\n        sizes = [(3,32),\n                 (32,64),\n                 (64,128),\n                 (128,256)]\n        self.conv = nn.Sequential(*[BuildingBlock(in_, out_)\n                                    for in_, out_ in sizes])  # A single star * unpacks a sequence into positional arguments\n\n        self.output = nn.Sequential(nn.Dropout(0.5),\n                                    nn.Linear(2*2*256, 512),\n                                    nn.ReLU(),\n                                    nn.Linear(512, 100),\n                                    nn.Softmax())\n    def forward(self, x):\n        val = self.conv(x)\n        val = torch.flatten(val, start_dim=1)\n        return self.output(val)\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nLet’s instantiate the CIFAR100 model. Note: training the CIFAR100 model takes awhile (43 minutes)\n\nmodel = CIFARModel()\n\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    epoch_loss = 0\n\n    for x_batch, y_batch in train_loader:\n\n        y_pred = model(x_batch)\n        y_batch = F.one_hot(y_batch, num_classes=100)\n\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_loss += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n\n\nepoch:  0 loss: 1.61e+03\nepoch:  10 loss: 8.59e+02\nepoch:  20 loss: 6.62e+02\nepoch:  30 loss: 5.58e+02\nepoch:  40 loss: 4.91e+02\n\n\nWe save the trained CIFAR100 model so we can access it later.\n\nfilename = 'cifar100-model'\n\n\ntorch.save(model.state_dict(), filename)\n\nLoad model:\n\nmodel.load_state_dict(torch.load(filename))\n\n&lt;All keys matched successfully&gt;\n\n\nLet’s calculate the accuracy of the model on a subset of the data.\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n\n\n\nacc\n\ntensor(0.4410)\n\n\nA random classifier for 100 classes would get 1%. So not bad… but better models can get up to 90s.\nLet’s look at some examples the model got wrong:\n\ncorrect =  torch.where(y_pred == y_batch)[0]\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = np.random.choice(correct.numpy(), size = 8)\ninds2 = np.random.choice(errors.numpy(), size = 8)\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, num_cols, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = x_batch[n], y_batch[n]\n    label = label_names[label]\n\n    plt.imshow(np.transpose(image,\n                        [1, 2, 0]),\n           interpolation=None)\n    plt.xlabel(label)\n    predicted_label = label_names[y_pred[n]]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"true={}, pred={}\".format(\n        label,\n        predicted_label),\n        color=color)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\n\nThe torchvision.models package provides pre-trained models like ResNet-50 (a ResNet model with 50 layers). Here, we get the predictions from ResNet-50 for pictures of the professors’ dogs. (Note: this uses code from ISLP)\n\nfrom torchvision.models import (resnet50, ResNet50_Weights)\nfrom torchvision.transforms import (Resize,\n                                    Normalize,\n                                    CenterCrop,\n                                    ToTensor)\nfrom torchvision.io import read_image\nfrom glob import glob\nimport json\nimport pandas as pd\n\n\nresnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n\n# We need to normalize the images for use for ResNet-50\n\nresize = Resize((232,232), antialias=True)\ncrop = CenterCrop(224)\nnormalize = Normalize([0.485,0.456,0.406],\n                      [0.229,0.224,0.225])\nimgfiles = sorted([f for f in glob('dogs/*')])\nimgs = torch.stack([torch.div(crop(resize(read_image(f))), 255)\n                    for f in imgfiles])\nimgs_no_norm = imgs\nimgs = normalize(imgs)\nimgs.size()\n\ntorch.Size([3, 3, 224, 224])\n\n\n\nplt.figure(figsize=(2 * len(imgfiles), 2))\n\nfor i, imgfile in enumerate(imgfiles):\n    plt.subplot(1, len(imgfiles), i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n\n    plt.imshow(np.transpose(imgs_no_norm[i].numpy(),\n                        [1, 2, 0]),\n           interpolation=None)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\nresnet_model.eval()\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\nimg_preds = resnet_model(imgs)\n\n\nimg_preds.shape\n\ntorch.Size([3, 1000])\n\n\n\n# apply softmax to the outputs of the ResNet\nimg_probs = np.exp(np.asarray(img_preds.detach()))\nimg_probs /= img_probs.sum(1)[:,None]\n\n\n# get class labels -- this is available at https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nlabs = json.load(open('imagenet_class_index.json'))\nclass_labels = pd.DataFrame([(int(k), v[1]) for k, v in \n                           labs.items()],\n                           columns=['idx', 'label'])\nclass_labels = class_labels.set_index('idx')\nclass_labels = class_labels.sort_index()\n\n\nfor i, imgfile in enumerate(imgfiles):\n    img_df = class_labels.copy()\n    img_df['prob'] = img_probs[i]\n    img_df = img_df.sort_values(by='prob', ascending=False)[:3]\n    print(f'Image: {imgfile}')\n    print(img_df.reset_index().drop(columns=['idx']))\n\nImage: dogs/bonnie.JPG\n               label      prob\n0  Brabancon_griffon  0.243078\n1      affenpinscher  0.013340\n2           Pekinese  0.004563\nImage: dogs/maya.jpeg\n                label      prob\n0            malinois  0.221570\n1     German_shepherd  0.102747\n2  Norwegian_elkhound  0.007779\nImage: dogs/milo.jpeg\n              label      prob\n0        toy_poodle  0.302865\n1  miniature_poodle  0.133538\n2          Shih-Tzu  0.024314",
    "crumbs": [
      "Home",
      "Lecture 3 - CNNs"
    ]
  },
  {
    "objectID": "lec-5/lecture-5.html",
    "href": "lec-5/lecture-5.html",
    "title": "Lecture 5 - VAEs",
    "section": "",
    "text": "import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse:\n    device = torch.device('cpu')\n    print('cuda/mps not available, using cpu instead')\n\n\n(mnist_train,\n mnist_test) = [torchvision.datasets.MNIST(root='../data',\n                      train=train,\n                      download=True,\n                      transform=torchvision.transforms.ToTensor())\n                for train in [True, False]]\n\n\ntrain_loader = DataLoader(dataset=mnist_train, batch_size=64, shuffle=True, num_workers=4)\ntest_loader = DataLoader(dataset=mnist_test, batch_size=64, shuffle=False)\n\n\n# what does one data batch look like?\n# fetch the next batch of data from the train_loader\n# iter(train_loader) converts the train_loader into an iterator\n# next(...) retrieves the next batch from the iterator\nbatch = next(iter(train_loader))\n\n\nbatch[0].shape  # this is the image, dims are: B, C, H, W (gray image so C=1)\n\ntorch.Size([64, 1, 28, 28])\n\n\n\nimgs = batch[0]\n\n\n# note: when we input the data, it will need to be B, H * W\nimgs = imgs.reshape(imgs.shape[0], 28*28)\n\n\nimgs.shape # this shape will be what we input to the VAE\n\ntorch.Size([64, 784])\n\n\n\nprint(torch.min(imgs), torch.max(imgs)) # range of pixel values. 0 is white pixels, 1 is black pixels, in between are gray\n\ntensor(0.) tensor(1.)\n\n\n\nbatch[1] # this is the label for each image\n\ntensor([2, 5, 2, 3, 5, 1, 6, 5, 7, 0, 8, 3, 0, 2, 1, 6, 0, 5, 7, 1, 4, 6, 1, 0,\n        7, 1, 2, 8, 1, 3, 3, 8, 3, 9, 1, 9, 3, 7, 7, 0, 9, 6, 8, 3, 3, 4, 5, 2,\n        5, 0, 2, 9, 2, 2, 9, 0, 7, 5, 0, 9, 1, 0, 8, 3])\n\n\n\nPlot data examples\n\nn_img = 10\nnum_cols = 5\nnum_rows = 2\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(n_img):\n\n  # for each label, get an image from the test dataset with that label\n\n  inds = torch.where(mnist_test.test_labels == i)[0].numpy()\n  ind = np.random.choice(inds)\n\n  x, label = mnist_test[ind]\n\n  plt.subplot(num_rows, 2 * num_cols, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(x[0].detach().cpu().numpy(), cmap=plt.cm.binary)\n\n\n\n\n\n\n\n\n\n\nDefine VAE class\n\nclass VAE(torch.nn.Module):\n    def __init__(self, input_dim, latent_dim, hidden_dim, loss_type='mse'):\n        super(VAE, self).__init__()\n        self.loss_type = loss_type\n\n        # define encoder and decoder functions in the init\n        self.q_z = nn.Sequential(nn.Linear(input_dim, hidden_dim),\n                                 nn.ReLU(),\n                                 nn.Linear(hidden_dim, hidden_dim),\n                                 nn.ReLU())\n\n        self.z_mean = nn.Linear(hidden_dim, latent_dim)\n        self.z_log_var = nn.Linear(hidden_dim, latent_dim)\n\n        # decoder\n        self.generator = nn.Sequential(nn.Linear(latent_dim, hidden_dim),\n                                       nn.ReLU(),\n                                       nn.Linear(hidden_dim, hidden_dim),\n                                       nn.ReLU(),\n                                       nn.Linear(hidden_dim, input_dim))\n\n    def encode(self, x):\n        q_z = self.q_z(x)\n        z_mean = self.z_mean(q_z)\n        z_log_var = self.z_log_var(q_z)\n        return z_mean, z_log_var\n\n    def reparameterize(self, mean, log_var):\n        std = torch.exp(0.5 * log_var) # std \n        eps = torch.randn_like(std)\n        sample = mean + eps * std # sample from q(z) ~ N(z_mean, exp(z_log_var))\n        return sample\n\n    def decode(self, z):\n        x_reconstructed = self.generator(z)\n\n        if self.loss_type == 'binary':\n            x_reconstructed = F.sigmoid(x_reconstructed)\n\n        return x_reconstructed\n    \n    def forward(self, x):\n        # encoding to get z_mean, z_log_var\n        z_mean, z_log_var = self.encode(x)\n\n        # draw sample from q(z) ~ N(z_mean, z_log_var)\n        z = self.reparameterize(z_mean, z_log_var)\n\n        # pass sample through decoder: E_{q(z)}[|| x - g(z)||^] \\approx ||x - g(z^{(l)}||^2) where z^{(l)} is the sample\n        x_mean = self.decode(z)\n        # f_theta(z^l) # one monte carlo sample\n        return x_mean, z, z_mean, z_log_var\n\n    def reconstruction_loss(self, x_pred, x):\n        if self.loss_type == 'mse':\n            loss = nn.MSELoss()\n            reconstruction_loss = 0.5 * loss(x_pred, x)\n\n        if self.loss_type == 'binary':\n            reconstruction_loss = x * torch.log(x_pred + 1e-6) + \\\n                  (1-x) * torch.log(1 - x_pred + 1e-6)\n            reconstruction_loss = -reconstruction_loss.sum(1).mean()\n\n        return reconstruction_loss\n\n    def vae_loss(self, x):\n\n        x_mean, z, z_mean, z_log_var = self.forward(x)\n\n        \n        kld = 1 + z_log_var - z_mean.pow(2) - z_log_var.exp()\n        kl_loss = -0.5 * torch.mean(kld)\n\n\n        reconstruction_loss = self.reconstruction_loss(x_mean, x)\n\n        return reconstruction_loss, kl_loss\n    \n    # total loss = reconstruction_loss + kl_loss &lt;- -ELBO\n\n\ninput_dim = 28 * 28 # mnist pixels 28x28=784\nhidden_dim = 300\nlatent_dim = 2 # we are compressing each image of 784 pixels to a 10 dim vector - we are compressing a lot of information\n\n\nmodel = VAE(input_dim, latent_dim, hidden_dim, loss_type='binary') # loss type = 'binary' because pixels are in [0, 1]\nlr = 1e-3\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.to(device)\nmodel.train()\n\nfor epoch in range(epochs):\n\n    epoch_loss = 0\n    epoch_recon = 0\n    epoch_kl = 0\n\n    for x_batch, _ in train_loader:\n\n        x_batch = x_batch.to(device).float()\n        x_batch = x_batch.reshape(x_batch.shape[0], 28*28)\n        reconstruction_loss, kl_loss = model.vae_loss(x_batch)\n        total_loss = reconstruction_loss + kl_loss\n\n        total_loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_loss += total_loss.item()\n        epoch_recon += reconstruction_loss.item()\n        epoch_kl += kl_loss.item()\n\n    epoch_loss = epoch_loss / len(train_loader)\n    epoch_recon = epoch_recon / len(train_loader)\n    epoch_kl = epoch_kl / len(train_loader)\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\", 'recon loss:', f\"{epoch_recon:.3}\",\n              'kld:', f\"{epoch_kl:.3}\")\n\nepoch:  0 loss: 1.75e+02 recon loss: 1.72e+02 kld: 2.66\nepoch:  10 loss: 1.39e+02 recon loss: 1.36e+02 kld: 3.7\nepoch:  20 loss: 1.36e+02 recon loss: 1.32e+02 kld: 3.83\nepoch:  30 loss: 1.35e+02 recon loss: 1.31e+02 kld: 3.91\nepoch:  40 loss: 1.34e+02 recon loss: 1.3e+02 kld: 3.95\n\n\n\n# save model\ntorch.save(model.state_dict(), \"vae_latent_\" + str(latent_dim) + \".pt\")\n\n\n# load models\nlatent_dim = 10\nmodel = VAE(input_dim, latent_dim, hidden_dim, loss_type='binary')\n\n\nmodel.load_state_dict(torch.load(\"vae_latent_\" + str(latent_dim) + \".pt\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\n\nPlot reconstructions\n\nsee how reconstructions change for models with different latent_dim\n\n\nmodel.eval()\n\nn_img = 10\nnum_cols = 5\nnum_rows = 2\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(n_img):\n\n  # for each label, get an image from the test dataset with that label\n\n  inds = torch.where(mnist_test.test_labels == i)[0].numpy()\n  ind = np.random.choice(inds)\n\n  x, label = mnist_test[ind]\n  x_batch = x.reshape(x.shape[0], 28*28)\n\n  x_mean, z, z_mean, z_log_var = model(x_batch.to(device).float())\n  x_recon = x_mean.reshape((28,28))\n\n  plt.subplot(num_rows, 2 * num_cols, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(x[0].detach().cpu().numpy(), cmap=plt.cm.binary)\n\n  plt.subplot(num_rows, 2 * num_cols, (i + 1) + 2 * num_cols)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(x_recon.detach().cpu().numpy(), cmap=plt.cm.binary)\n\n\n\n\n\n\n\n\n\n\n\nVAE\n\n\n\n\nPlot the 2-dimensional latent space\nBefore we had latent_dim=10. Now let’s look at latent_dim=2 (more compression)\n\n# load models\nlatent_dim = 2\nmodel = VAE(input_dim, latent_dim, hidden_dim, loss_type='binary')\n\nmodel.load_state_dict(torch.load(\"vae_latent_\" + str(latent_dim) + \".pt\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\nmodel.eval()\n\nn_img = 10\nnum_cols = 5\nnum_rows = 2\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(n_img):\n\n  # for each label, get an image from the test dataset with that label\n\n  inds = torch.where(mnist_test.test_labels == i)[0].numpy()\n  ind = np.random.choice(inds)\n\n  x, label = mnist_test[ind]\n  x_batch = x.reshape(x.shape[0], 28*28)\n\n  x_mean, z, z_mean, z_log_var = model(x_batch)\n  x_recon = x_mean.reshape((28,28))\n\n  plt.subplot(num_rows, 2 * num_cols, i + 1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(x[0].detach().cpu().numpy(), cmap=plt.cm.binary)\n\n  plt.subplot(num_rows, 2 * num_cols, (i + 1) + 2 * num_cols)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(x_recon.detach().cpu().numpy(), cmap=plt.cm.binary)\n\n\n\n\n\n\n\n\n\nlabels = mnist_test.test_labels\nlabels = labels.numpy()\nlabels = labels.reshape(-1, 1)\n\n\nx_test = mnist_test.data\nx_test = x_test.reshape(x_test.shape[0], 28*28)\nx_test = x_test.to(torch.float) \n\n\nz_test, _ = model.encode(x_test)\nz_np = z_test.detach().cpu().numpy()\n\n\ndf = pd.DataFrame(z_np, columns = ['z' + str(i+1) for i in range(z_np.shape[1])])\ndf['label'] = labels\ndf['label'] = df['label'].astype('category')\n\n\n# Create a scatter plot with colored clusters\nsns.scatterplot(x='z1', y='z2', hue='label', data=df)\n\n\n\n\n\n\n\n\n\n\nCompare with PCA\n\nPCA does linear dimension reduction (VAE does nonlinear dimension reduction)\n\n\nfrom sklearn.preprocessing import StandardScaler\nx_test_std = StandardScaler().fit_transform(x_test)\nprint(x_test_std.shape)\n\nlabels = mnist_test.test_labels\nlabels = labels.numpy()\n# labels = labels.reshape(-1, 1)\n\nfrom sklearn import decomposition\npca = decomposition.PCA()\n\npca.n_components = 2\n# select 2 components for 2-D visualization \npca_data = pca.fit_transform(x_test_std)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)\n\npca_data = np.vstack((pca_data.T, labels)).T\npca_df = pd.DataFrame(data=pca_data, columns=(\"z1\", \"z2\", \"label\"))\nsns.FacetGrid(pca_df, hue=\"label\", height=6, aspect=1).map(plt.scatter, 'z1', 'z2', s=10).add_legend()\n# sns.scatterplot(x='z1', y='z2', hue='label', data=pca_df)\nplt.show()\n\n(10000, 784)\nshape of pca_reduced.shape =  (10000, 2)\n\n\n\n\n\n\n\n\n\n\n\nPlot latent traversals\n\nHow can we better understand what the dimensions of the VAE latent space correspond to?\nWe can plot latent traversals for latent_dim=10\nWe consider a range of z values and see what their corresponding reconstructed x image looks like\n\n\n# load models\nlatent_dim = 10\nmodel = VAE(input_dim, latent_dim, hidden_dim, loss_type='binary')\n\nmodel.load_state_dict(torch.load(\"vae_latent_\" + str(latent_dim) + \".pt\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\n# Create a grid of latent vectors for traversal\nn_samples = 20\ngrid_x = np.linspace(-3, 3, n_samples)\ngrid_y = np.linspace(-3, 3, n_samples)\ntraversal_grid = torch.FloatTensor(n_samples, n_samples, 10)\n\nfor i, xi in enumerate(grid_x):\n    for j, yi in enumerate(grid_y):\n        z_sample = torch.FloatTensor([[xi, yi, 0, 0, 0, 0, 0, 0, 0, 0]])\n        traversal_grid[j, i] = z_sample\n\n\n# these are the range of x and y values we consider\nprint(grid_x)\nprint(grid_y)\n\n[-3.         -2.68421053 -2.36842105 -2.05263158 -1.73684211 -1.42105263\n -1.10526316 -0.78947368 -0.47368421 -0.15789474  0.15789474  0.47368421\n  0.78947368  1.10526316  1.42105263  1.73684211  2.05263158  2.36842105\n  2.68421053  3.        ]\n[-3.         -2.68421053 -2.36842105 -2.05263158 -1.73684211 -1.42105263\n -1.10526316 -0.78947368 -0.47368421 -0.15789474  0.15789474  0.47368421\n  0.78947368  1.10526316  1.42105263  1.73684211  2.05263158  2.36842105\n  2.68421053  3.        ]\n\n\n\n# we vary each dimension while holding the other dimension fixed so we can interpret each dimension\ntraversal_grid[0, 0, :]\n\ntensor([-3., -3.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.])\n\n\n\ntraversal_grid[1, 0, :]\n\ntensor([-3.0000, -2.6842,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n         0.0000,  0.0000])\n\n\n\n# Generate images from the latent traversal grid\nwith torch.no_grad():\n    traversal_grid = traversal_grid.view(-1, 10) # collapsing to 400 x 10\n    generated_images = model.decode(traversal_grid).view(-1, 1, 28, 28) # B x C x H x W\n\n\n# Plot the generated images\nplt.figure(figsize=(10, 10))\nfor i in range(n_samples):\n    for j in range(n_samples):\n        plt.subplot(n_samples, n_samples, i * n_samples + j + 1)\n        plt.imshow(generated_images[i * n_samples + j].squeeze().cpu().numpy(), cmap=\"gray\")\n        plt.axis('off')\n\nplt.show()\n\n# latent traversal plots\n\n\n\n\n\n\n\n\n\n# try other verison\n# Create a grid of latent vectors for traversal\nn_samples = 20\ngrid_x = np.linspace(-3, 3, n_samples)\ngrid_y = np.linspace(-3, 3, n_samples)\ntraversal_grid = torch.FloatTensor(n_samples, n_samples, 10)\n\nfor i, xi in enumerate(grid_x):\n    for j, yi in enumerate(grid_y):\n        z_sample = torch.FloatTensor([[0, 0, 0, xi, yi, 0, 0, 0, 0, 0]])\n        traversal_grid[j, i] = z_sample\n\n# Generate images from the latent traversal grid\nwith torch.no_grad():\n    traversal_grid = traversal_grid.view(-1, 10) # collapsing to 400 x 10\n    generated_images = model.decode(traversal_grid).view(-1, 1, 28, 28) # B x C x H x W\n\n# Plot the generated images\nplt.figure(figsize=(10, 10))\nfor i in range(n_samples):\n    for j in range(n_samples):\n        plt.subplot(n_samples, n_samples, i * n_samples + j + 1)\n        plt.imshow(generated_images[i * n_samples + j].squeeze().cpu().numpy(), cmap='gray')\n        plt.axis('off')\n\nplt.show()",
    "crumbs": [
      "Home",
      "Lecture 5 - VAEs"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lec-9/lecture-9.html",
    "href": "lec-9/lecture-9.html",
    "title": "Lecture 9 - Hypothesis Tests",
    "section": "",
    "text": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy\n\ntorch.manual_seed(42)\n\n&lt;torch._C.Generator at 0x112a1c3f0&gt;",
    "crumbs": [
      "Home",
      "Lecture 9 - Hypothesis Tests"
    ]
  },
  {
    "objectID": "lec-9/lecture-9.html#two-sample-test",
    "href": "lec-9/lecture-9.html#two-sample-test",
    "title": "Lecture 9 - Hypothesis Tests",
    "section": "Two-Sample Test",
    "text": "Two-Sample Test\nWe have two potential titles for a Youtube video. We want to see which one has a higher click-through-rate.\n\nTitle A: 98 views over 1000 impressions\nTitle B: 162 views over 2000 impressions\n\nWe conduct a permutation test to test whether CTR for A (\\theta_A) is equal to CTR for B (\\theta_B).\nH_0: \\theta_A=\\theta_B, \\quad H_1: \\theta_A \\neq \\theta_B.\nOur test statistic is |\\hat{\\theta}_A-\\hat{\\theta}_B|.\n\nmy_viewsA = 98 # number of clicks for video A\nmy_viewsB = 162 # number of clicks for video B\n\nn_impsA = 1000 # number of impressions for video A (total number of people who saw video title)\nn_impsB = 2000 # number of impresssion for video B\n\nDataset A: (1, \\dots, 1, 0, 0, \\dots, 0) (98 ones, 902 zeros)\nDataset B: (1,\\dots, 1, 0, \\dots, 0) (162 ones, 1838 zeros)\n\nmy_viewsA/n_impsA\n\n0.098\n\n\n\nmy_viewsB/n_impsB\n\n0.081\n\n\n\nobs_T = abs(my_viewsA/n_impsA - my_viewsB/n_impsB) # |theta_1 - theta_2|\n\n\nobs_T\n\n0.017\n\n\nWe pool the Dataset A and Dataset B. (We will shuffle Dataset A and Dataset B in the permutation test).\n\nall_views = my_viewsA + my_viewsB\nall_imps = n_impsA + n_impsB\npool = np.array([1]*all_views + [0]*(all_imps - all_views))\n\nHere is an example of a random draw from the null distribution (that the CTR of A and B is the same).\n\n# Sample without replacement for impsA\nimpsA = np.random.choice(pool, n_impsA, replace=False)\n# new permuted dataset of size n_impsA\nviewsA = np.sum(impsA)\nviewsB = all_views - viewsA\n\nPermutation test:\n\ndraw 500 samples from null distribution\nfor each sample s=1,\\dots, 500, calculate T_s = |\\hat{\\theta}_{A, s}-\\hat{\\theta}_{B, s}|\nlocate actual observed T_{obs}=|\\hat{\\theta}_A-\\hat{\\theta}_B| in sample distribution: \\text{p-val} = \\frac{1}{S}\\sum_{s=1}^{500} \\mathbb{1}(T_s\\geq T_{obs} )\n\n\nn_perm = 500\nresampled_Ts = np.zeros(n_perm) # where to store our permutation test statistics\n\nfor cur_sim in range(n_perm):\n    # Pool with 1s for views and 0s for non-views\n    pool = np.array([1]*all_views + [0]*(all_imps - all_views))\n    # create an array with (98+162) ones and (1000+2000 - 98-162) zeros\n    \n    # (1, 1, 1, 1, ..., 1, 0, 0, 0, ..., 0) (1, 1,.., 1, 0, 0, 0, 0, ...,0)\n    # (1, 1, 0, 0, 0, 1, ..., 0, 1, 0, )\n\n    # Sample without replacement for impsA\n    impsA = np.random.choice(pool, n_impsA, replace=False)\n    # new permuted dataset of size n_impsA\n    viewsA = np.sum(impsA)\n    viewsB = all_views - viewsA\n    \n    resampled_Ts[cur_sim] = abs(viewsA/n_impsA - viewsB/n_impsB) # | theta_1 -theta_2| for this dataset\n\np_value = sum([1 for t in resampled_Ts if t &gt;= obs_T]) / n_perm\n\nprint(f\"P-Value: {p_value:.4f}\")\n\nplt.hist(resampled_Ts, bins=20)\nplt.axvline(obs_T, color='red')\nplt.show()\n\nP-Value: 0.1140\n\n\n\n\n\n\n\n\n\nConclusion: we retain the null hypothesis.",
    "crumbs": [
      "Home",
      "Lecture 9 - Hypothesis Tests"
    ]
  },
  {
    "objectID": "lec-9/lecture-9.html#independence-test-with-neural-networks",
    "href": "lec-9/lecture-9.html#independence-test-with-neural-networks",
    "title": "Lecture 9 - Hypothesis Tests",
    "section": "Independence test with neural networks",
    "text": "Independence test with neural networks\nData: (X_1,Y_1), (X_2, Y_2),\\dots, (X_n, Y_n)\nWe consider two different methods to conduct an independence test:\nH_0: X \\text{ is independent of } Y \\text{ vs. } H_1: X, Y \\text{ not independent}\n\nMethod 1\n\nGenerate 500 random permutations (X_1, Y_{\\pi(1)}),\\dots, (X_n, Y_{\\pi(n)})\nFor each permutation:\n\nSplit into train/test\nTrain neural network on train data\nCalculate error on test data (T_s)\n\nFor actual data:\n\nSplit into train/test\nTrain neural network on train data\nCalculate error on test data (T_{obs})\n\nCalculate p-value: \\text{p-val} = \\frac{1}{S}\\sum_{s=1}^{500} \\mathbb{1}(T_s\\geq T_{obs} )\n\n\nclass NNet(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super(NNet, self).__init__()\n        self.sequential = nn.Sequential(\n            nn.Linear(in_dim, 2*in_dim),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(2*in_dim, out_dim))\n\n    def forward(self, x):\n        return self.sequential(x)\n\nWe create simulated data which we know is NOT independent.\nIdeally, we should reject the null hypothesis that Y and X are independent.\n\np1 = 5 # dimension of X\np2 = 3 # dimension of Y\n\nsignal_strength = 0.3\n\nn_samples = 10000\n\nX = np.random.randn(n_samples, p1)\n\nB = np.random.randn(p2, p1)\nB = B / np.linalg.norm(B, ord='fro') # some parameter\n\nY = signal_strength * X @ B.T + np.random.randn(n_samples, p2)\n# Y = B X + random noise\n\nX = torch.tensor(X, dtype=torch.float32)\nY = torch.tensor(Y, dtype=torch.float32)\n\n\ndef testerr(x, y, epochs=200):\n\n    # trains a neural network to predict y from x\n    ntrain = int(n_samples * 0.7) # train on 70% of the data\n    ntest = n_samples - ntrain\n\n    x_train = x[:ntrain]\n    x_test = x[ntrain:]\n\n    y_train = y[:ntrain]\n    y_test = y[ntrain:]\n\n    net = NNet(p1, p2)\n    net.train()\n    optimizer = optim.Adam(net.parameters(), lr=0.2)\n    for epoch in range(epochs):\n        optimizer.zero_grad()\n        outputs = net(x_train)\n        \n        loss = nn.MSELoss(reduction='mean')(outputs, y_train)\n        loss.backward()\n        optimizer.step()\n    \n    net.eval()\n    outputs = net(x_test)\n    loss = nn.MSELoss(reduction='mean')(outputs, y_test)\n\n    return loss.item()\n\n# our observed test statistic is the loss in predicting y from x\nobs_T = testerr(X, Y)\n\n\nobs_T\n\n1.0054599046707153\n\n\n\n# Perform permutation testing\nn_permutations = 500\nresampled_Ts = []\n\nfor _ in range(n_permutations):\n    permuted_Y = Y[torch.randperm(n_samples)]\n    resampled_T = testerr(X, permuted_Y) # train a neural network, output training loss\n    resampled_Ts.append(resampled_T)\n\n\n# Calculate p-value\np_value = sum([1 for t in resampled_Ts if t &lt;= obs_T]) / n_permutations\n\n# plot distribution of permuted_test_statistics\nplt.hist(resampled_Ts, bins=20)\nplt.axvline(obs_T, color='red')\nplt.show()\n\n\nprint(f\"Observed test statistic: {obs_T:.4f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n\n\n\n\n\n\n\nObserved test statistic: 1.0055\nP-Value: 0.0080\n\n\n\n\nMethod 2\n\nSplit into train and test data\nTrain f to predict Y from X\nTrain baseline f_{base} to predict Y (this is \\bar{Y})\nFor each datapoint, compute error: z_i = Loss(f(x_i), y_i),\\quad \\widetilde{z}_i=Loss(f_{base}, y_i)\nFor s=1,\\dots, 500:\n\nSwitch z_i and \\widetilde{z}_i with probability 0.5\nCalculate mean(z_{i,s}) - mean(\\widetilde{z}_{i,s})\n\nFor observed data:\n\nCalculate mean(z_i) - mean(\\widetilde{z}_i)\n\nCalculate p-value\n\n\n## sample split\nntrain = int(n_samples * 0.7) # train on 70% of the data\nntest = n_samples - ntrain\n\nX_train = X[:ntrain]\nX_test = X[ntrain:]\n\nY_train = Y[:ntrain]\nY_test = Y[ntrain:]\n\nnet = NNet(p1, p2)\n\n\nnet.train()\nepochs = 200\noptimizer = optim.Adam(net.parameters(), lr=.2)\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    outputs = net(X_train)\n        \n    loss = nn.MSELoss(reduction='mean')(outputs, Y_train)\n    loss.backward()\n    optimizer.step()\n\nnet.eval()\ntesterrs = torch.sum((Y_test - net(X_test))**2, dim=1)\nmean_testerr = torch.mean(testerrs)\n\n\ntesterrs\n\ntensor([0.6410, 0.7331, 6.1261,  ..., 4.2328, 1.2628, 4.6177],\n       grad_fn=&lt;SumBackward1&gt;)\n\n\n\nnullerrs = torch.sum((Y_test - torch.mean(Y_train, dim=0))**2, dim=1)\nmean_nullerr = torch.mean( nullerrs )\n\n\nnullerrs\n\ntensor([0.7893, 0.7796, 5.9333,  ..., 4.5801, 1.6773, 4.4518])\n\n\n\nprint(f\"Mean Test Errors: {mean_testerr:.4f}\")\nprint(f\"Mean Null Test Errors: {mean_nullerr:.4f}\")\n\nobs_T = mean_testerr - mean_nullerr\n\nMean Test Errors: 3.0165\nMean Null Test Errors: 3.0656\n\n\n\ncombined = torch.stack((testerrs, nullerrs), dim=1)\n\n\nheads = torch.randint(0, 2, (ntest,))\n\nresampled_testerrs = combined[torch.arange(ntest), heads]\nresampled_nullerrs = combined[torch.arange(ntest), 1-heads]\n\n\nn_permutations = 500\nresampled_Ts = []\n\nfor _ in range(n_permutations):\n    heads = torch.randint(0, 2, (ntest,))\n\n    resampled_testerrs = combined[torch.arange(ntest), heads]\n    resampled_nullerrs = combined[torch.arange(ntest), 1-heads]\n    # test error using X - test error not using X\n    resampled_T = torch.mean(resampled_testerrs) - torch.mean(resampled_nullerrs)\n    resampled_Ts.append(resampled_T)\n\n## Calculate p-value\np_value = sum([1 for t in resampled_Ts if t &lt;= obs_T]) / n_permutations\n\nresampled_Ts = [t.detach().numpy() for t in resampled_Ts]\nplt.hist(resampled_Ts, bins=20)\nplt.axvline(obs_T.detach().numpy(), color='red')\nplt.show()\n\nprint(f\"P-Value: {p_value:.4f}\")\n\n\n\n\n\n\n\n\nP-Value: 0.0000",
    "crumbs": [
      "Home",
      "Lecture 9 - Hypothesis Tests"
    ]
  },
  {
    "objectID": "lec-7/lecture-7.html",
    "href": "lec-7/lecture-7.html",
    "title": "Lecture 7 - Quantile Regression",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math\nfrom scipy.stats import t\nfrom sklearn.linear_model import LinearRegression\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nrng = np.random.default_rng()",
    "crumbs": [
      "Home",
      "Lecture 7 - Quantile Regression"
    ]
  },
  {
    "objectID": "lec-7/lecture-7.html#define-useful-functions",
    "href": "lec-7/lecture-7.html#define-useful-functions",
    "title": "Lecture 7 - Quantile Regression",
    "section": "Define useful functions",
    "text": "Define useful functions\n\ndef generateX(n):\n    x = np.random.randn(n)\n    return(x)\n\ndef generateNonlinearY(x, sigma):\n    n = len(x)\n    eps1 = rng.gamma(2, 2, n) - 4\n    # eps1 = rng.standard_t(df=3, size=n)\n    eps2 = rng.normal(0, .5, n)\n    \n    y = 2*np.maximum(x, 0) + sigma * ( (x + .5) * eps1 + eps2)\n    return y\n\ndef splitData(x, y, n1):\n    x1 = x[:n1]\n    y1 = y[:n1]\n    x2 = x[n1:]\n    y2 = y[n1:]\n    return x1, y1, x2, y2",
    "crumbs": [
      "Home",
      "Lecture 7 - Quantile Regression"
    ]
  },
  {
    "objectID": "lec-7/lecture-7.html#define-cqr-interval-function-and-residual-interval-function",
    "href": "lec-7/lecture-7.html#define-cqr-interval-function-and-residual-interval-function",
    "title": "Lecture 7 - Quantile Regression",
    "section": "Define CQR interval function and residual interval function",
    "text": "Define CQR interval function and residual interval function\n\n# scores on holdout data, Q_alpha/2(x), Q_{1-alpha/2}(x)\ndef CQRInterval(scores, y_out_l_pred, y_out_u_pred, alpha):\n    n = len(scores)\n    t = np.quantile(scores, np.ceil((1-alpha)*(n+1))/n )\n    y_out_l = y_out_l_pred - t\n    y_out_u = y_out_u_pred + t \n    return np.hstack([y_out_l.reshape(-1, 1), y_out_u.reshape(-1, 1)])\n\n# score = |Y-mu(x)|/sd(x)\ndef conformalResidueInterval(scores, mu, sigma, alpha):\n    n = len(scores)\n    t = np.quantile(scores, np.ceil((1-alpha)*(n+1))/n )\n    y_out_l = mu - t*sigma\n    y_out_u = mu + t*sigma\n    return np.hstack([y_out_l.reshape(-1, 1), y_out_u.reshape(-1, 1)])",
    "crumbs": [
      "Home",
      "Lecture 7 - Quantile Regression"
    ]
  },
  {
    "objectID": "lec-7/lecture-7.html#define-neural-network-for-quantile-regression-and-heteroskedastic-regression",
    "href": "lec-7/lecture-7.html#define-neural-network-for-quantile-regression-and-heteroskedastic-regression",
    "title": "Lecture 7 - Quantile Regression",
    "section": "Define neural network for quantile regression and heteroskedastic regression",
    "text": "Define neural network for quantile regression and heteroskedastic regression\n\nclass NNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(NNet, self).__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.layer2a = nn.Linear(hidden_dim, 1)\n        self.layer2b = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = torch.relu(x)\n        out1 = self.layer2a(x).squeeze()\n        out2 = self.layer2b(x).squeeze()\n        return out1, out2 # this outputs estimates of Q_alpha and Q_(1-alpha/2)\n    \n    \ndef quantileLoss(y_out_l, y_out_u, y, alpha):\n    # this is the row alpha function for both alpha/2 and (1-alpha/2)\n\n    lower_loss = torch.max( (alpha/2)*(y - y_out_l), (alpha/2-1)*(y - y_out_l) )\n    upper_loss = torch.max( (1-alpha/2)*(y - y_out_u), (-alpha/2)*(y - y_out_u) )\n    return torch.mean(lower_loss + upper_loss)\n\ndef heteroskedLoss(mu, log_nu, y):\n    return torch.mean( (y - mu)**2 / (2*torch.exp(log_nu)**2) + log_nu )",
    "crumbs": [
      "Home",
      "Lecture 7 - Quantile Regression"
    ]
  },
  {
    "objectID": "lec-7/lecture-7.html#experiment-parameters",
    "href": "lec-7/lecture-7.html#experiment-parameters",
    "title": "Lecture 7 - Quantile Regression",
    "section": "Experiment parameters",
    "text": "Experiment parameters\n\nn = 2000\nntrain = 500\n\nsigma = .2\nalpha = 0.05 # 95% prediction interval for Y_new",
    "crumbs": [
      "Home",
      "Lecture 7 - Quantile Regression"
    ]
  },
  {
    "objectID": "lec-6/lecture-6.html",
    "href": "lec-6/lecture-6.html",
    "title": "Lecture 6 - Conformal Inference",
    "section": "",
    "text": "import numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport math\nfrom scipy.stats import t\nfrom sklearn.linear_model import LinearRegression\nimport torch\nimport torch.nn as nn\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader",
    "crumbs": [
      "Home",
      "Lecture 6 - Conformal Inference"
    ]
  },
  {
    "objectID": "lec-6/lecture-6.html#experiment-4",
    "href": "lec-6/lecture-6.html#experiment-4",
    "title": "Lecture 6 - Conformal Inference",
    "section": "Experiment 4",
    "text": "Experiment 4\n\ntrue model is nonlinear with heteroskedastic Gaussian noise\nwe fit a nonlinear model with heteroskedastic noise\nwe use a conformal prediction interval with score = studentized residual\n\n\nx = generateX(n) \ny = generateNonlinearY(x, sigma)\n\nxtrain, ytrain, xtest, ytest = splitData(x, y, ntrain)\n\nn1 = int(ntrain/2)\nxtrain1, ytrain1, xtrain2, ytrain2 = splitData(xtrain, ytrain, n1)\n\n\n## train neural network on first half of training data\n\nclass NNet(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(NNet, self).__init__()\n        self.layer1 = nn.Linear(input_dim, hidden_dim)\n        self.mean_layer = nn.Linear(hidden_dim, 1)\n        self.logvar_layer = nn.Linear(hidden_dim, 1)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = torch.relu(x)\n        mean = self.mean_layer(x)\n        logvar = self.logvar_layer(x)\n\n        return mean.squeeze(), logvar.squeeze()\n    \n    def loss(self, x, mean, logvar):\n        var = torch.exp(logvar)\n\n        loss = torch.pow(x-mean, 2) / var + logvar\n        out = loss.mean()\n        return out\n    \nx_train1 = torch.tensor(xtrain1.reshape(-1, 1), dtype=torch.float32)\ny_train1 = torch.tensor(ytrain1, dtype=torch.float32)\n\nmodel = NNet(1, 20)\nlr = 0.01\nepochs = 200\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\nfor epoch in range(epochs):\n    optimizer.zero_grad()\n    mean, logvar = model(x_train1)\n    loss = model.loss(y_train1, mean, logvar)\n    loss.backward()\n    optimizer.step()\n\n    epoch_loss = loss.item()\n\n    if epoch % 20 == 0:\n        print('epoch', epoch, 'loss', f\"{epoch_loss:.3}\")\n\n\nepoch 0 loss 3.66\nepoch 20 loss 0.401\nepoch 40 loss -0.218\nepoch 60 loss -1.25\nepoch 80 loss -2.47\nepoch 100 loss -2.6\nepoch 120 loss -2.69\nepoch 140 loss -2.75\nepoch 160 loss -2.77\nepoch 180 loss -2.78\n\n\n\ndef conformalResidueInterval(scores, mu, sigma, alpha):\n    n = len(scores)\n    t = np.quantile(scores, np.ceil((1-alpha)*(n+1))/n )\n    y_out_l = mu - t*sigma\n    y_out_u = mu + t*sigma\n    return np.hstack([y_out_l.reshape(-1, 1), y_out_u.reshape(-1, 1)])\n\n\n## compute residues on second half of training data\n## and prediction intervals on test data\nx_train2 = torch.tensor(xtrain2.reshape(-1, 1), dtype=torch.float32)\n\nmean, logvar = model(x_train2)\nmean = mean.squeeze().detach().numpy()\nlogvar = logvar.squeeze().detach().numpy()\n\n# compute scores \nresids = np.abs(ytrain2 - mean) / np.exp(0.5 * logvar)\n\n# get mean and variance for intervals\nx_test = torch.tensor(xtest.reshape(-1, 1), dtype=torch.float32)\nmean, logvar = model(x_test)\nmean = mean.squeeze().detach().numpy()\nlogvar = logvar.squeeze().detach().numpy()\nsd = np.exp(0.5 * logvar)\n\nintervals = conformalResidueInterval(resids, mean, sd, alpha)\n\n\n# note that this is larger than 1.96, which it would be if Gaussian assumption held\ns_hat = np.quantile(resids, np.ceil((1-alpha)*(n+1))/n )\ns_hat\n\nnp.float64(1.9150904429715403)\n\n\n\ncov_test = [1 if ytest[i] &gt;= intervals[i, 0] and ytest[i] &lt;= intervals[i, 1] else 0 for i in range(len(ytest))]\nprint('Percent covered on test data: ', np.mean(cov_test))\n\nPercent covered on test data:  0.955\n\n\n\n## plot f\nsorted_ix = np.argsort(xtest)\nxgrid = xtest[sorted_ix]\nygrid = ytest[sorted_ix]\n\ninterval_grid = intervals[sorted_ix, :]\n\nxlinspace = np.linspace(xgrid.min(), xgrid.max(), 1000)\n\nxlinspace_torch = torch.tensor(xlinspace, dtype=torch.float32).reshape(-1,1)\nmean_linspace, logvar_linspace = model(xlinspace_torch)\nmean_linspace = mean_linspace.squeeze().detach().numpy()\nsd_linspace = np.exp(0.5 * logvar_linspace.squeeze().detach().numpy())\n\n\nplt.figure(figsize=(4, 3))\nplt.ylim(ygrid.min()-0.5, ygrid.max()+0.5)\n\nplt.scatter(xgrid, ygrid, color='blue', label='Data Points', s=1)\nplt.plot(xlinspace, mean_linspace, 'r-')\nplt.plot(xlinspace, mean_linspace + 1.96 * sd_linspace, 'g-')\nplt.plot(xlinspace, mean_linspace - 1.96 * sd_linspace, 'g-')\n\nplt.fill_between(xgrid, interval_grid[:, 0], interval_grid[:, 1], color='gray', alpha=0.5, label='Prediction Interval')\n\n\n\n\n\n\n\n\n\n## plot f\nsorted_ix = np.argsort(xtest)\nxgrid = xtest[sorted_ix]\nygrid = ytest[sorted_ix]\n\ninterval_grid = intervals[sorted_ix, :]\n\nxlinspace = np.linspace(xgrid.min(), xgrid.max(), 1000)\n\nxlinspace_torch = torch.tensor(xlinspace, dtype=torch.float32).reshape(-1,1)\nmean_linspace, logvar_linspace = model(xlinspace_torch)\nmean_linspace = mean_linspace.squeeze().detach().numpy()\nsd_linspace = np.exp(0.5 * logvar_linspace.squeeze().detach().numpy())\n\n\nplt.figure(figsize=(4, 3))\nplt.ylim(ygrid.min()-0.5, ygrid.max()+0.5)\n\nplt.scatter(xgrid, ygrid, color='blue', label='Data Points', s=1)\nplt.plot(xlinspace, mean_linspace, 'r-')\nplt.plot(xlinspace, mean_linspace + 1.96 * sd_linspace, 'g-')\nplt.plot(xlinspace, mean_linspace - 1.96 * sd_linspace, 'g-')\n\nplt.fill_between(xgrid, interval_grid[:, 0], interval_grid[:, 1], color='gray', alpha=0.5, label='Prediction Interval')\n\nplt.title('Conformal prediction with studentized residuals ' r'$(\\widehat{{s}} = {:.2f})$'.format(s_hat))\n\nText(0.5, 1.0, 'Conformal prediction with studentized residuals $(\\\\widehat{s} = 1.92)$')\n\n\n\n\n\n\n\n\n\n\n## plot f\nsorted_ix = np.argsort(xtest)\nxgrid = xtest[sorted_ix]\nygrid = ytest[sorted_ix]\n\ninterval_grid = intervals[sorted_ix, :]\n\nxlinspace = np.linspace(xgrid.min(), xgrid.max(), 1000)\nxlinspace_torch = torch.tensor(xlinspace, dtype=torch.float32).reshape(-1,1)\nmean_linspace, logvar_linspace = model(xlinspace_torch)\nmean_linspace = mean_linspace.squeeze().detach().numpy()\nsd_linspace = np.exp(0.5 * logvar_linspace.squeeze().detach().numpy())\n\nplt.figure(figsize=(4, 3))\nplt.ylim(ygrid.min()-0.5, ygrid.max()+0.5)\nplt.scatter(xgrid, ygrid, color='blue', label='Data Points', s=1)\nplt.plot(xlinspace, mean_linspace, 'r-')\nplt.plot(xlinspace, mean_linspace + 1.96 * sd_linspace, 'g-')\nplt.plot(xlinspace, mean_linspace - 1.96 * sd_linspace, 'g-')",
    "crumbs": [
      "Home",
      "Lecture 6 - Conformal Inference"
    ]
  },
  {
    "objectID": "lec-8/lecture-8-classification.html",
    "href": "lec-8/lecture-8-classification.html",
    "title": "Lecture 8 - Conformalized Classification",
    "section": "",
    "text": "Much of this notebook is from https://github.com/aangelopoulos/conformal-prediction/blob/main/notebooks/imagenet-aps.ipynb\n\nimport os\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n!pip install -U --no-cache-dir gdown --pre\n\nRequirement already satisfied: gdown in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (5.2.0)\nRequirement already satisfied: beautifulsoup4 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from gdown) (4.13.5)\nRequirement already satisfied: filelock in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from gdown) (3.19.1)\nRequirement already satisfied: requests[socks] in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from gdown) (2.32.5)\nRequirement already satisfied: tqdm in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from gdown) (4.67.1)\nRequirement already satisfied: soupsieve&gt;1.2 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from beautifulsoup4-&gt;gdown) (2.5)\nRequirement already satisfied: typing-extensions&gt;=4.0.0 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from beautifulsoup4-&gt;gdown) (4.15.0)\nRequirement already satisfied: charset_normalizer&lt;4,&gt;=2 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (3.3.2)\nRequirement already satisfied: idna&lt;4,&gt;=2.5 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (3.7)\nRequirement already satisfied: urllib3&lt;3,&gt;=1.21.1 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (2.5.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (2025.8.3)\nRequirement already satisfied: PySocks!=1.5.7,&gt;=1.5.6 in /Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages (from requests[socks]-&gt;gdown) (1.7.1)\n\n\n\n# Load cached data\nif not os.path.exists(\"../data/imagenet\"):\n    os.system(\"gdown 1h7S6N_Rx7gdfO3ZunzErZy6H7620EbZK -O ../data.tar.gz\")\n    os.system(\"tar -xf ../data.tar.gz -C ../\")\n    os.system(\"rm ../data.tar.gz\")\nif not os.path.exists(\"../data/imagenet/human_readable_labels.json\"):\n    from urllib.request import urlretrieve\n    urlretrieve(\"https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json\",\"../data/imagenet/human_readable_labels.json\")\n\ndata = np.load(\"../data/imagenet/imagenet-resnet152.npz\")\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nsmx = data[\"smx\"]\nlabels = data[\"labels\"].astype(int)\n\n\nsmx.shape\n\n(50000, 1000)\n\n\n\n# Problem setup\nn = 1000  # number of calibration points\nalpha = 0.1  # 1-alpha is the desired coverage (0.9 coverage)\n\n\n# Split the softmax scores into calibration and validation sets (save the shuffling)\nidx = np.array([1] * n + [0] * (smx.shape[0] - n)) &gt; 0\nnp.random.shuffle(idx)\ncal_smx, val_smx = smx[idx, :], smx[~idx, :]\ncal_labels, val_labels = labels[idx], labels[~idx]\n\n\n# get indices of sorted softmax (ascending)\ncal_pi = cal_smx.argsort(axis=1)\n\n# reverse, so descending\ncal_pi = cal_pi[:, ::-1] \n\n\n# order the softmax scores \ncal_srt = np.take_along_axis(cal_smx, cal_pi, axis=1)\n\n# take cumulative sum\ncal_srt = cal_srt.cumsum(axis=1)\n\n\ncal_srt.shape\n\n(1000, 1000)\n\n\n\n# applying argsort to cal_pi gets the rank - we reorder cal_srt to original space\n\ncal_scores = np.take_along_axis(cal_srt, cal_pi.argsort(axis=1), axis=1)[\n    range(n), cal_labels\n]\n\n\n# Get the score quantile\nqhat = np.quantile(\n    cal_scores, np.ceil((n + 1) * (1 - alpha)) / n, interpolation=\"higher\"\n)\n# Deploy (output=list of length n, each element is tensor of classes)\nval_pi = val_smx.argsort(1)[:, ::-1]\nval_srt = np.take_along_axis(val_smx, val_pi, axis=1).cumsum(axis=1)\nprediction_sets = np.take_along_axis(val_srt &lt;= qhat, val_pi.argsort(axis=1), axis=1)\n\n\n# Calculate empirical coverage\nempirical_coverage = prediction_sets[\n    np.arange(prediction_sets.shape[0]), val_labels\n].mean()\nprint(f\"The empirical coverage is: {empirical_coverage}\")\n\nThe empirical coverage is: 0.8913877551020408\n\n\n\n# Show some examples\nwith open(\"../data/imagenet/human_readable_labels.json\") as f:\n    label_strings = np.array(json.load(f))\n\nexample_paths = os.listdir(\"../data/imagenet/examples\")\nfor i in range(10):\n    rand_path = np.random.choice(example_paths)\n    img = imread(\"../data/imagenet/examples/\" + rand_path)\n    img_index = int(rand_path.split(\".\")[0])\n    img_pi = smx[img_index].argsort()[::-1]\n    img_srt = np.take_along_axis(smx[img_index], img_pi, axis=0).cumsum()\n    prediction_set = np.take_along_axis(img_srt &lt;= qhat, img_pi.argsort(), axis=0)\n    plt.figure()\n    plt.imshow(img)\n    plt.axis(\"off\")\n    plt.show()\n    print(f\"The prediction set is: {list(label_strings[prediction_set])}\")\n\n\n\n\n\n\n\n\nThe prediction set is: ['great white shark', 'hen', 'bulbul', 'jay', 'chickadee', 'American dipper', 'leatherback sea turtle', 'mud turtle', 'Nile crocodile', 'American alligator', 'triceratops', 'eastern hog-nosed snake', 'Saharan horned viper', 'southern black widow', 'tick', 'grey parrot', 'wallaby', 'wombat', 'Dungeness crab', 'American lobster', 'spiny lobster', 'crayfish', 'common gallinule', 'Chihuahua', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Redbone Coonhound', 'Irish Wolfhound', 'Italian Greyhound', 'Whippet', 'Ibizan Hound', 'Norwegian Elkhound', 'Saluki', 'Scottish Deerhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Border Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Flat-Coated Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Kuvasz', 'Schipperke', 'Groenendael', 'Malinois', 'Briard', 'Australian Kelpie', 'Shetland Sheepdog', 'collie', 'Border Collie', 'Bouvier des Flandres', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Appenzeller Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'St. Bernard', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Basenji', 'pug', 'Leonberger', 'Chow Chow', 'Keeshond', 'Griffon Bruxellois', 'Pembroke Welsh Corgi', 'Cardigan Welsh Corgi', 'Toy Poodle', 'Standard Poodle', 'Mexican hairless dog', 'grey wolf', 'Alaskan tundra wolf', 'red wolf', 'coyote', 'dingo', 'dhole', 'African wild dog', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'Siamese cat', 'Egyptian Mau', 'cougar', 'lynx', 'ground beetle', 'longhorn beetle', 'rhinoceros beetle', 'weevil', 'grasshopper', 'cricket', 'cockroach', 'starfish', 'hare', 'weasel', 'black-footed ferret', 'badger', 'armadillo', 'baboon', 'snoek', 'coho salmon', 'garfish', 'accordion', 'apron', 'backpack', 'ballpoint pen', 'baseball', 'bath towel', 'bib', 'bolo tie', 'poke bonnet', 'bow tie', 'bra', 'bucket', 'buckle', 'bulletproof vest', 'cauldron', 'canoe', 'cardigan', 'tool kit', 'carton', 'automated teller machine', 'catamaran', 'mobile phone', 'chain', 'chain-link fence', 'chainsaw', 'Christmas stocking', 'cleaver', 'clogs', 'coffee mug', 'coffeemaker', 'coil', 'combination lock', 'computer keyboard', 'cowboy hat', 'digital watch', 'dishcloth', 'dishwasher', 'dog sled', 'doormat', 'drum', 'drumstick', 'fountain pen', 'fur coat', 'grand piano', 'hammer', 'hair dryer', 'hand-held computer', 'handkerchief', 'harvester', 'hatchet', 'holster', 'horse-drawn vehicle', 'iPod', 'clothes iron', 'jeans', 'jeep', 'T-shirt', 'jigsaw puzzle', 'joystick', 'kimono', 'knee pad', 'knot', 'ladle', 'lawn mower', 'lens cap', 'lighter', 'slip-on shoe', 'magnetic compass', 'mail bag', 'tights', 'mask', 'match', 'microphone', 'military uniform', 'mitten', 'mortar', 'mousetrap', 'muzzle', 'nail', 'oscilloscope', 'packet', 'pan flute', 'paper towel', 'pencil sharpener', 'Pickelhaube', 'ping-pong ball', 'plastic bag', 'plunger', 'Polaroid camera', 'pole', 'police van', 'power drill', 'purse', 'quilt', 'racket', 'radiator', 'reel', 'reflex camera', 'remote control', 'revolver', 'eraser', 'running shoe', 'sandal', 'scabbard', 'schooner', 'screw', 'screwdriver', 'seat belt', 'shield', 'shoe store', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'ski mask', 'sleeping bag', 'sliding door', 'soccer ball', 'solar thermal collector', 'sombrero', 'soup bowl', 'space heater', 'spatula', 'spindle', 'spotlight', 'scarf', 'stopwatch', 'stove', 'strainer', 'suit', 'sunglass', 'sunglasses', 'mop', 'sweatshirt', 'syringe', 'television', 'tennis ball', 'trench coat', 'tripod', 'tub', 'vacuum cleaner', 'sink', 'water bottle', 'whistle', 'window screen', 'Windsor tie', 'wok', 'wool', 'split-rail fence', 'crossword', 'dust jacket', 'plate', 'guacamole', 'ice pop', 'pretzel', 'broccoli', 'zucchini', 'orange', 'lemon', 'pineapple', 'pizza', 'corn', 'acorn', 'rose hip', 'ear', 'toilet paper']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['tennis ball']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'stingray', 'cock', 'hen', 'ostrich', 'brambling', 'goldfinch', 'house finch', 'junco', 'indigo bunting', 'American robin', 'bulbul', 'jay', 'magpie', 'chickadee', 'American dipper', 'kite', 'bald eagle', 'vulture', 'great grey owl', 'axolotl', 'American bullfrog', 'tree frog', 'tailed frog', 'loggerhead sea turtle', 'leatherback sea turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'green iguana', 'Carolina anole', 'desert grassland whiptail lizard', 'agama', 'frilled-necked lizard', 'alligator lizard', 'European green lizard', 'chameleon', 'Komodo dragon', 'American alligator', 'triceratops', 'worm snake', 'eastern hog-nosed snake', 'smooth green snake', 'garter snake', 'water snake', 'vine snake', 'night snake', 'boa constrictor', 'African rock python', 'Indian cobra', 'green mamba', 'Saharan horned viper', 'eastern diamondback rattlesnake', 'harvestman', 'yellow garden spider', 'barn spider', 'European garden spider', 'southern black widow', 'tarantula', 'wolf spider', 'tick', 'centipede', 'ruffed grouse', 'prairie grouse', 'peacock', 'quail', 'partridge', 'grey parrot', 'macaw', 'sulphur-crested cockatoo', 'lorikeet', 'coucal', 'bee eater', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'duck', 'goose', 'tusker', 'wallaby', 'koala', 'wombat', 'jellyfish', 'sea anemone', 'nematode', 'conch', 'snail', 'slug', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'crayfish', 'white stork', 'black stork', 'spoonbill', 'flamingo', 'little blue heron', 'great egret', 'bittern', 'crane (bird)', 'limpkin', 'common gallinule', 'American coot', 'bustard', 'dunlin', 'dowitcher', 'pelican', 'king penguin', 'albatross', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Shih Tzu', 'King Charles Spaniel', 'Papillon', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Redbone Coonhound', 'Irish Wolfhound', 'Italian Greyhound', 'Whippet', 'Ibizan Hound', 'Norwegian Elkhound', 'Otterhound', 'Saluki', 'Scottish Deerhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Border Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Lhasa Apso', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'English Setter', 'Irish Setter', 'Gordon Setter', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniels', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael', 'Malinois', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Shetland Sheepdog', 'collie', 'Bouvier des Flandres', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'St. Bernard', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Newfoundland', 'Pyrenean Mountain Dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Keeshond', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog', 'red wolf', 'dingo', 'African wild dog', 'red fox', 'Arctic fox', 'grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'tiger', 'brown bear', 'American black bear', 'polar bear', 'sloth bear', 'mongoose', 'meerkat', 'ladybug', 'ground beetle', 'longhorn beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'fly', 'bee', 'ant', 'grasshopper', 'cricket', 'stick insect', 'mantis', 'cicada', 'leafhopper', 'lacewing', 'dragonfly', 'damselfly', 'red admiral', 'ringlet', 'monarch butterfly', 'small white', 'sulphur butterfly', 'gossamer-winged butterfly', 'starfish', 'cottontail rabbit', 'hare', 'Angora rabbit', 'hamster', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'common sorrel', 'zebra', 'pig', 'wild boar', 'hippopotamus', 'ox', 'water buffalo', 'Alpine ibex', 'impala', 'gazelle', 'dromedary', 'llama', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas monkey', 'baboon', 'macaque', 'langur', 'black-and-white colobus', 'proboscis monkey', 'marmoset', 'white-headed capuchin', 'howler monkey', 'titi', \"Geoffroy's spider monkey\", 'common squirrel monkey', 'ring-tailed lemur', 'indri', 'Asian elephant', 'African bush elephant', 'red panda', 'giant panda', 'clownfish', 'abacus', 'abaya', 'accordion', 'acoustic guitar', 'aircraft carrier', 'airliner', 'airship', 'ambulance', 'amphibious vehicle', 'analog clock', 'apiary', 'waste container', 'assault rifle', 'backpack', 'bakery', 'balloon', 'Band-Aid', 'banjo', 'baluster', 'barbell', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'basketball', 'bassinet', 'bassoon', 'swimming cap', 'bath towel', 'bathtub', 'station wagon', 'lighthouse', 'military cap', 'bell-cot', 'bib', 'bikini', 'binoculars', 'birdhouse', 'boathouse', 'poke bonnet', 'bookcase', 'bookstore', 'bow tie', 'brass', 'bra', 'breakwater', 'breastplate', 'broom', 'bucket', 'bulletproof vest', 'high-speed train', 'taxicab', 'cauldron', 'candle', 'cannon', 'canoe', 'can opener', 'car mirror', 'carousel', 'carton', 'car wheel', 'automated teller machine', 'cassette player', 'castle', 'catamaran', 'CD player', 'cello', 'mobile phone', 'chain', 'chain-link fence', 'chainsaw', 'chest', 'chiffonier', 'chime', 'Christmas stocking', 'church', 'movie theater', 'cleaver', 'cloak', 'clogs', 'cocktail shaker', 'coffee mug', 'coffeemaker', 'coil', 'computer keyboard', 'confectionery store', 'convertible', 'cowboy hat', 'cradle', 'crane (machine)', 'crash helmet', 'crate', 'infant bed', 'Crock Pot', 'croquet ball', 'crutch', 'dam', 'desk', 'desktop computer', 'rotary dial telephone', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishcloth', 'dishwasher', 'disc brake', 'dock', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric locomotive', 'entertainment center', 'envelope', 'filing cabinet', 'fireboat', 'fire engine', 'fire screen sheet', 'flagpole', 'flute', 'folding chair', 'football helmet', 'forklift', 'fountain', 'four-poster bed', 'French horn', 'frying pan', 'garbage truck', 'gas mask', 'gas pump', 'goblet', 'golf ball', 'golf cart', 'gondola', 'gong', 'gown', 'grand piano', 'greenhouse', 'grille', 'grocery store', 'guillotine', 'barrette', 'half-track', 'hamper', 'hand-held computer', 'handkerchief', 'harmonica', 'harvester', 'home theater', 'honeycomb', 'hook', 'hoop skirt', 'horse-drawn vehicle', \"jack-o'-lantern\", 'jeans', 'jeep', 'T-shirt', 'pulled rickshaw', 'joystick', 'kimono', 'knee pad', 'knot', 'lab coat', 'ladle', 'lampshade', 'lawn mower', 'library', 'lifeboat', 'lighter', 'limousine', 'ocean liner', 'lipstick', 'lotion', 'speaker', 'loupe', 'sawmill', 'magnetic compass', 'mail bag', 'mailbox', 'tights', 'tank suit', 'maraca', 'marimba', 'mask', 'maypole', 'maze', 'measuring cup', 'microphone', 'military uniform', 'milk can', 'minibus', 'miniskirt', 'minivan', 'missile', 'mitten', 'mixing bowl', 'mobile home', 'Model T', 'monastery', 'monitor', 'moped', 'mortar', 'square academic cap', 'mosque', 'mosquito net', 'scooter', 'mountain bike', 'computer mouse', 'mousetrap', 'moving van', 'neck brace', 'necklace', 'nipple', 'obelisk', 'oboe', 'organ', 'oscilloscope', 'bullock cart', 'oxygen mask', 'packet', 'paddle', 'pajamas', 'palace', 'pan flute', 'paper towel', 'parachute', 'park bench', 'parking meter', 'patio', 'payphone', 'pedestal', 'perfume', 'Petri dish', 'photocopier', 'Pickelhaube', 'picket fence', 'pickup truck', 'pier', 'piggy bank', 'pillow', 'ping-pong ball', 'pinwheel', 'pitcher', 'planetarium', 'plastic bag', 'plow', 'plunger', 'pole', 'police van', 'poncho', 'billiard table', 'soda bottle', 'pot', \"potter's wheel\", 'power drill', 'prayer rug', 'printer', 'prison', 'projectile', 'punching bag', 'purse', 'quilt', 'racket', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reflex camera', 'refrigerator', 'restaurant', 'rifle', 'rocking chair', 'eraser', 'rugby ball', 'ruler', 'safe', 'salt shaker', 'sandal', 'sarong', 'saxophone', 'weighing scale', 'school bus', 'schooner', 'scoreboard', 'CRT screen', 'screw', 'seat belt', 'sewing machine', 'shoe store', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'shower curtain', 'ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'soup bowl', 'space bar', 'space heater', 'space shuttle', 'spatula', 'motorboat', 'spider web', 'spotlight', 'steam locomotive', 'through arch bridge', 'stethoscope', 'scarf', 'stone wall', 'stove', 'strainer', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglass', 'sunglasses', 'sunscreen', 'suspension bridge', 'mop', 'sweatshirt', 'swimsuit', 'swing', 'syringe', 'table lamp', 'tank', 'tape player', 'teapot', 'teddy bear', 'television', 'tennis ball', 'thatched roof', 'front curtain', 'threshing machine', 'tile roof', 'toaster', 'tobacco shop', 'totem pole', 'tow truck', 'toy store', 'tractor', 'tray', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'tub', 'turnstile', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vase', 'vault', 'velvet', 'vending machine', 'vestment', 'viaduct', 'volleyball', 'wall clock', 'wardrobe', 'military aircraft', 'sink', 'washing machine', 'water bottle', 'water jug', 'water tower', 'whiskey jug', 'whistle', 'wig', 'window screen', 'window shade', 'wine bottle', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'shipwreck', 'yurt', 'website', 'comic book', 'crossword', 'traffic sign', 'traffic light', 'dust jacket', 'menu', 'plate', 'guacamole', 'consomme', 'hot pot', 'ice pop', 'baguette', 'bagel', 'pretzel', 'cheeseburger', 'hot dog', 'mashed potato', 'cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'cucumber', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'Granny Smith', 'strawberry', 'orange', 'lemon', 'fig', 'pineapple', 'banana', 'jackfruit', 'custard apple', 'pomegranate', 'hay', 'carbonara', 'dough', 'meatloaf', 'pot pie', 'burrito', 'red wine', 'cup', 'eggnog', 'alp', 'bubble', 'cliff', 'coral reef', 'lakeshore', 'promontory', 'shoal', 'seashore', 'valley', 'volcano', 'bridegroom', 'scuba diver', 'rapeseed', 'daisy', \"yellow lady's slipper\", 'corn', 'acorn', 'rose hip', 'horse chestnut seed', 'agaric', 'gyromitra', 'stinkhorn mushroom', 'earth star', 'hen-of-the-woods', 'bolete', 'ear']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['accordion', 'banjo', 'bassoon', 'cornet', 'flute', 'French horn', 'microphone', 'oboe', 'saxophone', 'stage', 'trombone']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['jay', 'agama', 'trilobite', 'tick', 'centipede', 'grey parrot', 'macaw', 'slug', 'fiddler crab', 'common gallinule', 'Pekingese', 'Shih Tzu', 'Papillon', 'toy terrier', 'Redbone Coonhound', 'West Highland White Terrier', 'Schipperke', 'Groenendael', 'Shetland Sheepdog', 'collie', 'Entlebucher Sennenhund', 'Samoyed', 'Pomeranian', 'Keeshond', 'Pembroke Welsh Corgi', 'tabby cat', 'tiger cat', 'Egyptian Mau', 'ground beetle', 'dung beetle', 'rhinoceros beetle', 'leafhopper', 'dragonfly', 'damselfly', 'sea urchin', 'armadillo', 'garfish', 'abacus', 'accordion', 'analog clock', 'backpack', 'ballpoint pen', 'Band-Aid', 'banjo', 'baseball', 'bassoon', 'beaker', 'bib', 'ring binder', 'bolo tie', 'bookcase', 'bookstore', 'bow', 'bow tie', 'broom', 'bucket', 'buckle', 'cauldron', 'candle', 'can opener', 'tool kit', 'carton', 'cassette', 'cassette player', 'CD player', 'mobile phone', 'chain', 'chime', 'Christmas stocking', 'cleaver', 'cloak', 'coffee mug', 'coil', 'computer keyboard', 'confectionery store', 'corkscrew', 'croquet ball', 'crutch', 'desk', 'desktop computer', 'diaper', 'digital clock', 'digital watch', 'dishcloth', 'doormat', 'drum', 'drumstick', 'electric guitar', 'envelope', 'face powder', 'feather boa', 'filing cabinet', 'flute', 'fountain pen', 'grand piano', 'barrette', 'hair spray', 'hammer', 'hand-held computer', 'handkerchief', 'hard disk drive', 'harmonica', 'holster', 'hook', 'iPod', 'jeans', 'jigsaw puzzle', 'knot', 'lab coat', 'ladle', 'laptop computer', 'lens cap', 'paper knife', 'library', 'lighter', 'lipstick', 'lotion', 'loupe', 'mail bag', 'maraca', 'marimba', 'mask', 'match', 'maypole', 'medicine chest', 'microphone', 'miniskirt', 'mitten', 'modem', 'square academic cap', 'computer mouse', 'nail', 'necklace', 'nipple', 'oboe', 'ocarina', 'oil filter', 'organ', 'oscilloscope', 'packet', 'paddle', 'paintbrush', 'pan flute', 'paper towel', 'pencil case', 'pencil sharpener', 'perfume', 'plectrum', 'piggy bank', 'pill bottle', 'pinwheel', 'plastic bag', 'plunger', 'Polaroid camera', 'pole', 'pot', 'prayer rug', 'printer', 'purse', 'quill', 'racket', 'radiator', 'radio', 'reel', 'remote control', 'revolver', 'rifle', 'eraser', 'ruler', 'safety pin', 'salt shaker', 'scabbard', 'screw', 'screwdriver', 'shovel', 'ski', 'ski mask', 'slide rule', 'snorkel', 'sock', 'sombrero', 'soup bowl', 'spatula', 'spindle', 'steel drum', 'stethoscope', 'scarf', 'strainer', 'sunglass', 'sunglasses', 'sunscreen', 'mop', 'sweatshirt', 'switch', 'syringe', 'tape player', 'tennis ball', 'thimble', 'tobacco shop', 'torch', 'tray', 'tripod', 'umbrella', 'upright piano', 'vase', 'velvet', 'violin', 'wall clock', 'wallet', 'water bottle', 'whistle', 'wig', 'Windsor tie', 'wooden spoon', 'wool', 'comic book', 'crossword', 'dust jacket', 'ice pop', 'zucchini', 'Granny Smith', 'pineapple', 'cup', 'bubble', 'shoal', 'toilet paper']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['wallaby', 'hyena', 'mongoose', 'meerkat', 'marmot', 'badger', 'ring-tailed lemur']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['Japanese Chin', 'Pekingese', 'Shih Tzu', 'Lhasa Apso', 'Tibetan Mastiff', 'Affenpinscher', 'pug', 'Pomeranian', 'Keeshond', 'Griffon Bruxellois', 'Persian cat']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['tench', 'great white shark', 'tiger shark', 'hammerhead shark', 'electric ray', 'cock', 'hen', 'ostrich', 'brambling', 'magpie', 'American dipper', 'kite', 'vulture', 'great grey owl', 'fire salamander', 'smooth newt', 'newt', 'spotted salamander', 'axolotl', 'American bullfrog', 'tree frog', 'tailed frog', 'loggerhead sea turtle', 'leatherback sea turtle', 'mud turtle', 'terrapin', 'box turtle', 'banded gecko', 'green iguana', 'desert grassland whiptail lizard', 'agama', 'frilled-necked lizard', 'alligator lizard', 'Gila monster', 'chameleon', 'Komodo dragon', 'Nile crocodile', 'American alligator', 'triceratops', 'worm snake', 'ring-necked snake', 'eastern hog-nosed snake', 'kingsnake', 'garter snake', 'vine snake', 'night snake', 'boa constrictor', 'African rock python', 'Indian cobra', 'green mamba', 'sea snake', 'Saharan horned viper', 'eastern diamondback rattlesnake', 'sidewinder', 'trilobite', 'harvestman', 'scorpion', 'yellow garden spider', 'barn spider', 'tarantula', 'tick', 'centipede', 'black grouse', 'ptarmigan', 'ruffed grouse', 'prairie grouse', 'peacock', 'quail', 'partridge', 'grey parrot', 'coucal', 'hornbill', 'hummingbird', 'jacamar', 'toucan', 'red-breasted merganser', 'goose', 'black swan', 'tusker', 'echidna', 'platypus', 'wallaby', 'koala', 'wombat', 'brain coral', 'flatworm', 'nematode', 'conch', 'snail', 'slug', 'sea slug', 'chiton', 'chambered nautilus', 'Dungeness crab', 'rock crab', 'fiddler crab', 'red king crab', 'American lobster', 'isopod', 'black stork', 'great egret', 'crane (bird)', 'limpkin', 'common gallinule', 'American coot', 'bustard', 'dowitcher', 'pelican', 'king penguin', 'albatross', 'grey whale', 'killer whale', 'dugong', 'sea lion', 'Chihuahua', 'Japanese Chin', 'Maltese', 'Pekingese', 'Shih Tzu', 'King Charles Spaniel', 'Papillon', 'toy terrier', 'Rhodesian Ridgeback', 'Afghan Hound', 'Basset Hound', 'Beagle', 'Bloodhound', 'Bluetick Coonhound', 'Black and Tan Coonhound', 'Treeing Walker Coonhound', 'English foxhound', 'Redbone Coonhound', 'borzoi', 'Irish Wolfhound', 'Italian Greyhound', 'Whippet', 'Ibizan Hound', 'Norwegian Elkhound', 'Otterhound', 'Saluki', 'Scottish Deerhound', 'Weimaraner', 'Staffordshire Bull Terrier', 'American Staffordshire Terrier', 'Bedlington Terrier', 'Border Terrier', 'Kerry Blue Terrier', 'Irish Terrier', 'Norfolk Terrier', 'Norwich Terrier', 'Yorkshire Terrier', 'Wire Fox Terrier', 'Lakeland Terrier', 'Sealyham Terrier', 'Airedale Terrier', 'Cairn Terrier', 'Australian Terrier', 'Dandie Dinmont Terrier', 'Boston Terrier', 'Miniature Schnauzer', 'Giant Schnauzer', 'Standard Schnauzer', 'Scottish Terrier', 'Tibetan Terrier', 'Australian Silky Terrier', 'Soft-coated Wheaten Terrier', 'West Highland White Terrier', 'Flat-Coated Retriever', 'Curly-coated Retriever', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'German Shorthaired Pointer', 'Vizsla', 'English Setter', 'Irish Setter', 'Gordon Setter', 'Brittany', 'Clumber Spaniel', 'English Springer Spaniel', 'Welsh Springer Spaniel', 'Cocker Spaniels', 'Sussex Spaniel', 'Irish Water Spaniel', 'Kuvasz', 'Schipperke', 'Groenendael', 'Malinois', 'Briard', 'Australian Kelpie', 'Komondor', 'Old English Sheepdog', 'Shetland Sheepdog', 'collie', 'Border Collie', 'Bouvier des Flandres', 'Rottweiler', 'German Shepherd Dog', 'Dobermann', 'Miniature Pinscher', 'Greater Swiss Mountain Dog', 'Bernese Mountain Dog', 'Appenzeller Sennenhund', 'Entlebucher Sennenhund', 'Boxer', 'Bullmastiff', 'Tibetan Mastiff', 'French Bulldog', 'Great Dane', 'St. Bernard', 'husky', 'Alaskan Malamute', 'Siberian Husky', 'Dalmatian', 'Affenpinscher', 'Basenji', 'pug', 'Leonberger', 'Newfoundland', 'Pyrenean Mountain Dog', 'Samoyed', 'Pomeranian', 'Chow Chow', 'Keeshond', 'Griffon Bruxellois', 'Pembroke Welsh Corgi', 'Cardigan Welsh Corgi', 'Toy Poodle', 'Miniature Poodle', 'Standard Poodle', 'Mexican hairless dog', 'grey wolf', 'Alaskan tundra wolf', 'red wolf', 'coyote', 'dingo', 'dhole', 'African wild dog', 'hyena', 'red fox', 'kit fox', 'Arctic fox', 'grey fox', 'tabby cat', 'tiger cat', 'Persian cat', 'Siamese cat', 'Egyptian Mau', 'cougar', 'lynx', 'leopard', 'snow leopard', 'jaguar', 'lion', 'tiger', 'cheetah', 'brown bear', 'American black bear', 'polar bear', 'sloth bear', 'mongoose', 'meerkat', 'tiger beetle', 'ground beetle', 'longhorn beetle', 'leaf beetle', 'dung beetle', 'rhinoceros beetle', 'weevil', 'bee', 'ant', 'grasshopper', 'cricket', 'cockroach', 'lacewing', 'ringlet', 'gossamer-winged butterfly', 'sea cucumber', 'cottontail rabbit', 'hare', 'Angora rabbit', 'porcupine', 'fox squirrel', 'marmot', 'beaver', 'guinea pig', 'common sorrel', 'zebra', 'pig', 'wild boar', 'warthog', 'hippopotamus', 'ox', 'water buffalo', 'bison', 'ram', 'bighorn sheep', 'Alpine ibex', 'hartebeest', 'impala', 'gazelle', 'dromedary', 'llama', 'weasel', 'mink', 'European polecat', 'black-footed ferret', 'otter', 'skunk', 'badger', 'armadillo', 'three-toed sloth', 'orangutan', 'gorilla', 'chimpanzee', 'gibbon', 'siamang', 'guenon', 'patas monkey', 'baboon', 'macaque', 'langur', 'black-and-white colobus', 'proboscis monkey', 'marmoset', 'white-headed capuchin', 'howler monkey', 'titi', \"Geoffroy's spider monkey\", 'common squirrel monkey', 'ring-tailed lemur', 'indri', 'Asian elephant', 'African bush elephant', 'red panda', 'giant panda', 'snoek', 'eel', 'coho salmon', 'rock beauty', 'sturgeon', 'garfish', 'pufferfish', 'abaya', 'academic gown', 'accordion', 'airliner', 'airship', 'ambulance', 'amphibious vehicle', 'analog clock', 'waste container', 'assault rifle', 'backpack', 'balance beam', 'balloon', 'ballpoint pen', 'Band-Aid', 'banjo', 'baluster', 'barber chair', 'barbershop', 'barn', 'barometer', 'barrel', 'wheelbarrow', 'baseball', 'bassinet', 'bassoon', 'bathtub', 'station wagon', 'lighthouse', 'beaker', 'military cap', 'beer bottle', 'bell-cot', 'tandem bicycle', 'ring binder', 'binoculars', 'boathouse', 'bobsleigh', 'bolo tie', 'poke bonnet', 'bookcase', 'bookstore', 'bottle cap', 'bow', 'brass', 'bra', 'breakwater', 'breastplate', 'broom', 'bucket', 'buckle', 'bulletproof vest', 'high-speed train', 'butcher shop', 'taxicab', 'cauldron', 'candle', 'cannon', 'can opener', 'tool kit', 'carton', 'car wheel', 'automated teller machine', 'cassette', 'cassette player', 'castle', 'catamaran', 'cello', 'chain', 'chain-link fence', 'chain mail', 'chainsaw', 'chest', 'chiffonier', 'chime', 'Christmas stocking', 'church', 'movie theater', 'cleaver', 'cliff dwelling', 'cloak', 'clogs', 'cocktail shaker', 'coffeemaker', 'coil', 'combination lock', 'computer keyboard', 'convertible', 'corkscrew', 'cornet', 'cowboy boot', 'cowboy hat', 'cradle', 'crane (machine)', 'crash helmet', 'crate', 'Crock Pot', 'croquet ball', 'cuirass', 'dam', 'desktop computer', 'diaper', 'digital clock', 'digital watch', 'dining table', 'dishwasher', 'disc brake', 'dock', 'dog sled', 'dome', 'doormat', 'drilling rig', 'drum', 'drumstick', 'dumbbell', 'Dutch oven', 'electric fan', 'electric guitar', 'electric locomotive', 'entertainment center', 'envelope', 'espresso machine', 'face powder', 'feather boa', 'filing cabinet', 'fire screen sheet', 'flagpole', 'flute', 'folding chair', 'forklift', 'fountain', 'fountain pen', 'four-poster bed', 'freight car', 'French horn', 'frying pan', 'fur coat', 'garbage truck', 'gas mask', 'golf ball', 'golf cart', 'gondola', 'gong', 'greenhouse', 'grille', 'grocery store', 'guillotine', 'barrette', 'half-track', 'hammer', 'hamper', 'hair dryer', 'hand-held computer', 'handkerchief', 'harmonica', 'harp', 'harvester', 'hatchet', 'home theater', 'honeycomb', 'hook', 'hoop skirt', 'horizontal bar', 'horse-drawn vehicle', 'hourglass', 'clothes iron', \"jack-o'-lantern\", 'jeep', 'T-shirt', 'jigsaw puzzle', 'pulled rickshaw', 'joystick', 'knee pad', 'lab coat', 'ladle', 'lampshade', 'laptop computer', 'lawn mower', 'paper knife', 'library', 'lighter', 'limousine', 'lipstick', 'slip-on shoe', 'lotion', 'speaker', 'loupe', 'sawmill', 'magnetic compass', 'mail bag', 'tank suit', 'manhole cover', 'maraca', 'marimba', 'mask', 'match', 'maypole', 'maze', 'measuring cup', 'megalith', 'microphone', 'microwave oven', 'milk can', 'minibus', 'minivan', 'missile', 'mitten', 'mobile home', 'Model T', 'modem', 'monastery', 'moped', 'mortar', 'square academic cap', 'mosque', 'mosquito net', 'scooter', 'mountain bike', 'tent', 'computer mouse', 'mousetrap', 'moving van', 'muzzle', 'nail', 'neck brace', 'nipple', 'notebook computer', 'obelisk', 'oboe', 'ocarina', 'oil filter', 'organ', 'oscilloscope', 'overskirt', 'bullock cart', 'oxygen mask', 'packet', 'paddle', 'paddle wheel', 'padlock', 'paintbrush', 'palace', 'pan flute', 'paper towel', 'parachute', 'parallel bars', 'park bench', 'passenger car', 'patio', 'payphone', 'pedestal', 'pencil sharpener', 'Petri dish', 'photocopier', 'plectrum', 'Pickelhaube', 'pickup truck', 'pier', 'piggy bank', 'pill bottle', 'ping-pong ball', 'pirate ship', 'pitcher', 'hand plane', 'planetarium', 'plastic bag', 'plate rack', 'plow', 'plunger', 'Polaroid camera', 'pole', 'police van', 'billiard table', 'pot', \"potter's wheel\", 'power drill', 'prayer rug', 'printer', 'prison', 'projectile', 'projector', 'hockey puck', 'punching bag', 'quill', 'quilt', 'race car', 'radiator', 'radio', 'radio telescope', 'rain barrel', 'recreational vehicle', 'reel', 'reflex camera', 'remote control', 'restaurant', 'revolver', 'rifle', 'rocking chair', 'rotisserie', 'eraser', 'ruler', 'running shoe', 'safety pin', 'sandal', 'sarong', 'saxophone', 'scabbard', 'weighing scale', 'school bus', 'scoreboard', 'CRT screen', 'screw', 'screwdriver', 'shield', 'shoji', 'shopping basket', 'shopping cart', 'shovel', 'shower cap', 'ski', 'ski mask', 'sleeping bag', 'slide rule', 'sliding door', 'slot machine', 'snorkel', 'snowmobile', 'snowplow', 'soap dispenser', 'soccer ball', 'sock', 'solar thermal collector', 'sombrero', 'space bar', 'space heater', 'space shuttle', 'spatula', 'motorboat', 'sports car', 'spotlight', 'steam locomotive', 'through arch bridge', 'steel drum', 'stethoscope', 'stone wall', 'stopwatch', 'stove', 'tram', 'stretcher', 'couch', 'stupa', 'submarine', 'suit', 'sundial', 'sunglass', 'sunscreen', 'suspension bridge', 'sweatshirt', 'swimsuit', 'swing', 'switch', 'table lamp', 'tank', 'tape player', 'television', 'tennis ball', 'thatched roof', 'front curtain', 'thimble', 'threshing machine', 'throne', 'tile roof', 'toaster', 'tobacco shop', 'toilet seat', 'torch', 'totem pole', 'tow truck', 'toy store', 'tractor', 'semi-trailer truck', 'trench coat', 'tricycle', 'trimaran', 'tripod', 'triumphal arch', 'trolleybus', 'tub', 'typewriter keyboard', 'umbrella', 'unicycle', 'upright piano', 'vacuum cleaner', 'vault', 'velvet', 'viaduct', 'violin', 'waffle iron', 'wall clock', 'wallet', 'wardrobe', 'military aircraft', 'sink', 'water jug', 'whiskey jug', 'whistle', 'window screen', 'window shade', 'Windsor tie', 'wok', 'wooden spoon', 'wool', 'split-rail fence', 'shipwreck', 'yawl', 'yurt', 'website', 'comic book', 'crossword', 'traffic sign', 'dust jacket', 'plate', 'consomme', 'hot pot', 'trifle', 'baguette', 'pretzel', 'cabbage', 'broccoli', 'cauliflower', 'zucchini', 'spaghetti squash', 'acorn squash', 'butternut squash', 'artichoke', 'bell pepper', 'cardoon', 'mushroom', 'strawberry', 'fig', 'jackfruit', 'custard apple', 'hay', 'carbonara', 'chocolate syrup', 'meatloaf', 'pot pie', 'eggnog', 'alp', 'cliff', 'coral reef', 'geyser', 'lakeshore', 'promontory', 'shoal', 'seashore', 'valley', 'volcano', 'baseball player', 'scuba diver', 'daisy', \"yellow lady's slipper\", 'corn', 'acorn', 'rose hip', 'horse chestnut seed', 'coral fungus', 'gyromitra', 'stinkhorn mushroom', 'earth star', 'hen-of-the-woods', 'bolete', 'ear']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['electric ray', 'stingray']\n\n\n\n\n\n\n\n\n\nThe prediction set is: ['loggerhead sea turtle', 'leatherback sea turtle', 'terrapin', 'box turtle']",
    "crumbs": [
      "Home",
      "Lecture 8 - Conformalized Classification"
    ]
  }
]