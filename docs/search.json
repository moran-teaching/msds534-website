[
  {
    "objectID": "lec-1/lecture-1.html",
    "href": "lec-1/lecture-1.html",
    "title": "Lecture 1",
    "section": "",
    "text": "# import packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker, cm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom scipy.interpolate import splrep, BSpline\nfrom sklearn.decomposition import PCA\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib.pyplot import subplots\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nimport sklearn\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#linear-regression-example",
    "href": "lec-1/lecture-1.html#linear-regression-example",
    "title": "Lecture 1",
    "section": "Linear Regression example",
    "text": "Linear Regression example\nSpecify parameters\n\nn = 100\np = 1\nbeta = 3\nsigma = 1\n\nGenerate data\n\nx = np.random.normal(size=(n, p))\ny = x * beta + np.random.normal(size=(n, 1)) * sigma\n\ncolnames = ['x' + str(i) for i in range(1, p+1)]\ncolnames.insert(0, 'y')\n\ndf = pd.DataFrame(np.hstack((y, x)), columns = colnames)\n\nFit linear regression model using sklearn\n\nlm = LinearRegression()\nlm.fit(x, y)\ny_hat = lm.predict(x)\nresid = y - y_hat\n\nPlot x vs. y using seaborn\n\nsns.set_theme()\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0])\n\n\n\n\n\n\n\n\nPlot x vs. y, including residual distances\n\ny_min = np.minimum(y, y_hat)\ny_max = np.maximum(y, y_hat)\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0])\nlm_plot.ax.vlines(x=list(x[:,0]), ymin=list(y_min[:,0]), ymax=list(y_max[:,0]), color = 'red', alpha=0.5)",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#overfitting-example",
    "href": "lec-1/lecture-1.html#overfitting-example",
    "title": "Lecture 1",
    "section": "Overfitting example",
    "text": "Overfitting example\n\nsort_ind = np.argsort(x, axis=0)\nxsort = np.take_along_axis(x, sort_ind, axis=0)\nysort = np.take_along_axis(y, sort_ind, axis=0)\ntck = splrep(xsort, ysort, s=20)\n\nxspline = np.arange(x.min(), x.max(), 0.01)\nyspline = BSpline(*tck)(xspline)\n\nlm_plot = sns.relplot(df, x='x1', y='y', height = 3.5, aspect = 1.2)\nplt.axline((0,lm.intercept_[0]), slope=lm.coef_[0][0], label = \"Linear Regression\")\nplt.plot(xspline, yspline,color='orange', label = \"Spline\")\nplt.legend(loc='upper left')",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#shrinkage-plot",
    "href": "lec-1/lecture-1.html#shrinkage-plot",
    "title": "Lecture 1",
    "section": "Shrinkage plot",
    "text": "Shrinkage plot",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#lasso-example",
    "href": "lec-1/lecture-1.html#lasso-example",
    "title": "Lecture 1",
    "section": "Lasso example",
    "text": "Lasso example\nThis code is adapted from ISLP labs.\n\nn = 100\np = 90\nbeta = np.zeros(p)\nbeta[0]=3\nbeta[1]=3\ncov = 0.6 * np.ones((p, p))\nnp.fill_diagonal(cov, 1)\nx = np.random.multivariate_normal(mean=np.zeros(p), cov=cov, size=n)\ny = np.matmul(x, beta) + np.random.normal(size=n)\n\nx_columns = ['x' + str(i+1) for i in range(p)]\n\n\n# set up cross-validation\nK=5\nkfold = sklearn.model_selection.KFold(K,random_state=0,shuffle=True)\n\n# function to standardize input\nscaler = StandardScaler(with_mean=True, with_std=True)\n\n\nlassoCV = sklearn.linear_model.ElasticNetCV(n_alphas=100, l1_ratio=1,cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),('lasso', lassoCV)])\npipeCV.fit(x, y)\ntuned_lasso = pipeCV.named_steps['lasso']\ntuned_lasso.alpha_\n\nlambdas, soln_array = sklearn.linear_model.Lasso.path(x, y,l1_ratio=1,n_alphas=100)[:2]\nsoln_path = pd.DataFrame(soln_array.T,columns=x_columns, index=-np.log(lambdas))\n\n\npath_fig, ax = subplots(figsize=(8,8))\nsoln_path.plot(ax=ax, legend=False)\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Standardized coefficiients', fontsize=20);\n\n\n\n\n\n\n\n\n\nlassoCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(tuned_lasso.alphas_),tuned_lasso.mse_path_.mean(1),yerr=tuned_lasso.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_lasso.alpha_), c='k', ls='--')\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#ridge-regression",
    "href": "lec-1/lecture-1.html#ridge-regression",
    "title": "Lecture 1",
    "section": "Ridge regression",
    "text": "Ridge regression\nThis code is adapted from ISLP labs\n\nlambdas = 10**np.linspace(8, -2, 100) / y.std()\nridgeCV = sklearn.linear_model.ElasticNetCV(alphas=lambdas, \n                           l1_ratio=0,\n                           cv=kfold)\npipeCV = Pipeline(steps=[('scaler', scaler),\n                         ('ridge', ridgeCV)])\npipeCV.fit(x, y)\n\nPipeline(steps=[('scaler', StandardScaler()),\n                ('ridge',\n                 ElasticNetCV(alphas=array([1.84404932e+07, 1.46137755e+07, 1.15811671e+07, 9.17787690e+06,\n       7.27331049e+06, 5.76397417e+06, 4.56785096e+06, 3.61994377e+06,\n       2.86874353e+06, 2.27343019e+06, 1.80165454e+06, 1.42778041e+06,\n       1.13149156e+06, 8.96687712e+05, 7.10609677e+05, 5.63146016e+05,\n       4.46283587e+05, 3.53672111e+05,...\n       1.53096214e-01, 1.21326131e-01, 9.61488842e-02, 7.61963464e-02,\n       6.03843014e-02, 4.78535262e-02, 3.79231012e-02, 3.00534091e-02,\n       2.38168128e-02, 1.88744168e-02, 1.49576525e-02, 1.18536838e-02,\n       9.39384172e-03, 7.44445891e-03, 5.89960638e-03, 4.67533716e-03,\n       3.70512474e-03, 2.93624800e-03, 2.32692632e-03, 1.84404932e-03]),\n                              cv=KFold(n_splits=5, random_state=0, shuffle=True),\n                              l1_ratio=0))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.Pipeline?Documentation for PipelineiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nsteps \n[('scaler', ...), ('ridge', ...)]\n\n\n\ntransform_input \nNone\n\n\n\nmemory \nNone\n\n\n\nverbose \nFalse\n\n\n\n\n            \n        \n    StandardScaler?Documentation for StandardScaler\n        \n            \n                Parameters\n                \n\n\n\n\ncopy \nTrue\n\n\n\nwith_mean \nTrue\n\n\n\nwith_std \nTrue\n\n\n\n\n            \n        \n    ElasticNetCV?Documentation for ElasticNetCV\n        \n            \n                Parameters\n                \n\n\n\n\nl1_ratio \n0\n\n\n\neps \n0.001\n\n\n\nn_alphas \n'deprecated'\n\n\n\nalphas \narray([1.8440...84404932e-03])\n\n\n\nfit_intercept \nTrue\n\n\n\nprecompute \n'auto'\n\n\n\nmax_iter \n1000\n\n\n\ntol \n0.0001\n\n\n\ncv \nKFold(n_split... shuffle=True)\n\n\n\ncopy_X \nTrue\n\n\n\nverbose \n0\n\n\n\nn_jobs \nNone\n\n\n\npositive \nFalse\n\n\n\nrandom_state \nNone\n\n\n\nselection \n'cyclic'\n\n\n\n\n            \n        \n    \n\n\n\nlambdas, soln_array = sklearn.linear_model.ElasticNet.path(x, y,l1_ratio=0,alphas=lambdas)[:2]\nsoln_path = pd.DataFrame(soln_array.T,columns=x_columns, index=-np.log(lambdas))\n\npath_fig, ax = subplots(figsize=(8,8))\nsoln_path.plot(ax=ax, legend=False)\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Standardized coefficiients', fontsize=20);\n\n\n\n\n\n\n\n\n\ntuned_ridge = pipeCV.named_steps['ridge']\nridgeCV_fig, ax = subplots(figsize=(8,8))\nax.errorbar(-np.log(lambdas),\n            tuned_ridge.mse_path_.mean(1),\n            yerr=tuned_ridge.mse_path_.std(1) / np.sqrt(K))\nax.axvline(-np.log(tuned_ridge.alpha_), c='k', ls='--')\nax.set_xlabel('$-\\log(\\lambda)$', fontsize=20)\nax.set_ylabel('Cross-validated MSE', fontsize=20);",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#logistic-regression-example",
    "href": "lec-1/lecture-1.html#logistic-regression-example",
    "title": "Lecture 1",
    "section": "Logistic regression example",
    "text": "Logistic regression example\nGenerate data\n\nn = 100\np = 2\nx = np.random.uniform(-2, 2, size=(n, p))\n\nbeta = np.array([2.5, -2.5])\nmu = np.matmul(x, beta)\nprob = 1/(1 + np.exp(-mu))\n\ny = np.zeros((n))\nfor i in range(n):\n    y[i] = np.random.binomial(1, prob[i], 1)[0]\n\ndf = np.hstack([y.reshape((n, 1)), x])\ndf = pd.DataFrame(df, columns = ['y', 'x1', 'x2'])\n\nsns.set_theme()\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nlogit_plot.figure.subplots_adjust(top=.9)\n\n\n\n\n\n\n\n\nFitting the logistic regression model\n\nlog_fit = LogisticRegression()\nlog_fit.fit(x, y)\ncoeffs = log_fit.coef_[0]\ncoeff = -coeffs[0]/coeffs[1]\n\nPlot x_1\\beta_1 + x_2\\beta_2 = 0\n\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nplt.axline([0,0], slope=coeff)\n## title\nlogit_plot.figure.subplots_adjust(top=.9)\nlogit_plot.figure.suptitle(str(round(coeffs[0], 2)) + r'$x_1$ - ' + str(round(-coeffs[1], 2)) + r'$x_2 = 0$')\n## fill in area\nx_fill = np.linspace(-2, 2, num=200)\ny_line = coeff * x_fill\nlogit_plot.ax.fill_between(x_fill, y_line, 2, color='blue', alpha=0.2)\nlogit_plot.ax.fill_between(x_fill, -2, y_line, color='orange', alpha=0.2)\n\nlogit_plot.ax.annotate(r'$\\bf P(Y=1)&lt;0.5$', (0.5, 2.1), color='blue')\nlogit_plot.ax.annotate(r'$\\bf P(Y=1)&gt;0.5$', (0.5, -2.2), color='darkorange')\n\nText(0.5, -2.2, '$\\\\bf P(Y=1)&gt;0.5$')\n\n\n\n\n\n\n\n\n\n\n# Create a meshgrid for x1 and x2\nx1_range = np.linspace(x[:, 0].min(), x[:, 0].max(), 200)\nx2_range = np.linspace(x[:, 1].min(), x[:, 1].max(), 200)\nX1, X2 = np.meshgrid(x1_range, x2_range)\n\n# Compute the sigmoid function using the fitted logistic regression coefficients\nZ = 1 / (1 + np.exp(-(log_fit.intercept_[0] + log_fit.coef_[0,0]*X1 + log_fit.coef_[0,1]*X2)))\n\nfig = plt.figure(figsize=(5, 7))\nax = fig.add_subplot(111, projection='3d')\n\n# Set background to white\nax.set_facecolor('white')\nfig.patch.set_facecolor('white')\n\n# Plot with smooth color transitions\nsurf = ax.plot_surface(X1, X2, Z, cmap='coolwarm', antialiased=True, linewidth=0, rstride=1, cstride=1)\n\nax.view_init(elev=15, azim=65+155)\nax.set_xlabel('x1')\nax.set_ylabel('x2')\nax.set_zlabel(r'P(y=1)')\n\nax.set_title(\" \"*115) \nax.text2D(0.5, 0.91, r'g(' + str(round(coeffs[0], 2)) + r'$x_1$  ' +\n         str(round(coeffs[1], 2)) + r'$x_2)$',\n         transform=ax.transAxes, ha='center', va='top')\n\nText(0.5, 0.91, 'g(1.76$x_1$  -1.8$x_2)$')",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "lec-1/lecture-1.html#principal-components-analysis",
    "href": "lec-1/lecture-1.html#principal-components-analysis",
    "title": "Lecture 1",
    "section": "Principal Components Analysis",
    "text": "Principal Components Analysis\n\npenguins = sns.load_dataset(\"penguins\")\n\nfig = plt.figure()\n\nax = Axes3D(fig)\nfig.add_axes(ax)\n\ncmap = matplotlib.colors.ListedColormap(sns.color_palette(\"Paired\", 3))\n\ncols = penguins['species'].copy()\ncols[cols=='Adelie']=1\ncols[cols=='Chinstrap']=2\ncols[cols=='Gentoo']=3\n\nsc = ax.scatter3D(penguins['bill_depth_mm'],\n                penguins['bill_length_mm'],\n                penguins['flipper_length_mm'],\n                c = cols,\n                cmap=cmap,\n                alpha=1)\nax.set_xlabel('bill depth')\nax.set_ylabel('bill length')\nax.set_zlabel('flipper length')\nax.set_facecolor((1.0, 1.0, 1.0, 0.0))\n\n\n\n\n\n\n\n\n\nx = penguins[['bill_depth_mm', 'bill_length_mm', 'flipper_length_mm']]\nx = x.dropna(axis=0)\n\npca_fit = PCA()\npca_fit.fit(x)\nz = pca_fit.transform(x)\n\nz_df = pd.DataFrame(z[:, 0:2], columns = ['z1', 'z2'])\nz_df['species']=penguins['species']\n\nsns.set_theme()\npca_plot = sns.relplot(z_df, x='z1', y='z2', hue='species', palette=sns.color_palette(\"Paired\", 3), height=4)\n\n\n\n\n\n\n\n\n\nPC_values = np.linspace(1,3,3).reshape(3,1)\nscree_df = np.hstack([PC_values, pca_fit.explained_variance_ratio_.reshape(3,1)])\nscree_df = pd.DataFrame(scree_df, columns = ['Principal Components', 'Explained Variance Ratio'])\nscree_plot = sns.relplot(scree_df, x='Principal Components', y='Explained Variance Ratio', marker='o', kind='line', height=4)",
    "crumbs": [
      "Home",
      "Lecture 1"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "lec-2/lecture-2.html",
    "href": "lec-2/lecture-2.html",
    "title": "Lecture 2",
    "section": "",
    "text": "# import packages\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\nimport warnings\nwarnings.filterwarnings('ignore')",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#pytorch",
    "href": "lec-2/lecture-2.html#pytorch",
    "title": "Lecture 2",
    "section": "PyTorch",
    "text": "PyTorch\nPyTorch is an open-source machine learning library. It is one of the most popular frameworks for deep learning.\nIn Python, the PyTorch package is called torch, available here.",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#simple-linear-regression",
    "href": "lec-2/lecture-2.html#simple-linear-regression",
    "title": "Lecture 2",
    "section": "Simple Linear Regression",
    "text": "Simple Linear Regression\nLet’s see how PyTorch can help us solve a predictive problem using the most simple linear regression model.\nA simple linear regression model assumes that the observed data sample y_1, y_2, ..., y_n follows the following structure:\n\ny_i = a + bx_i + \\epsilon_i, \\quad \\epsilon_i \\sim N(0,1).\n\n\nData Generation\nLet’s start by creating some synthetic data to work with. The following code block does the following:\n\nWe generate x_1, x_2, ..., x_{100} with each x_i \\sim \\text{Uniform}(-2,2).\nWe let a = 1 and b = 2.\nWe create y_1, y_2, ..., y_{100} by setting y_i = 1 + 2x_i + \\epsilon_i, where \\epsilon_i is some standard Gaussian noise.\n\n\n# Data Generation\n# np.random.uniform generates random samples from a uniform distribution over [0, 1).\nx = np.random.uniform(low=-2,high=2, size =(100, 1))                              \n# the resulting dimension of vector x will be 100 x 1.\n\n# np.random.randn generates random samples from a standard normal distribution.\ny = 1 + 2 * x + np.random.randn(100, 1)            \n\n\n\nTrain-Validation Split\nNow we do a train-validation split, by randomly picking 80% of the indices as the train set and the rest as validation.\n\n# Shuffles the indices\nidx = np.arange(100)\nnp.random.shuffle(idx)\n\n# Uses first 80 random indices for train\ntrain_idx = idx[:80]\n# Uses the remaining indices for validation\nval_idx = idx[80:]\n\n# Generates train and validation sets\nx_train, y_train = x[train_idx], y[train_idx]\nx_val, y_val = x[val_idx], y[val_idx]\n\n\n## Below shows a plot of the two sets of data.\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\naxes[0].scatter(x_train, y_train, c = 'darkblue')\naxes[0].set_xlabel('x')\naxes[0].set_ylabel('y')\naxes[0].set_title('Generated Data - Train')\naxes[1].scatter(x_val, y_val, c = 'darkred')\naxes[1].set_xlabel('x')\naxes[1].set_title('Generated Data - Validation')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nMoving to PyTorch\nLet’s fit a linear regression model using PyTorch.\nFirst, we need to transform our data to PyTorch tensors.\n\n# Our data was in Numpy arrays, but we need to transform them into PyTorch's Tensors\nx_train_tensor = torch.tensor(x_train, dtype=torch.float)      \ny_train_tensor = torch.tensor(y_train, dtype=torch.float)\n\n\n# Here we can see the difference \nprint(type(x_train), type(x_train_tensor), x_train_tensor.type())\n\n&lt;class 'numpy.ndarray'&gt; &lt;class 'torch.Tensor'&gt; torch.FloatTensor\n\n\n\nInitialize Parameters and Require Gradients\n\n# Since we want to apply gradient descent on these parameters, we need\n# to set requires_grad = True\n\n### Initialize parameters... \na = torch.randn(1, requires_grad=True)      \nb = torch.randn(1, requires_grad=True)\n### ---------------------- \n\nprint(a, b)\n\ntensor([0.0764], requires_grad=True) tensor([1.3134], requires_grad=True)\n\n\n\n\nGradient Computation with Autograd\nThe loss is:\n L(a, b) = \\frac{1}{n} \\sum_{i=1}^n (y_i - a- b\\cdot x_i)^2 \nThe gradients are:\n \\frac{\\partial L}{\\partial a} = -2 \\cdot \\frac{1}{n} \\sum_{i=1}^n (y_i - a - b\\cdot x_i)  \\frac{\\partial L}{\\partial b} = -2 \\cdot \\frac{1}{n} \\sum_{i=1}^n x_i(y_i - a - b\\cdot x_i)\nWe won’t actually have to compute gradients - autograd is PyTorch’s automatic differentiation package, which will do it for us.\n\nThe backward() method helps us to compute partial derivatives of the loss function w.r.t. our parameters. It is essentially saying “do backpropagation”.\nWe obtain the computed gradients via the .grad attribute.\n\n\n# Specifying a learning rate\nlr = 1e-1\n\nyhat = a + b * x_train_tensor\nerror = y_train_tensor - yhat\nloss = (error ** 2).mean()\n\n# We just tell PyTorch to work its way BACKWARDS from the specified loss!\nloss.backward()\n# Let's check the computed gradients...\nprint('a grad (pytorch)', a.grad)\nprint('b grad (pytorch)', b.grad)\n\n# compare to actual gradients\nwith torch.no_grad():\n    a_grad = -2 * error.mean()\n    b_grad = -2 * (x_train_tensor * error).mean()\nprint('a grad (manual)', a_grad)\nprint('b grad (manual)', b_grad)\n\na grad (pytorch) tensor([-1.9982])\nb grad (pytorch) tensor([-2.5759])\na grad (manual) tensor(-1.9982)\nb grad (manual) tensor(-2.5759)\n\n\nNote: we use torch.no_grad() so PyTorch doesn’t keep track of the operations (otherwise, they may be included in the computation graph that PyTorch uses to calculate gradients).\nLet’s now take a gradient descent step.\n\n# update the parameters \nwith torch.no_grad():       \n    a -= lr * a.grad\n    b -= lr * b.grad\n# ---------------------- \n\n# PyTorch is \"clingy\" to its computed gradients, we need to tell it to zero out\na.grad.zero_()      # note the \"_\" which means \"in-place\"\nb.grad.zero_()\n\ntensor([0.])\n\n\nPutting it all together:\n\nn_epochs = 100\n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    # No more manual computation of gradients! \n    # a_grad = -2 * error.mean()\n    # b_grad = -2 * (x_tensor * error).mean()\n\n    ### We just tell PyTorch to work its way BACKWARDS from the specified loss! \n    loss.backward()            \n    ### ---------------------- \n\n    # Let's check the computed gradients...\n    if epoch % 5 == 0:\n        print('Epoch:', epoch, '(a, b) grad', a.grad, b.grad)\n    \n    # Updating the parameters\n    with torch.no_grad():\n        a -= lr * a.grad\n        b -= lr * b.grad\n    \n    a.grad.zero_()\n    b.grad.zero_()\n    ### ---------------------- \n    \nprint('Final (a, b)', a, b)\n\nEpoch: 0 (a, b) grad tensor([-1.5175]) tensor([-1.7315])\nEpoch: 5 (a, b) grad tensor([-0.4175]) tensor([-0.2149])\nEpoch: 10 (a, b) grad tensor([-0.1279]) tensor([-0.0152])\nEpoch: 15 (a, b) grad tensor([-0.0417]) tensor([0.0038])\nEpoch: 20 (a, b) grad tensor([-0.0141]) tensor([0.0027])\nEpoch: 25 (a, b) grad tensor([-0.0048]) tensor([0.0012])\nEpoch: 30 (a, b) grad tensor([-0.0017]) tensor([0.0004])\nEpoch: 35 (a, b) grad tensor([-0.0006]) tensor([0.0002])\nEpoch: 40 (a, b) grad tensor([-0.0002]) tensor([5.5239e-05])\nEpoch: 45 (a, b) grad tensor([-6.8247e-05]) tensor([1.9357e-05])\nEpoch: 50 (a, b) grad tensor([-2.3618e-05]) tensor([6.4149e-06])\nEpoch: 55 (a, b) grad tensor([-8.0764e-06]) tensor([2.4475e-06])\nEpoch: 60 (a, b) grad tensor([-2.6971e-06]) tensor([1.3188e-06])\nEpoch: 65 (a, b) grad tensor([-8.7917e-07]) tensor([8.7172e-07])\nEpoch: 70 (a, b) grad tensor([-3.4273e-07]) tensor([7.8976e-07])\nEpoch: 75 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 80 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 85 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 90 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nEpoch: 95 (a, b) grad tensor([-1.7881e-07]) tensor([9.4995e-07])\nFinal (a, b) tensor([0.9563], requires_grad=True) tensor([2.0711], requires_grad=True)\n\n\nThis is the same as the simple linear regression formula:\n\nb_hat = ((x_train-x_train.mean()) * (y_train-y_train.mean())).sum() / ((x_train-x_train.mean())**2).sum()\n\n\na_hat = y_train.mean() - b_hat * x_train.mean()\n\n\nprint(f\"a (formula): {a_hat:.4f}, b (formula): {b_hat:.4f}\")\n\na (ls): 0.9563, b (ls): 2.0711\n\n\n\n\nUpdate All Parameters Simultaneously with Optimizer\nPreviously, we’ve been manually updating the parameters using the computed gradients. That’s probably fine for two parameters… but what if we had a whole lot of them?! We use one of PyTorch’s optimizers, like SGD or Adam.\nAn optimizer takes the parameters we want to update, the learning rate we want to use (and possibly many other hyper-parameters as well!) and performs the updates through its step() method.\nBesides, we also don’t need to zero the gradients one by one anymore. We just invoke the optimizer’s zero_grad() method and that’s it!\nIn the code below, we create a Stochastic Gradient Descent (SGD) optimizer to update our parameters a and b. Note: we will use the whole dataset, not batches, so it is technically GD (not SGD). However, the optimizer in torch is still called SGD.\n\na = torch.randn(1, requires_grad=True)\nb = torch.randn(1, requires_grad=True)\nprint(f\"Initializations: a = {a.item():.3f}, b = {b.item():.3f}\")\n\nlr = 1e-1\nn_epochs = 1000\n\n### Defines a SGD optimizer to update the parameters\noptimizer = optim.SGD([a, b], lr=lr)\n### ---------------------- \n\nfor epoch in range(n_epochs):\n    yhat = a + b * x_train_tensor\n    error = y_train_tensor - yhat\n    loss = (error ** 2).mean()\n\n    loss.backward()    \n    \n    # No more manual update!\n    # with torch.no_grad():\n    #     a -= lr * a.grad\n    #     b -= lr * b.grad\n\n    ### perform the update via step() ###\n    optimizer.step()\n    ### ---------------------- ###\n    \n    # No more telling PyTorch to let gradients go!\n    # a.grad.zero_()\n    # b.grad.zero_()\n\n    ### clearing the gradient ###\n    optimizer.zero_grad()\n    ### ---------------------- ###\n\nprint(f\"Output: a = {a.item():.3f}, b = {b.item():.3f}\")\n\nInitializations: a = -0.684, b = 0.537\nOutput: a = 1.113, b = 1.973\n\n\n\n\n\nModel Building\nPython is an object-oriented programming language.\nEvery item you interact with is an object. An object has a type, as well as:\n\nattributes (values)\nmethods (functions that can be applied to the object)\n\nAs an example, 'hello' in Python is an object of type str, with methods such as split, join etc.\nA Python class is a “blueprint” for an object.\nInheritance allows us to define a class that inherits all the methods and properties from another class.\n\nnn.Module Class\nIn PyTorch, a model is represented by a regular Python class that inherits from the nn.Module class. nn.Module is everywhere in PyTorch and represents mappings in neural networks.\nLet’s look at an example:\n\nclass ManualLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()              # inherits from nn.Module\n        # Initialization of a and b.\n        # To make \"a\" and \"b\" real parameters of the model, we need to wrap them with nn.Parameter\n        self.a = nn.Parameter(torch.randn(1, requires_grad=True))\n        self.b = nn.Parameter(torch.randn(1, requires_grad=True))\n        \n    def forward(self, x):\n        # Computes the outputs / predictions\n        return self.a + self.b * x\n\n\nclass ManualLinearRegression(nn.Module)\n\nthis statement declares a class ManualLinearRegression which inherits from the base class nn.Module.\n\n\nIndented beneath the class statement are the methods of this class: __init__ and forward:\n\n__init__\n\nall classes have an __init__ which is executed when the class is instantiated\nself refers to an instance of the class\nin __init__, we have attached the parameters a and b as attributes\nsuper().__init__() is a call to the __init__ of nn.Module. For torch models, we will always be making this super() call as it is necessary for the model to be properly interpreted by torch.\n\nforward is called when the neural network is run on input data.\n\n\n# Now we can create a model \nmodel = ManualLinearRegression()\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # In PyTorch, models have a train() method which sets the model to training mode.\n    \n    ### enter training mode \n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n\n    ### this is the forward step \n    yhat = model(x_train_tensor)\n    ### ---------------------- \n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())\n\nOrderedDict({'a': tensor([-0.6481]), 'b': tensor([1.4267])})\nOrderedDict({'a': tensor([1.1129]), 'b': tensor([1.9731])})\n\n\n\n\nUtilizing PyTorch Layers\nFor simple tasks like building a linear regression, we could directly create a and b as nn.Parameters.\nInstead, we can use PyTorch’s nn.Linear to create a linear layer. Later, this will let us build more complicated networks. Note: nn.Linear automatically adds a bias term.\n\nclass PyTorchLinearRegression(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 1) # arguments: (input dim, output dim)\n                \n    def forward(self, x):\n        return self.linear(x)\n\n\nfor name, param in PyTorchLinearRegression().named_parameters():\n    if param.requires_grad:\n        print(name, param.data)\n\nlinear.weight tensor([[-0.5715]])\nlinear.bias tensor([-0.5772])\n\n\n\n# Now we can create a model \nmodel = PyTorchLinearRegression()\n# We can also inspect its parameters using its state_dict\nprint(model.state_dict())\n\nlr = 1e-1\nn_epochs = 1000\n\nloss_fn = nn.MSELoss(reduction='mean')\noptimizer = optim.SGD(model.parameters(), lr=lr)\n\nfor epoch in range(n_epochs):\n    # In PyTorch, models have a train() method which sets the model to training mode.\n    \n    ### enter training mode ###\n    model.train()\n\n    # No more manual prediction!\n    # yhat = a + b * x_tensor\n\n    ### this is the forward step ###\n    yhat = model(x_train_tensor)\n    ### ---------------------- ###\n    \n    loss = loss_fn(y_train_tensor, yhat)\n    loss.backward()    \n    optimizer.step()\n    optimizer.zero_grad()\n    \nprint(model.state_dict())\n\nOrderedDict({'linear.weight': tensor([[-0.0886]]), 'linear.bias': tensor([0.0320])})\nOrderedDict({'linear.weight': tensor([[1.9731]]), 'linear.bias': tensor([1.1129])})",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#xor-problem",
    "href": "lec-2/lecture-2.html#xor-problem",
    "title": "Lecture 2",
    "section": "XOR problem",
    "text": "XOR problem\nThe “XOR” (or “exclusive OR”) problem is often used to illustrate the ability of neural networks to fit complicated functions. The XOR problem has a checkerboard structure:\n\nn = 5000\np = 2\nx = np.random.uniform(-2, 2, size=(n, p))\n\n#x[,1] &lt;- first column (R)\n#x[:, 0] &lt;- first column (Python)\n\ny = ((x[:,0] &lt; 0) & (x[:, 1] &gt; 0)).astype(x.dtype) + ((x[:,0] &gt; 0) & (x[:, 1] &lt; 0)).astype(x.dtype)\n\ndf = np.hstack([y.reshape((n, 1)), x])\ndf = pd.DataFrame(df, columns = ['y', 'x1', 'x2'])\n\nsns.set_theme()\nxor_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nxor_plot.figure.subplots_adjust(top=.9)\n\n\n\n\n\n\n\n\nLogistic regression can only fit linear decision boundaries, and so fails the XOR problem.\n\n## logitstic regression doesn't work\nlog_fit = LogisticRegression()\nlog_fit.fit(x, y)\ncoeffs = log_fit.coef_[0]\ncoeff = -coeffs[0]/coeffs[1]\n\n## plot\nlogit_plot = sns.relplot(df, x='x1', y='x2', hue='y', style='y')\nplt.axline([0,0], slope=coeff)\n## title\nlogit_plot.figure.subplots_adjust(top=.9)\nlogit_plot.figure.suptitle(str(round(coeffs[0], 2)) + r'$x_1$ + ' + '(' + str(round(coeffs[1], 2)) + ')' + r'$x_2$')\n## fill in area\nx_fill = np.linspace(-2, 2, num=200)\ny_line = coeff * x_fill\nlogit_plot.ax.fill_between(x_fill, y_line, 2, color='blue', alpha=0.2)\nlogit_plot.ax.fill_between(x_fill, -2, y_line, color='orange', alpha=0.2)\nlogit_plot.ax.set_ylim(-2,2)\n\n\n\n\n\n\n\n\n\nMulti-Layer Neural Network\nHere is an example of a one hidden-layer neural network in torch named XORNet.\nInstead of a single linear layer, we have:\n\na linear layer\na ReLU activation\na linear layer\na sigmoid activation\n\nnn.Sequential allows us to stack these layers together - we store the final object as an attribute of the model: self.sequential.\nThe forward method then passes input data through self.sequential to get the output. That is:\n f(x_i) = g(W^{(2)}(a(W^{(1)}x_i + b^{(1)}))+b^{(2)})\n\nclass XORNet(nn.Module):\n\n    def __init__(self, input_dim, hidden_dim):\n        super().__init__()\n        self.sequential = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        y = self.sequential(x)\n        y = torch.flatten(y) # we do this to turn y into a one-dim tensor\n        return y\n\n    def loss_fn(self, y, y_pred):\n        loss = y * torch.log(y_pred + 1e-8) + (1-y) * torch.log(1-y_pred + 1e-8)\n        output = -loss.sum()\n        return output\n\n\nhidden_dim=2\nmodel = XORNet(input_dim=p, hidden_dim=hidden_dim)\n\n\nmodel\n\nXORNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=2, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)\n\n\n\n\nData\nFor torch to read the data, it needs to be a torch.tensor type:\n\nx_train = torch.tensor(x, dtype=torch.float)\ny_train = torch.tensor(y, dtype=torch.float)\n\nWe combine x_train and y_train into a TensorDataset, a dataset recognizable by torch. TensorDataset stores the samples and their labels. It is a subclass of the more general torch.utils.data.Dataset, which you can customize for non-standard data.\n\ntrain_data = TensorDataset(x_train, y_train)\n\nTensorDataset is helpful as it can be passed to DataLoader(). DataLoader wraps an iterable around the Dataset class. This lets us loop over the DataLoader to extract a mini-batch at each iteration.\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nTo retrieve a sample mini-batch, the following will return a list containing two tensors: one for the features, another one for the labels.\n\nnext(iter(train_loader))\n\n[tensor([[-1.0248, -0.2131],\n         [-0.7861,  1.6893],\n         [-1.7932, -0.0574],\n         [ 0.7178, -0.4897],\n         [ 1.9000,  1.6038],\n         [ 0.7674, -1.9498],\n         [-0.2336, -1.4478],\n         [-1.9838, -0.8512],\n         [-0.2094,  0.1471],\n         [-1.2884, -0.1538]]),\n tensor([0., 1., 0., 1., 0., 1., 0., 0., 1., 0.])]\n\n\n\n\nTraining\nWe now set up the optimizer for training. We use Adam, and a base learning rate of lr=0.01. We set the number of epochs to 100. (Rule of thumb: pick largest lr that still results in convergence)\n\nlr = 0.01\nepochs = 100\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nXORNet(\n  (sequential): Sequential(\n    (0): Linear(in_features=2, out_features=2, bias=True)\n    (1): ReLU()\n    (2): Linear(in_features=2, out_features=1, bias=True)\n    (3): Sigmoid()\n  )\n)\n\n\nNow we train the model:\n\nfor epoch in range(epochs):\n    \n    # in each epoch, iterate over all batches of data (easily accessed through train_loader)\n    l = 0\n\n    for x_batch, y_batch in train_loader:\n\n        pred = model(x_batch)                   # this is the output from the forward function\n        loss = model.loss_fn(y_batch, pred)     # calculate loss function\n\n        loss.backward()                         # computes gradients wrt loss function\n        optimizer.step()                        # updates parameters \n        optimizer.zero_grad()                   # set the gradients back to zero (otherwise grads are accumulated)\n\n        l += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 2.55e+03\nepoch:  10 loss: 1.79e+03\nepoch:  20 loss: 1.79e+03\nepoch:  30 loss: 1.79e+03\nepoch:  40 loss: 1.79e+03\nepoch:  50 loss: 1.79e+03\nepoch:  60 loss: 1.79e+03\nepoch:  70 loss: 1.79e+03\nepoch:  80 loss: 1.79e+03\nepoch:  90 loss: 1.78e+03\n\n\nTo visualize the end result, we plot the predicted values over the whole space (the decision surface).\n\nx1 = np.arange(-2, 2, 0.05)\nx2 = np.arange(-2, 2, 0.05)\n\nx_test_np = np.array([(i, j) for i in x1 for j in x2])\ny_test_np = ((x_test_np[:,0] &lt; 0) & (x_test_np[:, 1] &gt; 0)).astype(x_test_np.dtype) + ((x_test_np[:,0] &gt; 0) & (x_test_np[:, 1] &lt; 0)).astype(x_test_np.dtype)\n\nx_test = torch.tensor(x_test_np, dtype=torch.float)\ny_test = torch.tensor(y_test_np)\n\n\nmodel.eval()\ny_pred = model(x_test)\n\ny_pred_np = y_pred.detach().numpy()\ny_pred_np = y_pred_np.reshape(x1.shape[0], x2.shape[0])\n\nseaborn_cols = sns.color_palette(\"tab10\")\ncols = [seaborn_cols[int(i)] for i in y]\n\ncustom_cmap = sns.diverging_palette(220, 50, s=70, l=70, as_cmap=True)\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(4, 4)\nax.contourf(x1, x2, y_pred_np, cmap=custom_cmap)\nax.scatter(x[0:100,0], x[0:100,1], c=cols[0:100])\n\n\n\n\n\n\n\n\n\nmodel.sequential[0].weight\n\nParameter containing:\ntensor([[-2.3491,  2.3204],\n        [ 2.4103, -2.3208]], requires_grad=True)\n\n\n\nmodel.sequential[0].bias\n\nParameter containing:\ntensor([-0.0632, -0.1064], requires_grad=True)\n\n\n\nmodel.sequential[2].weight\n\nParameter containing:\ntensor([[1.3308, 1.3007]], requires_grad=True)\n\n\n\nmodel.sequential[2].bias\n\nParameter containing:\ntensor([-3.4300], requires_grad=True)\n\n\nPlay around with different sizes of hidden_dim and see the difference!",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#mnist-example",
    "href": "lec-2/lecture-2.html#mnist-example",
    "title": "Lecture 2",
    "section": "MNIST example",
    "text": "MNIST example\nWe use torchvision.datasets to download the MNIST data.\n\n(mnist_train,\n mnist_test) = [torchvision.datasets.MNIST(root='data',\n                      train=train,\n                      download=True,\n                      transform=torchvision.transforms.ToTensor())\n                for train in [True, False]]\n\nplt.figure(figsize=(10, 10))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    train_image, label = mnist_train[i]\n    plt.imshow(train_image[0], cmap=plt.cm.binary)\n    plt.xlabel(label)\nplt.show()\n\n\n\n\n\n\n\n\nSet up our dataloaders.\n\ntrain_loader = DataLoader(dataset=mnist_train, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=mnist_test, batch_size=10000, shuffle=False)\n\nLet’s define our neural network for the MNIST classification problem.\n\nclass MNISTNet(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n\n        self.layers = nn.Sequential(\n            nn.Linear(28*28, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 10),\n            nn.Softmax()\n        )\n\n    def forward(self, x):\n        prob = self.layers(x)\n        return prob\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nInstantiate our model:\n\nmodel = MNISTNet()\n\nTrain our model:\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    l = 0\n    for x_batch, y_batch in train_loader:\n\n        x_batch = x_batch.reshape(x_batch.shape[0], 28*28)\n        y_batch = F.one_hot(y_batch, num_classes=10)\n        y_pred = model(x_batch)\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        l += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 2.59e+02\nepoch:  10 loss: 13.7\nepoch:  20 loss: 7.05\nepoch:  30 loss: 7.11\nepoch:  40 loss: 7.72\n\n\nCalculate our accuracy:\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\nx_batch = x_batch.reshape(x_batch.shape[0], 28 * 28)\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\nLet’s look at some interesting results (code adapted from here)\n\n# find interesting test images\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = range(8)\ninds2 = errors[:8]\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, 2 * num_cols, 2 * i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = mnist_test[n]\n    plt.imshow(image[0], cmap=plt.cm.binary)\n    plt.xlabel(label)\n    predicted_label = y_pred[n]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"truth={}, pred={}, score={:2.0f}%\".format(\n        label,\n        predicted_label,\n        100 * np.max(pred_array[n])),\n        color=color)\n\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n    thisplot = plt.bar(range(10), pred_array[n], color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(pred_array[n])\n    thisplot[predicted_label].set_color('red')\n    thisplot[label].set_color('blue')",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-2/lecture-2.html#heteroskedastic-regression",
    "href": "lec-2/lecture-2.html#heteroskedastic-regression",
    "title": "Lecture 2",
    "section": "Heteroskedastic regression",
    "text": "Heteroskedastic regression\nThis code is adapted from here.\n\n# Make data\n\nx_range = [-20, 60]  # test\nx_ranges = [[-20, 60]]\nns = [1000]\n\ndef load_dataset():\n    def s(x):  # std of noise\n        g = (x - x_range[0]) / (x_range[1] - x_range[0])\n        return 0.25 + g**2.0\n\n    x = []\n    y = []\n    for i in range(len(ns)):\n        n = ns[i]\n        xr = x_ranges[i]\n        x1 = np.linspace(xr[0], xr[1], n)\n        eps = np.random.randn(n) * s(x1)\n        y1 = (1 * np.sin(0.2 * x1) + 0.1 * x1) + eps\n        x = np.concatenate((x, x1))\n        y = np.concatenate((y, y1))\n    # print(x.shape)\n    x = x[..., np.newaxis]\n    n_test = 150\n    x_test = np.linspace(*x_range, num=n_test).astype(np.float32)\n    x_test = x_test[..., np.newaxis]\n    return y, x, x_test\n\ny, x, x_test = load_dataset()\n\nDefine neural network\n\nclass HetNet(nn.Module):\n\n    def __init__(self, input_dim, output_dim, hidden_dims, mean_dims, var_dims):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dims\n        self.mean_dims = mean_dims\n        self.var_dims = var_dims\n\n        # create backbone\n        current_dim = input_dim\n        self.layers = nn.ModuleList()\n        for i in range(len(hidden_dims)):\n            hdim = hidden_dims[i]\n            self.layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n\n        # create heads\n        core_dim = hidden_dims[-1]\n        current_dim = core_dim\n        self.mean_layers = nn.ModuleList()\n        for i in range(len(mean_dims)):\n            hdim = mean_dims[i]\n            self.mean_layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n        self.mean_layers.append(nn.Linear(current_dim, output_dim))\n\n        current_dim = core_dim\n        self.var_layers = nn.ModuleList()\n        for i in range(len(var_dims)):\n            hdim = var_dims[i]\n            self.var_layers.append(nn.Linear(current_dim, hdim))\n            current_dim = hdim\n        self.var_layers.append(nn.Linear(current_dim, output_dim))\n\n    def core_net(self, x):\n        for layer in self.layers:\n            x = F.relu(layer(x))\n        return x\n\n    def mean_net(self, x):\n        for layer in self.mean_layers[:-1]:\n            x = F.relu(layer(x))\n        x = self.mean_layers[-1](x)\n        return x\n\n    def var_net(self, x):\n        for layer in self.var_layers[:-1]:\n            x = F.relu(layer(x))\n        x = self.var_layers[-1](x)\n        return x\n\n    def forward(self, x):\n        mean = self.mean_net(self.core_net(x))\n        log_var = self.var_net(self.core_net(x))\n\n        return mean, log_var\n\n    def loss_fn(self, x, y):\n        mean, log_var = self.forward(x)\n        var = torch.exp(log_var)\n\n        loss = torch.pow(y-mean, 2) / var + log_var\n        out = loss.mean()\n\n        return out\n\nSet up data\n\nx_train = torch.tensor(x, dtype=torch.float)\ny_train = torch.tensor(y, dtype=torch.float)\ny_train = y_train.unsqueeze(-1)\n\ntrain_data = TensorDataset(x_train, y_train)\ntrain_loader = DataLoader(dataset=train_data, batch_size=10, shuffle=True)\n\nInitialize model\n\nhidden_dims = [50, 50]\nmean_dims = [20]\nvar_dims = [20]\nmodel = HetNet(input_dim=1, output_dim=1, hidden_dims=hidden_dims, mean_dims=mean_dims, var_dims=var_dims)\n\nTrain\n\nlr = 0.001\nepochs = 500\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n\n    l = 0\n    for x_batch, y_batch in train_loader:\n\n        loss = model.loss_fn(x_batch, y_batch)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n        l += loss.item()\n\n    if epoch % 50 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{l:.3}\")\n\nepoch:  0 loss: 1.27e+02\nepoch:  50 loss: 2.11\nepoch:  100 loss: -13.7\nepoch:  150 loss: -23.8\nepoch:  200 loss: -26.2\nepoch:  250 loss: -26.0\nepoch:  300 loss: -31.2\nepoch:  350 loss: -31.0\nepoch:  400 loss: -31.0\nepoch:  450 loss: -29.0\n\n\nPlot results\n\nmodel.eval()\nmean, log_var = model(x_train)\nsd = torch.exp(0.5 * log_var)\nmean_np = mean.detach().numpy()\nsd_np = sd.detach().numpy()\n\nfig, ax = plt.subplots(1, 1)\nfig.set_size_inches(6, 4)\nax.plot(x, y, '.', label=\"observed\")\nax.plot(x, mean_np, 'r-')\nax.plot(x, mean_np + 2 * sd_np, 'g-')\nax.plot(x, mean_np - 2 * sd_np, 'g-')",
    "crumbs": [
      "Home",
      "Lecture 2"
    ]
  },
  {
    "objectID": "lec-4/gpt.html",
    "href": "lec-4/gpt.html",
    "title": "GPT",
    "section": "",
    "text": "Here is a small GPT model from Andrej Karpathy.\n\n\ngpt.py\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\n# hyperparameters\nbatch_size = 64 # how many independent sequences will we process in parallel?\nblock_size = 256 # what is the maximum context length for predictions?\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\nif torch.backends.mps.is_available():\n    device = torch.device('mps')\nelse: \n    device = 'cpu'\neval_iters = 200\nn_embd = 384\nn_head = 6\nn_layer = 6\ndropout = 0.2\n# ------------\n\ntorch.manual_seed(1337)\n\n# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\n# Train and test splits\ndata = torch.tensor(encode(text), dtype=torch.long)\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n# data loading\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    x, y = x.to(device), y.to(device)\n    return x, y\n\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split)\n            logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\nclass Head(nn.Module):\n    \"\"\" one head of self-attention \"\"\"\n\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # input of size (batch, time-step, channels)\n        # output of size (batch, time-step, head size)\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,hs)\n        q = self.query(x) # (B,T,hs)\n        # compute attention scores (\"affinities\")\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -&gt; (B, T, T)\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n        wei = F.softmax(wei, dim=-1) # (B, T, T)\n        wei = self.dropout(wei)\n        # perform the weighted aggregation of the values\n        v = self.value(x) # (B,T,hs)\n        out = wei @ v # (B, T, T) @ (B, T, hs) -&gt; (B, T, hs)\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" multiple heads of self-attention in parallel \"\"\"\n\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        self.proj = nn.Linear(head_size * num_heads, n_embd)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        out = torch.cat([h(x) for h in self.heads], dim=-1)\n        out = self.dropout(self.proj(out))\n        return out\n\nclass FeedFoward(nn.Module):\n    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd),\n            nn.ReLU(),\n            nn.Linear(4 * n_embd, n_embd),\n            nn.Dropout(dropout),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformer block: communication followed by computation \"\"\"\n\n    def __init__(self, n_embd, n_head):\n        # n_embd: embedding dimension, n_head: the number of heads we'd like\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size)\n        self.ffwd = FeedFoward(n_embd)\n        self.ln1 = nn.LayerNorm(n_embd)\n        self.ln2 = nn.LayerNorm(n_embd)\n\n    def forward(self, x):\n        x = x + self.sa(self.ln1(x))\n        x = x + self.ffwd(self.ln2(x))\n        return x\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # each token directly reads off the logits for the next token from a lookup table\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n        self.lm_head = nn.Linear(n_embd, vocab_size)\n\n        # better init, not covered in the original GPT video, but important, will cover in followup video\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        # idx and targets are both (B,T) tensor of integers\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C)\n        x = self.ln_f(x) # (B,T,C)\n        logits = self.lm_head(x) # (B,T,vocab_size)\n\n        if targets is None:\n            loss = None\n        else:\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idx is (B, T) array of indices in the current context\n        for _ in range(max_new_tokens):\n            # crop idx to the last block_size tokens\n            idx_cond = idx[:, -block_size:]\n            # get the predictions\n            logits, loss = self(idx_cond)\n            # focus only on the last time step\n            logits = logits[:, -1, :] # becomes (B, C)\n            # apply softmax to get probabilities\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # sample from the distribution\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # append sampled index to the running sequence\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nmodel = GPTLanguageModel()\nm = model.to(device)\n# print the number of parameters in the model\nprint(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n\n# create a PyTorch optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n\nfor iter in range(max_iters):\n\n    # every once in a while evaluate the loss on train and val sets\n    if iter % eval_interval == 0 or iter == max_iters - 1:\n        losses = estimate_loss()\n        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n\n    # sample a batch of data\n    xb, yb = get_batch('train')\n\n    # evaluate the loss\n    logits, loss = model(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\n# generate from the model\ncontext = torch.zeros((1, 1), dtype=torch.long, device=device)\nprint(decode(m.generate(context, max_new_tokens=500)[0].tolist()))\n#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))",
    "crumbs": [
      "Home",
      "Lecture 4",
      "GPT"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html",
    "href": "lec-4/lecture-4.html",
    "title": "Lecture 4",
    "section": "",
    "text": "In this lecture, we study:\n\nattention\nbuilding a transformer\nfine-tuning language models\n\n\n\nNote: much of this code is from Andrej Karpathy’s excellent tutorials on building GPT from scratch:\n\nYoutube link\nGoogle Collab notebook link",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#overview",
    "href": "lec-4/lecture-4.html#overview",
    "title": "Lecture 4",
    "section": "",
    "text": "In this lecture, we study:\n\nattention\nbuilding a transformer\nfine-tuning language models\n\n\n\nNote: much of this code is from Andrej Karpathy’s excellent tutorials on building GPT from scratch:\n\nYoutube link\nGoogle Collab notebook link",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#attention",
    "href": "lec-4/lecture-4.html#attention",
    "title": "Lecture 4",
    "section": "Attention",
    "text": "Attention\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\n&lt;torch._C.Generator at 0x1170f8470&gt;\n\n\nWe are going to look at the Tiny Shakespeare dataset, which contains all the work of Shakespeare in a .txt file.\n\n# read it in to inspect it\nwith open('data/tinyshakespeare.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n\nprint(\"length of dataset in characters: \", len(text))\n\nlength of dataset in characters:  1115393\n\n\n\nprint(text[:400])\n\nFirst Citizen:\nBefore we proceed any further, hear me speak.\n\nAll:\nSpeak, speak.\n\nFirst Citizen:\nYou are all resolved rather to die than to famish?\n\nAll:\nResolved. resolved.\n\nFirst Citizen:\nFirst, you know Caius Marcius is chief enemy to the people.\n\nAll:\nWe know't, we know't.\n\nFirst Citizen:\nLet us kill him, and we'll have corn at our own price.\nIs't a verdict?\n\nAll:\nNo more talking on't; let it \n\n\n\n# here are all the unique characters that occur in this text\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\nprint(vocab_size)\n\n\n !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n65\n\n\nHow do we represent Tiny Shakespeare as numerical values?\n\n# create a mapping from characters to integers\nstoi = { ch:i for i,ch in enumerate(chars) } # string to integer\nitos = { i:ch for i,ch in enumerate(chars) } # integer to string\nencode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n\nprint(encode(\"hello there\"))\nprint(decode(encode(\"hello there\")))\n\n[46, 43, 50, 50, 53, 1, 58, 46, 43, 56, 43]\nhello there\n\n\n\n# let's now encode the entire text dataset and store it into a torch.Tensor\nimport torch \ndata = torch.tensor(encode(text), dtype=torch.float)\nprint(data.shape, data.dtype)\nprint(data[:200]) \n\ntorch.Size([1115393]) torch.float32\ntensor([18., 47., 56., 57., 58.,  1., 15., 47., 58., 47., 64., 43., 52., 10.,\n         0., 14., 43., 44., 53., 56., 43.,  1., 61., 43.,  1., 54., 56., 53.,\n        41., 43., 43., 42.,  1., 39., 52., 63.,  1., 44., 59., 56., 58., 46.,\n        43., 56.,  6.,  1., 46., 43., 39., 56.,  1., 51., 43.,  1., 57., 54.,\n        43., 39., 49.,  8.,  0.,  0., 13., 50., 50., 10.,  0., 31., 54., 43.,\n        39., 49.,  6.,  1., 57., 54., 43., 39., 49.,  8.,  0.,  0., 18., 47.,\n        56., 57., 58.,  1., 15., 47., 58., 47., 64., 43., 52., 10.,  0., 37.,\n        53., 59.,  1., 39., 56., 43.,  1., 39., 50., 50.,  1., 56., 43., 57.,\n        53., 50., 60., 43., 42.,  1., 56., 39., 58., 46., 43., 56.,  1., 58.,\n        53.,  1., 42., 47., 43.,  1., 58., 46., 39., 52.,  1., 58., 53.,  1.,\n        44., 39., 51., 47., 57., 46., 12.,  0.,  0., 13., 50., 50., 10.,  0.,\n        30., 43., 57., 53., 50., 60., 43., 42.,  8.,  1., 56., 43., 57., 53.,\n        50., 60., 43., 42.,  8.,  0.,  0., 18., 47., 56., 57., 58.,  1., 15.,\n        47., 58., 47., 64., 43., 52., 10.,  0., 18., 47., 56., 57., 58.,  6.,\n         1., 63., 53., 59.])\n\n\n\n# Let's now split up the data into train and validation sets\nn = int(0.9*len(data)) # first 90% will be train, rest val\ntrain_data = data[:n]\nval_data = data[n:]\n\n\nblock_size = 8\ntrain_data[:block_size+1]\n\ntensor([18., 47., 56., 57., 58.,  1., 15., 47., 58.])\n\n\nIn language modeling, we want to predict the next word in a sequence. For a given block, what are we predicting?\n\nx = train_data[:block_size]\ny = train_data[1:block_size+1]\nfor t in range(block_size):\n    context = x[:t+1]\n    target = y[t]\n    print(f\"when input is {context} the target: {target}\")\n\nwhen input is tensor([18.]) the target: 47.0\nwhen input is tensor([18., 47.]) the target: 56.0\nwhen input is tensor([18., 47., 56.]) the target: 57.0\nwhen input is tensor([18., 47., 56., 57.]) the target: 58.0\nwhen input is tensor([18., 47., 56., 57., 58.]) the target: 1.0\nwhen input is tensor([18., 47., 56., 57., 58.,  1.]) the target: 15.0\nwhen input is tensor([18., 47., 56., 57., 58.,  1., 15.]) the target: 47.0\nwhen input is tensor([18., 47., 56., 57., 58.,  1., 15., 47.]) the target: 58.0\n\n\n\nbatch_size = 4 # how many independent sequences will we process in parallel?\nblock_size = 8 # what is the maximum context length for predictions?\n\ndef get_batch(split):\n    # generate a small batch of data of inputs x and targets y\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,)) # select a random integer from len(data) - block_size\n    x = torch.stack([data[i:i+block_size] for i in ix])\n    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n    return x, y\n\nxb, yb = get_batch('train')\nprint('inputs:')\nprint(xb.shape)\nprint(xb)\nprint('targets:')\nprint(yb.shape)\nprint(yb)\n\nprint('----')\n\nfor b in range(2): # batch dimension\n    for t in range(block_size): # time dimension\n        context = xb[b, :t+1]\n        target = yb[b,t]\n        print(f\"when input is {context.tolist()} the target: {target}\")\n\ninputs:\ntorch.Size([4, 8])\ntensor([[53., 59.,  6.,  1., 58., 56., 47., 40.],\n        [49., 43., 43., 54.,  1., 47., 58.,  1.],\n        [13., 52., 45., 43., 50., 53.,  8.,  0.],\n        [ 1., 39.,  1., 46., 53., 59., 57., 43.]])\ntargets:\ntorch.Size([4, 8])\ntensor([[59.,  6.,  1., 58., 56., 47., 40., 59.],\n        [43., 43., 54.,  1., 47., 58.,  1., 58.],\n        [52., 45., 43., 50., 53.,  8.,  0., 26.],\n        [39.,  1., 46., 53., 59., 57., 43.,  0.]])\n----\nwhen input is [53.0] the target: 59.0\nwhen input is [53.0, 59.0] the target: 6.0\nwhen input is [53.0, 59.0, 6.0] the target: 1.0\nwhen input is [53.0, 59.0, 6.0, 1.0] the target: 58.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0] the target: 56.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0] the target: 47.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0, 47.0] the target: 40.0\nwhen input is [53.0, 59.0, 6.0, 1.0, 58.0, 56.0, 47.0, 40.0] the target: 59.0\nwhen input is [49.0] the target: 43.0\nwhen input is [49.0, 43.0] the target: 43.0\nwhen input is [49.0, 43.0, 43.0] the target: 54.0\nwhen input is [49.0, 43.0, 43.0, 54.0] the target: 1.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0] the target: 47.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0] the target: 58.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0, 58.0] the target: 1.0\nwhen input is [49.0, 43.0, 43.0, 54.0, 1.0, 47.0, 58.0, 1.0] the target: 58.0\n\n\nAs discussed in class, attention is the weighted average of previous word embeddings:\nh_T = \\sum_{t=1}^T \\alpha_t x_t\nLet’s first write up the average (instead of weighted average). h_T = \\frac{1}{T}\\sum_{t=1}^T x_t\n\n# consider the following toy example:\n\nB,T,C = 4,8,2 # batch, time, channels\nx = torch.randn(B,T,C)\nx.shape\n\ntorch.Size([4, 8, 2])\n\n\n\n# We want x[b,t] = mean_{i&lt;=t} x[b,i]\nxrep = torch.zeros((B,T,C)) # this is our new representation, h\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t,C)\n        xrep[b,t] = torch.mean(xprev, axis=0)\n\n\n# version 2: using matrix multiply for a weighted aggregation\nwei = torch.tril(torch.ones(T, T))\nwei = wei / wei.sum(1, keepdim=True)\nxrep2 = wei @ x # (B, T, T) @ (B, T, C) ----&gt; (B, T, C)\ntorch.allclose(xrep, xrep2)\n\nTrue\n\n\n\n# version 3: use Softmax\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf'))\nwei = F.softmax(wei, dim=-1)\nxrep3 = wei @ x\ntorch.allclose(xrep, xrep3)\n\nTrue\n\n\n\nwei\n\ntensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n\n\n\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (channels is the embedding size)\nx = torch.randn(B,T,C)\n\n# let's see a single Head perform self-attention\nhead_size = 16\nkey = nn.Linear(C, head_size, bias=False) # W_K matrix\nquery = nn.Linear(C, head_size, bias=False) # W_Q matrix\nvalue = nn.Linear(C, head_size, bias=False) # W_V matrix\nk = key(x)   # (B, T, 16) W_K x\nq = query(x) # (B, T, 16) W_Q x\nwei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---&gt; (B, T, T)   \n\ntril = torch.tril(torch.ones(T, T)) \n#wei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # mask the weight matrix so that we can't pay attention to future tokens\nwei = F.softmax(wei, dim=-1)\n\nv = value(x)\nout = wei @ v\n#out = wei @ x\n\nout.shape\n\ntorch.Size([4, 8, 16])\n\n\nFor regularization and to stabilize training, layer norm is used (instead of batch norm). Layer norm is the same as batch norm, except averages are taken over the sequence of tokens, not the batches.\n\nclass LayerNorm1d:\n\n  def __init__(self, dim, eps=1e-5, momentum=0.1):\n    self.eps = eps\n    self.gamma = torch.ones(dim)\n    self.beta = torch.zeros(dim)\n\n  def __call__(self, x):\n    # calculate the forward pass\n    xmean = x.mean(1, keepdim=True) # sequence mean (in batch norm, axis=0 instead)\n    xvar = x.var(1, keepdim=True) # sequence variance (in batch norm, axis=0 instead)\n    xhat = (x - xmean) / torch.sqrt(xvar + self.eps) # normalize to unit variance\n    self.out = self.gamma * xhat + self.beta\n    return self.out\n\n  def parameters(self):\n    return [self.gamma, self.beta]\n\ntorch.manual_seed(1337)\nmodule = LayerNorm1d(100)\nx = torch.randn(32, 100) # batch size 32 of 100-dimensional vectors\nx = module(x)\nx.shape\n\ntorch.Size([32, 100])",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#transformer",
    "href": "lec-4/lecture-4.html#transformer",
    "title": "Lecture 4",
    "section": "Transformer",
    "text": "Transformer\nHere is a small GPT model from Andrej Karpathy.",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#tokenization",
    "href": "lec-4/lecture-4.html#tokenization",
    "title": "Lecture 4",
    "section": "Tokenization",
    "text": "Tokenization\nTokenization is the process of turning text into discrete units (called tokens). We saw we could map letters to numbers as we did with Tiny Shakespeare. However, this can be very inefficient.\nMany modern tokenizers use an algorithm such as byte pair encoding that greedily merges commonly occurring sub-strings based on their frequency.\nUnderstanding Deep Learning, Figure 12.8\n\n\n\nUnderstanding Deep Learning\n\n\nTikTokenizer is a nice tool to see how LLMs encode text into tokens:\nTokenization is at the heart of much weirdness of LLMs. \n\n127 + 456 = 583\n\nApple.\nI have an apple.\napple.\nApple.\n\nfor i in range(1, 101):\n    if i % 2 == 0:\n        print(\"hello world\")",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#finetuning",
    "href": "lec-4/lecture-4.html#finetuning",
    "title": "Lecture 4",
    "section": "Finetuning",
    "text": "Finetuning\nWe now look at finetuning a language representation model called DistilBERT (Sanh et al. 2019) [arXiv].\nDistilBERT is a 40% smaller, distilled version of BERT, which retains 97% of the original BERT model capabilities.\nWe can obtain pretrained models from Hugging Face.\nWe need to install Hugging Face’s transformers package:\npip install transformers\n\nfrom transformers import AutoTokenizer, AutoModel\nfrom torch import nn\nimport torch\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nIncluded also is the model tokenizer.\n\ntokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\nbert_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n\nexample_text = \"Hello world\"\n\ntoken_res = tokenizer(example_text, return_tensors='pt', max_length=10, padding='max_length') # pt is for pytorch tensors\n\nprint(token_res['input_ids']) ## vector of token IDs\nprint(token_res['attention_mask']) ## vector of 0/1 to indicate real tokens vs padding tokens\n\nout_text = tokenizer.decode(token_res['input_ids'][0])\nprint(out_text)\n\ntensor([[ 101, 7592, 2088,  102,    0,    0,    0,    0,    0,    0]])\ntensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]])\n[CLS] hello world [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n\n\n\nbert_model.config.hidden_size\n\n768\n\n\n\nWine dataset\nWe will finetune a wine dataset to predict price based on the wine description.\nThe dataset has 120K wines.\n\nwine_df = pd.read_csv(\"data/wines.csv\")\n\n## keep only wines whose price is not NaN\nwine_df = wine_df[wine_df['price'].notna()]\n\nprint(wine_df.shape)\n\n## key variables: price, description\nfor i in range(3):\n    print(\"Description: \", wine_df['description'].iloc[i])\n    print(\"Price: \", wine_df['price'].iloc[i])\n\n## find the wine with the highest price\nmax_price_idx = wine_df['price'].argmax()\nprint(\"Most expensive wine: \", wine_df['description'].iloc[max_price_idx])\nprint(\"Price: \", wine_df['price'].iloc[max_price_idx])\n\n## make box-plot of prices\nplt.boxplot(np.log(wine_df['price']))\n\n(120975, 14)\nDescription:  This is ripe and fruity, a wine that is smooth while still structured. Firm tannins are filled out with juicy red berry fruits and freshened with acidity. It's  already drinkable, although it will certainly be better from 2016.\nPrice:  15.0\nDescription:  Tart and snappy, the flavors of lime flesh and rind dominate. Some green pineapple pokes through, with crisp acidity underscoring the flavors. The wine was all stainless-steel fermented.\nPrice:  14.0\nDescription:  Pineapple rind, lemon pith and orange blossom start off the aromas. The palate is a bit more opulent, with notes of honey-drizzled guava and mango giving way to a slightly astringent, semidry finish.\nPrice:  13.0\nMost expensive wine:  This ripe wine shows plenty of blackberry fruits balanced well with some dry tannins. It is fresh, juicy with plenty of acidity, For a light vintage, it's perfumed, full of fresh flavors and will be ready to drink from 2017.\nPrice:  3300.0\n\n\n{'whiskers': [&lt;matplotlib.lines.Line2D at 0x34ea5d6d0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x34e9eb4d0&gt;],\n 'caps': [&lt;matplotlib.lines.Line2D at 0x34e9e91d0&gt;,\n  &lt;matplotlib.lines.Line2D at 0x34e9e8e10&gt;],\n 'boxes': [&lt;matplotlib.lines.Line2D at 0x34ee4f890&gt;],\n 'medians': [&lt;matplotlib.lines.Line2D at 0x34e9bfed0&gt;],\n 'fliers': [&lt;matplotlib.lines.Line2D at 0x34e9bd6d0&gt;],\n 'means': []}\n\n\n\n\n\n\n\n\n\n\nclass textClassDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n        self.tokens = tokenizer(df['description'].tolist(), return_tensors='pt', max_length=self.max_len, \n                                      padding='max_length', truncation=True) \n        self.price = torch.tensor(df['price'].to_numpy(), dtype=torch.float)\n\n    def __len__(self):\n        return len(self.price)\n    \n    def __getitem__(self, idx):\n        \n        input_ids = self.tokens['input_ids'][idx]\n        attention_mask = self.tokens['attention_mask'][idx]\n        price = self.price[idx]\n\n        return input_ids, attention_mask, price\n\n\ndataset = textClassDataset(wine_df, tokenizer, 128)\n\n## split into train and test datasets\ntrain_size = int(0.5 * len(dataset))\ntest_size = len(dataset) - train_size\n\ntrain_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n\nprint(\"training data size: \" + str(len(train_dataset)))\n\n## create dataloaders\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True)\n\ntraining data size: 60487\n\n\n\nbatch = next(iter(train_dataloader))\n\n\nclass BertRegressor(nn.Module):\n    def __init__(self):\n        super(BertRegressor, self).__init__()\n        self.bert = bert_model\n\n        ## for distilbert-base-uncased, hidden_size is 768\n        self.layer1 = nn.Linear(self.bert.config.hidden_size, 1)\n\n    def forward(self, input_ids, attention_mask):\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = bert_outputs[0][:, 0, :] # [CLS] token1 token2 ... this grabs [CLS] token\n\n        x = self.layer1(pooled_output)\n        x = x.squeeze(1)\n        return x\n    \n\n\nmodel = BertRegressor()\n\n\n## We can freeze bert parameters so that we only update the\n## prediction head\n# for param in model.bert.parameters():\n#     param.requires_grad = False\n\nloss_fn = nn.MSELoss()\noptimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n                             lr=1e-4)\n\nnum_epochs = 1\n\nmodel.train()\n\n\nTraining\n\n1 epoch of training takes about 20 mins.\nYou can skip the training to directly load from the saved model parameter file.\n\n\nit = 0\n\nfor epoch in range(num_epochs):\n\n    for batch in train_dataloader:\n\n        input_ids = batch[0]\n        attention_mask = batch[1]\n        logprice = np.log(batch[2])\n\n        pred = model(input_ids, attention_mask)\n        loss = loss_fn(pred, logprice)\n        loss.backward()\n        optimizer.step()\n\n        optimizer.zero_grad()\n        \n        it = it + 1\n        if (it % 100 == 0):\n            print(\"epoch: \", epoch, \"sgd iter: \" + str(it))\n    print(\"Epoch: {}, Loss: {}\".format(epoch, loss.item()))\n\n\n## save model parameters\ntorch.save(model.state_dict(), \"fine-tuned-distilbert.pt\")\n\n\n## load model\nmodel.load_state_dict(torch.load(\"fine-tuned-distilbert.pt\"))\n\n&lt;All keys matched successfully&gt;\n\n\n\n## calculate testing error\n## takes about 2 minutes to run\n\nmodel.eval()\nmse = 0\n\nn_test = 600\n\ny_test = np.array([np.log(test_dataset[i][2]) for i in range(n_test)])\n\nfor i in range(n_test):\n    pred = model(test_dataset[i][0].unsqueeze(0), test_dataset[i][1].unsqueeze(0))\n    mse = mse + (pred - y_test[i])**2\n\nmse = mse / n_test\n\nprint(\"MSE:\", mse.item(), \"  Test R-squared:\", 1 - mse.item() / np.var(y_test))\n\n/var/folders/f0/m7l23y8s7p3_0x04b3td9nyjr2hyc8/T/ipykernel_6343/1264298161.py:9: DeprecationWarning: __array_wrap__ must accept context and return_scalar arguments (positionally) in the future. (Deprecated NumPy 2.0)\n  y_test = np.array([np.log(test_dataset[i][2]) for i in range(n_test)])\n\n\nMSE: 0.21289405226707458   Test R-squared: 0.4635502\n\n\n\n## \n\nmy_reviews = [\"This white is both sour and bitter; it has a funny smell\",\n                \"the most amazing wine I have ever tasted\",\n                \"not bad at all; I would buy it again\",\n                \"actually quite bad; avoid if possible\",\n                \"great red and pretty cheap\",\n                \"great red but overpriced\",\n                \"great red and great price\"]\n\nfor my_review in my_reviews:\n\n    token_res = tokenizer(my_review, return_tensors='pt')\n\n    pred = model(token_res['input_ids'], token_res['attention_mask'])\n    \n    print(\"My Description:\", my_review)\n    print(\"Predicted price: \", torch.exp(pred).item(), '\\n')\n\nMy Description: This white is both sour and bitter; it has a funny smell\nPredicted price:  15.726286888122559 \n\nMy Description: the most amazing wine I have ever tasted\nPredicted price:  47.7934684753418 \n\nMy Description: not bad at all; I would buy it again\nPredicted price:  12.506088256835938 \n\nMy Description: actually quite bad; avoid if possible\nPredicted price:  14.870586395263672 \n\nMy Description: great red and pretty cheap\nPredicted price:  12.206925392150879 \n\nMy Description: great red but overpriced\nPredicted price:  24.468101501464844 \n\nMy Description: great red and great price\nPredicted price:  12.49232006072998",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-4/lecture-4.html#resources",
    "href": "lec-4/lecture-4.html#resources",
    "title": "Lecture 4",
    "section": "Resources",
    "text": "Resources\n\nText Classification with BERT\nHugging Face Tutorial",
    "crumbs": [
      "Home",
      "Lecture 4"
    ]
  },
  {
    "objectID": "lec-3/lecture-3.html",
    "href": "lec-3/lecture-3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "This notebook is adapted from ISLP.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\n\nFirst, download CIFAR100 data.\n\nbatch_size = 128\n\n(cifar_train,\n cifar_test) = [torchvision.datasets.CIFAR100(root=\"data\",\n                         train=train,\n                         download=True)\n             for train in [True, False]]\n\nlabel_names = cifar_test.classes\n\ntransform = torchvision.transforms.ToTensor()\ncifar_train_X = torch.stack([transform(x) for x in\n                            cifar_train.data])\ncifar_test_X = torch.stack([transform(x) for x in\n                            cifar_test.data])\ncifar_train = TensorDataset(cifar_train_X,\n                            torch.tensor(cifar_train.targets))\ncifar_test = TensorDataset(cifar_test_X,\n                            torch.tensor(cifar_test.targets))\n\ntrain_loader = DataLoader(cifar_train, batch_size=batch_size)\ntest_loader = DataLoader(cifar_test, batch_size=1000)\n\n\nfig, axes = plt.subplots(5, 5, figsize=(10,10))\nrng = np.random.default_rng(4)\nindices = rng.choice(np.arange(len(cifar_train)), 25,\n                     replace=False).reshape((5,5))\nfor i in range(5):\n    for j in range(5):\n        idx = indices[i,j]\n        axes[i,j].imshow(np.transpose(cifar_train[idx][0],\n                                      [1,2,0]),\n                                      interpolation=None)\n        axes[i,j].set_xticks([])\n        axes[i,j].set_yticks([])\n\n\n# note that torch requires image tensors to be [N, C, H, W]\n# N: samples, C: channels, H: height, W: width\n\n\n\n\n\n\n\n\nLet’s set up our convolutional neural network for CIFAR100.\n\n\nclass BuildingBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n\n        super(BuildingBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=(3,3),\n                              padding='same')\n        self.activation = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n\n    def forward(self, x):\n        return self.pool(self.activation(self.conv(x)))\n\n# If the syntax *expression appears in the function call,\n# expression must evaluate to an iterable. Elements from\n# this iterable are treated as if they were additional\n# positional arguments; if there are positional arguments\n# x1, ..., xN, and expression evaluates to a sequence y1, ..., yM,\n# this is equivalent to a call with M+N positional arguments x1, ..., xN, y1, ..., yM.\n\nclass CIFARModel(nn.Module):\n\n    def __init__(self):\n        super(CIFARModel, self).__init__()\n        sizes = [(3,32),\n                 (32,64),\n                 (64,128),\n                 (128,256)]\n        self.conv = nn.Sequential(*[BuildingBlock(in_, out_)\n                                    for in_, out_ in sizes])  # A single star * unpacks a sequence into positional arguments\n\n        self.output = nn.Sequential(nn.Dropout(0.5),\n                                    nn.Linear(2*2*256, 512),\n                                    nn.ReLU(),\n                                    nn.Linear(512, 100),\n                                    nn.Softmax())\n    def forward(self, x):\n        val = self.conv(x)\n        val = torch.flatten(val, start_dim=1)\n        return self.output(val)\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nLet’s instantiate the CIFAR100 model. Note: training the CIFAR100 model takes awhile (43 minutes)\n\nmodel = CIFARModel()\n\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    epoch_loss = 0\n\n    for x_batch, y_batch in train_loader:\n\n        y_pred = model(x_batch)\n        y_batch = F.one_hot(y_batch, num_classes=100)\n\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_loss += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n\n\nepoch:  0 loss: 1.61e+03\nepoch:  10 loss: 8.59e+02\nepoch:  20 loss: 6.62e+02\nepoch:  30 loss: 5.58e+02\nepoch:  40 loss: 4.91e+02\n\n\nWe save the trained CIFAR100 model so we can access it later.\n\nfilename = 'cifar100-model'\n\n\ntorch.save(model.state_dict(), filename)\n\nLoad model:\n\nmodel.load_state_dict(torch.load(filename))\n\n&lt;All keys matched successfully&gt;\n\n\nLet’s calculate the accuracy of the model on a subset of the data.\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n\n\n\nacc\n\ntensor(0.4410)\n\n\nA random classifier for 100 classes would get 1%. So not bad… but better models can get up to 90s.\nLet’s look at some examples the model got wrong:\n\ncorrect =  torch.where(y_pred == y_batch)[0]\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = np.random.choice(correct.numpy(), size = 8)\ninds2 = np.random.choice(errors.numpy(), size = 8)\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, num_cols, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = x_batch[n], y_batch[n]\n    label = label_names[label]\n\n    plt.imshow(np.transpose(image,\n                        [1, 2, 0]),\n           interpolation=None)\n    plt.xlabel(label)\n    predicted_label = label_names[y_pred[n]]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"true={}, pred={}\".format(\n        label,\n        predicted_label),\n        color=color)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\n\nThe torchvision.models package provides pre-trained models like ResNet-50 (a ResNet model with 50 layers). Here, we get the predictions from ResNet-50 for pictures of the professors’ dogs. (Note this uses code from https://github.com/intro-stat-learning/ISLP_labs/blob/stable/Ch10-deeplearning-lab.ipynb)\n\nfrom torchvision.models import (resnet50, ResNet50_Weights)\nfrom torchvision.transforms import (Resize,\n                                    Normalize,\n                                    CenterCrop,\n                                    ToTensor)\nfrom torchvision.io import read_image\nfrom glob import glob\nimport json\nimport pandas as pd\n\n\nresnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n\n# We need to normalize the images for use for ResNet-50\n\nresize = Resize((232,232), antialias=True)\ncrop = CenterCrop(224)\nnormalize = Normalize([0.485,0.456,0.406],\n                      [0.229,0.224,0.225])\nimgfiles = sorted([f for f in glob('dogs/*')])\nimgs = torch.stack([torch.div(crop(resize(read_image(f))), 255)\n                    for f in imgfiles])\nimgs_no_norm = imgs\nimgs = normalize(imgs)\nimgs.size()\n\ntorch.Size([3, 3, 224, 224])\n\n\n\nplt.figure(figsize=(2 * len(imgfiles), 2))\n\nfor i, imgfile in enumerate(imgfiles):\n    plt.subplot(1, len(imgfiles), i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n\n    plt.imshow(np.transpose(imgs_no_norm[i].numpy(),\n                        [1, 2, 0]),\n           interpolation=None)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\nresnet_model.eval()\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\nimg_preds = resnet_model(imgs)\n\n\nimg_preds.shape\n\ntorch.Size([3, 1000])\n\n\n\n# apply softmax to the outputs of the ResNet\nimg_probs = np.exp(np.asarray(img_preds.detach()))\nimg_probs /= img_probs.sum(1)[:,None]\n\n\n# get class labels -- this is available at https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nlabs = json.load(open('imagenet_class_index.json'))\nclass_labels = pd.DataFrame([(int(k), v[1]) for k, v in \n                           labs.items()],\n                           columns=['idx', 'label'])\nclass_labels = class_labels.set_index('idx')\nclass_labels = class_labels.sort_index()\n\n\nfor i, imgfile in enumerate(imgfiles):\n    img_df = class_labels.copy()\n    img_df['prob'] = img_probs[i]\n    img_df = img_df.sort_values(by='prob', ascending=False)[:3]\n    print(f'Image: {imgfile}')\n    print(img_df.reset_index().drop(columns=['idx']))\n\nImage: dogs/bonnie.JPG\n               label      prob\n0  Brabancon_griffon  0.243078\n1      affenpinscher  0.013340\n2           Pekinese  0.004563\nImage: dogs/maya.jpeg\n                label      prob\n0            malinois  0.221570\n1     German_shepherd  0.102747\n2  Norwegian_elkhound  0.007779\nImage: dogs/milo.jpeg\n              label      prob\n0        toy_poodle  0.302865\n1  miniature_poodle  0.133538\n2          Shih-Tzu  0.024314",
    "crumbs": [
      "Home",
      "Lecture 3"
    ]
  },
  {
    "objectID": "lec-3/lecture-3.html#convolutional-neural-networks",
    "href": "lec-3/lecture-3.html#convolutional-neural-networks",
    "title": "Lecture 3",
    "section": "",
    "text": "This notebook is adapted from ISLP.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torchvision\n\nFirst, download CIFAR100 data.\n\nbatch_size = 128\n\n(cifar_train,\n cifar_test) = [torchvision.datasets.CIFAR100(root=\"data\",\n                         train=train,\n                         download=True)\n             for train in [True, False]]\n\nlabel_names = cifar_test.classes\n\ntransform = torchvision.transforms.ToTensor()\ncifar_train_X = torch.stack([transform(x) for x in\n                            cifar_train.data])\ncifar_test_X = torch.stack([transform(x) for x in\n                            cifar_test.data])\ncifar_train = TensorDataset(cifar_train_X,\n                            torch.tensor(cifar_train.targets))\ncifar_test = TensorDataset(cifar_test_X,\n                            torch.tensor(cifar_test.targets))\n\ntrain_loader = DataLoader(cifar_train, batch_size=batch_size)\ntest_loader = DataLoader(cifar_test, batch_size=1000)\n\n\nfig, axes = plt.subplots(5, 5, figsize=(10,10))\nrng = np.random.default_rng(4)\nindices = rng.choice(np.arange(len(cifar_train)), 25,\n                     replace=False).reshape((5,5))\nfor i in range(5):\n    for j in range(5):\n        idx = indices[i,j]\n        axes[i,j].imshow(np.transpose(cifar_train[idx][0],\n                                      [1,2,0]),\n                                      interpolation=None)\n        axes[i,j].set_xticks([])\n        axes[i,j].set_yticks([])\n\n\n# note that torch requires image tensors to be [N, C, H, W]\n# N: samples, C: channels, H: height, W: width\n\n\n\n\n\n\n\n\nLet’s set up our convolutional neural network for CIFAR100.\n\n\nclass BuildingBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n\n        super(BuildingBlock, self).__init__()\n        self.conv = nn.Conv2d(in_channels=in_channels,\n                              out_channels=out_channels,\n                              kernel_size=(3,3),\n                              padding='same')\n        self.activation = nn.ReLU()\n        self.pool = nn.MaxPool2d(kernel_size=(2,2))\n\n    def forward(self, x):\n        return self.pool(self.activation(self.conv(x)))\n\n# If the syntax *expression appears in the function call,\n# expression must evaluate to an iterable. Elements from\n# this iterable are treated as if they were additional\n# positional arguments; if there are positional arguments\n# x1, ..., xN, and expression evaluates to a sequence y1, ..., yM,\n# this is equivalent to a call with M+N positional arguments x1, ..., xN, y1, ..., yM.\n\nclass CIFARModel(nn.Module):\n\n    def __init__(self):\n        super(CIFARModel, self).__init__()\n        sizes = [(3,32),\n                 (32,64),\n                 (64,128),\n                 (128,256)]\n        self.conv = nn.Sequential(*[BuildingBlock(in_, out_)\n                                    for in_, out_ in sizes])  # A single star * unpacks a sequence into positional arguments\n\n        self.output = nn.Sequential(nn.Dropout(0.5),\n                                    nn.Linear(2*2*256, 512),\n                                    nn.ReLU(),\n                                    nn.Linear(512, 100),\n                                    nn.Softmax())\n    def forward(self, x):\n        val = self.conv(x)\n        val = torch.flatten(val, start_dim=1)\n        return self.output(val)\n\n    def loss_fn(self, y, y_pred):\n        log_pred = torch.log(y_pred + 1e-8)\n        loss = -(log_pred * y).sum(1).mean()\n        return loss\n\nLet’s instantiate the CIFAR100 model. Note: training the CIFAR100 model takes awhile (43 minutes)\n\nmodel = CIFARModel()\n\n\nlr = 0.001\nepochs = 50\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\n\n# set model to training mode\nmodel.train()\n\nfor epoch in range(epochs):\n    epoch_loss = 0\n\n    for x_batch, y_batch in train_loader:\n\n        y_pred = model(x_batch)\n        y_batch = F.one_hot(y_batch, num_classes=100)\n\n        loss = model.loss_fn(y_batch, y_pred)\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        epoch_loss += loss.item()\n\n    if epoch % 10 == 0:\n        print('epoch: ', epoch, 'loss:', f\"{epoch_loss:.3}\")\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.11/site-packages/torch/nn/modules/container.py:217: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  input = module(input)\n\n\nepoch:  0 loss: 1.61e+03\nepoch:  10 loss: 8.59e+02\nepoch:  20 loss: 6.62e+02\nepoch:  30 loss: 5.58e+02\nepoch:  40 loss: 4.91e+02\n\n\nWe save the trained CIFAR100 model so we can access it later.\n\nfilename = 'cifar100-model'\n\n\ntorch.save(model.state_dict(), filename)\n\nLoad model:\n\nmodel.load_state_dict(torch.load(filename))\n\n&lt;All keys matched successfully&gt;\n\n\nLet’s calculate the accuracy of the model on a subset of the data.\n\nmodel.eval()\nx_batch, y_batch = next(iter(test_loader))\ny_pred_array = model(x_batch)\ny_pred = torch.argmax(y_pred_array, axis=1)\nacc = (y_pred == y_batch).sum()\nacc = acc / len(y_pred)\n\n/Users/gm845/anaconda3/envs/msds534/lib/python3.13/site-packages/torch/nn/modules/module.py:1773: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  return self._call_impl(*args, **kwargs)\n\n\n\nacc\n\ntensor(0.4410)\n\n\nA random classifier for 100 classes would get 1%. So not bad… but better models can get up to 90s.\nLet’s look at some examples the model got wrong:\n\ncorrect =  torch.where(y_pred == y_batch)[0]\nerrors = torch.where(y_pred != y_batch)[0]\ninds1 = np.random.choice(correct.numpy(), size = 8)\ninds2 = np.random.choice(errors.numpy(), size = 8)\ninds = np.concatenate((inds1, inds2))\n\npred_array = y_pred_array.detach().numpy()\n\nnum_rows = 4\nnum_cols = 4\nnum_images = num_rows * num_cols\nplt.figure(figsize=(2 * 2 * num_cols, 2 * num_rows))\n\nfor i in range(num_images):\n    n = inds[i]\n    plt.subplot(num_rows, num_cols, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    image, label = x_batch[n], y_batch[n]\n    label = label_names[label]\n\n    plt.imshow(np.transpose(image,\n                        [1, 2, 0]),\n           interpolation=None)\n    plt.xlabel(label)\n    predicted_label = label_names[y_pred[n]]\n    if predicted_label == label:\n        color = 'blue'\n    else:\n        color = 'red'\n    plt.xlabel(\"true={}, pred={}\".format(\n        label,\n        predicted_label),\n        color=color)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\n\nThe torchvision.models package provides pre-trained models like ResNet-50 (a ResNet model with 50 layers). Here, we get the predictions from ResNet-50 for pictures of the professors’ dogs. (Note this uses code from https://github.com/intro-stat-learning/ISLP_labs/blob/stable/Ch10-deeplearning-lab.ipynb)\n\nfrom torchvision.models import (resnet50, ResNet50_Weights)\nfrom torchvision.transforms import (Resize,\n                                    Normalize,\n                                    CenterCrop,\n                                    ToTensor)\nfrom torchvision.io import read_image\nfrom glob import glob\nimport json\nimport pandas as pd\n\n\nresnet_model = resnet50(weights=ResNet50_Weights.DEFAULT)\n\n\n# We need to normalize the images for use for ResNet-50\n\nresize = Resize((232,232), antialias=True)\ncrop = CenterCrop(224)\nnormalize = Normalize([0.485,0.456,0.406],\n                      [0.229,0.224,0.225])\nimgfiles = sorted([f for f in glob('dogs/*')])\nimgs = torch.stack([torch.div(crop(resize(read_image(f))), 255)\n                    for f in imgfiles])\nimgs_no_norm = imgs\nimgs = normalize(imgs)\nimgs.size()\n\ntorch.Size([3, 3, 224, 224])\n\n\n\nplt.figure(figsize=(2 * len(imgfiles), 2))\n\nfor i, imgfile in enumerate(imgfiles):\n    plt.subplot(1, len(imgfiles), i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n\n    plt.imshow(np.transpose(imgs_no_norm[i].numpy(),\n                        [1, 2, 0]),\n           interpolation=None)\n    \n\nplt.tight_layout() \n\n\n\n\n\n\n\n\n\nresnet_model.eval()\n\nResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n    )\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)\n\n\n\nimg_preds = resnet_model(imgs)\n\n\nimg_preds.shape\n\ntorch.Size([3, 1000])\n\n\n\n# apply softmax to the outputs of the ResNet\nimg_probs = np.exp(np.asarray(img_preds.detach()))\nimg_probs /= img_probs.sum(1)[:,None]\n\n\n# get class labels -- this is available at https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\nlabs = json.load(open('imagenet_class_index.json'))\nclass_labels = pd.DataFrame([(int(k), v[1]) for k, v in \n                           labs.items()],\n                           columns=['idx', 'label'])\nclass_labels = class_labels.set_index('idx')\nclass_labels = class_labels.sort_index()\n\n\nfor i, imgfile in enumerate(imgfiles):\n    img_df = class_labels.copy()\n    img_df['prob'] = img_probs[i]\n    img_df = img_df.sort_values(by='prob', ascending=False)[:3]\n    print(f'Image: {imgfile}')\n    print(img_df.reset_index().drop(columns=['idx']))\n\nImage: dogs/bonnie.JPG\n               label      prob\n0  Brabancon_griffon  0.243078\n1      affenpinscher  0.013340\n2           Pekinese  0.004563\nImage: dogs/maya.jpeg\n                label      prob\n0            malinois  0.221570\n1     German_shepherd  0.102747\n2  Norwegian_elkhound  0.007779\nImage: dogs/milo.jpeg\n              label      prob\n0        toy_poodle  0.302865\n1  miniature_poodle  0.133538\n2          Shih-Tzu  0.024314",
    "crumbs": [
      "Home",
      "Lecture 3"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#fall-2025",
    "href": "index.html#fall-2025",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "",
    "text": "Instructor: Gemma Moran\nInstructor website: link"
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "Getting started",
    "text": "Getting started\nSee here for details about how to get started with python, VSCode and PyTorch."
  },
  {
    "objectID": "index.html#lectures",
    "href": "index.html#lectures",
    "title": "Rutgers MSDS-534 - Statistical Learning",
    "section": "Lectures",
    "text": "Lectures\nThe code from the lectures is in the left sidebar.\nLecture slides and information about homeworks, office hours etc. can be found on Canvas."
  },
  {
    "objectID": "getting-started/getting-started.html",
    "href": "getting-started/getting-started.html",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#anaconda-installation",
    "href": "getting-started/getting-started.html#anaconda-installation",
    "title": "Getting started",
    "section": "",
    "text": "We will use Python via the Anaconda distribution.\nDownload Anaconda here.\n\n\n\n\n\n\nNote\n\n\n\nThere are a variety of different Python distributions; for statistics and machine learning, we recommend Anaconda as it comes with many useful packages pre-installed.\n(It is also a good idea NOT to use your computer’s pre-installed Python - you don’t want to accidentally change any system settings!)\n\n\nAnaconda requires a few GBs of storage - a more lightweight version is Miniconda, which you can download here.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#managing-packages",
    "href": "getting-started/getting-started.html#managing-packages",
    "title": "Getting started",
    "section": "Managing packages",
    "text": "Managing packages\nThere are many open source Python packages for statistics and machine learning.\nTo download packages, two popular package managers are Conda and Pip. Both Conda and Pip come with the Anaconda distribution.\nConda is a general-purpose package management system, designed to build and manage software of any type from any language. This means conda can take advantage of many non-python packages (like BLAS, for linear algebra operations).\nPip is a package manager for python. You may see people using pip with environments using virtualenv or venv.\nWe recommend:\n\nuse a conda environment\nwithin this environment, use conda to install base packages such as pandas and numpy\nif a package is not available via conda, then use pip\n\nSee here for some conda vs pip misconceptions, and why conda is helpful.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#environments",
    "href": "getting-started/getting-started.html#environments",
    "title": "Getting started",
    "section": "Environments",
    "text": "Environments\n\nAbout\nIt is good coding practice to use virtual environments with Python. From this blog:\n\nA Python virtual environment consists of two essential components: the Python interpreter that the virtual environment runs on and a folder containing third-party libraries installed in the virtual environment. These virtual environments are isolated from the other virtual environments, which means any changes on dependencies installed in a virtual environment don’t affect the dependencies of the other virtual environments or the system-wide libraries. Thus, we can create multiple virtual environments with different Python versions, plus different libraries or the same libraries in different versions.\n\n\n\n\nCreating an environment for MSDS-534\nWe recommend creating a virtual environment for your MSDS-534 coding projects. This way, you can have an environment with all the necessary packages and you can easily keep track of what versions of the packages you used.\n\nOpen Terminal (macOS) or a shell\nCreate an environment called msds534 using Conda with the command: conda create --name msds534\nTo install packages in your environment, first activate your environment: conda activate msds534\nThen, install the following packages using the command: conda install numpy pandas scikit-learn matplotlib seaborn jupyter ipykernel\nInstall PyTorch by running the appropriate command from here (for macOS, the command is: pip3 install torch torchvision)\nTo exit your environment: conda deactivate\n\nHere is a helpful cheatsheet for conda environment commands.\nFor more details about the shell / bash, here is a helpful resource.",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#vscode",
    "href": "getting-started/getting-started.html#vscode",
    "title": "Getting started",
    "section": "VSCode",
    "text": "VSCode\nThere are a number of Python IDEs (integrated development environments). In class, we will be using VSCode (download here).\n\nDownload lecture-1.ipynb here and open it in VSCode.\nTo use your msds534 environment, on the top right hand corner, click “Select Kernel” &gt; “Python Environments” &gt; msds534. If it prompts you to install ipykernel, follow the prompts to install it.\n\nJupyter notebooks (.ipynb files) are useful to combine code cells with text (as markdown cells).\nVSCode also has a Python interactive window (details here).",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  },
  {
    "objectID": "getting-started/getting-started.html#learning-python",
    "href": "getting-started/getting-started.html#learning-python",
    "title": "Getting started",
    "section": "Learning Python",
    "text": "Learning Python\nIn this class, we will assume some familiarity with:\n\nNumpy\nPandas\nObject-oriented programming",
    "crumbs": [
      "Home",
      "Getting started"
    ]
  }
]